---
title: "human_datawrangle"
output: html_document
date: "2022-10-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(janitor)
library(vegan)
```

## SAVED FOR SOURCING AS HUMAN_COUNT_SCRIPT

## plan

I think the best way to do this will be to combine file skills and tidyverse skills

1. pull in filenames from all human photos (Zooniverse > people_images)
2. add in underscores to filenames where necessary, like before (will have to add to the existing code)
3. count
4. divide by 8
5. make sure dates and sites match those in the zooniverse export that we are using human data for (right now we only have one export - 24oct22)

```{r data import from filenames}

filenames <- list.files("/Users/zoe/Library/CloudStorage/GoogleDrive-zilz@ucsb.edu/My Drive/SCHOOL/PHD_AT_UCSB/GaviotaCameraTrapping/Zooniverse/people_images",
                        
                        include.dirs = FALSE,
                        full.names = FALSE,
                        all.files = FALSE) #sometimes takes a long time bc files are stored on the cloud

```

```{r and clean}

hooman <- read.table(text = filenames) %>%  # turns filename char vec into dataframe
  distinct() %>%     # removes dups which happen when drive creates temp files
  filter(!row_number() %in% 1 ) %>%  #why is THIS THE ONLY WAY TO REMOVE ONE FUCKING ROW
  filter(!grepl("IMG", V1)) %>%  # i guess grepl() is an alternative to str_detect?? taking out all unnamed (oops)
  
  ## need to fix the fact that filenames are in two different formats because I'm an idiot and it's hard to batch fix shit like this outside of command line:
  mutate(V2 = str_replace_all(V1, c("bc17apr22" = "bc_17apr22",
                                    "bc28apr22" = "bc_28apr22",
                                    "cove17apr22" = "cov_17apr22",
                                    "gov17apr22" = "gov_17apr22", 
                                    "nvs28apr22" = "nvs_28apr22",
                                    "pb28apr22" = "pb_28apr22",
                                    "pl17apr22" = "pl_17apr22",
                                    "pp17apr22" = "pp_17apr22",
                                    "gc28apr22" = "gc_28apr22",
                                    "pd28apr22" = "pd_28apr22")
                              )
         ) %>%  # not sure why this is the syntax but for str_replace for a dataframe this is what works?
  
  separate(V2, c("site", "date", "photonum"), sep = "_", remove = FALSE) %>%  # separates filename into site/date identifier and orig photo number
  mutate(photonum = str_remove(photonum, ".JPG")) %>%   # remove the the .JPG from photonum
  mutate(photonum = as.numeric(photonum)) %>% 
  arrange(site, date, photonum) %>% 
  select(site, date, photonum)

#we can ignore the below, even though it would make things slightly easier. later we can just divide human seconds by 8

  #mutate(imgnum = rep_len(1:8, nrow(.))) %>%  # adds a column repeating 1:8 # CAROLINE SAYS YOU NEED THE DOT I HATE HER
  #mutate(subject = rep(1:(nrow(.)/8), each = 8)) %>%   # adds a column linking all images related to one subj together
### now need to expand the dataframe long ways so that 1:8 are 8 different columns
  # need to reorder the dataframe by sitedate and THEN photonum
  #select(!photonum) %>% # have to take photonum out because it adds a unique identifier to each photo which messes up pivot_wider
  #pivot_wider(names_from = imgnum, values_from = V1, names_prefix = "image") # creates a new column (image1-image8) for each of the 8 images per subject (V1 is the col of filenames)


# get a nice summary table of counts of humans per site

hooman_count <- hooman %>% 
  count(site, date) %>% 
  mutate(seconds = n/8)

```


Going to try to use magick tesseract to extract date/time data from images
https://cran.r-project.org/web/packages/tesseract/vignettes/intro.html#Language_Data
https://johnpvanek.weebly.com/intothephrag/extracting-text-from-camera-trap-photos

```{r install and load packages}

library(tesseract) #OCR package
library(magick) # image processing package
library(here)

```

testing using one test image
```{r test images}

test_1 <- image_read(here("test_image_recon.JPG")) %>% 
  #image_resize("2000x") %>%
  image_convert(type = 'Grayscale') %>%
  #image_trim(fuzz = 40) %>%
  #image_write(format = 'png', density = '300x300')
  ocr()


```

