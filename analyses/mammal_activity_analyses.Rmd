---
title: "Mammunity vs Humans"
output: html_document
date: "2024-07-23"
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
# setting all chunks not to display anything but output so i can easily knit and send to hillary

library(tidyverse)
library(car)
library(ggpubr)
library(ggprism) # paste pvalues
library(geosphere)
library(lubridate)
library(hms)
library(janitor)
library(kableExtra)
library(formattable)
library(webshot2)
library(vegan)
library(here)
library(RColorBrewer)
library(rstatix)
library(Polychrome) #package to make palettes witha shit ton of distinct colors
library(MuMIn)
library(glmmTMB)
library(lme4)
library(corrplot)
library(DHARMa)
library(AICcmodavg)
library(ggeffects)
library(effects)
library(bbmle)

```

# Catch All Markdown for Mammal Community and Activity Summary Figures and Stats
### Behind the Scenes
#### Pull and Clean Data
```{r data import and cleaning}
#### data versioning log ###
# 23 jul 2024: had to manually paste in some NVS photos with the correct date 
# 31 jul using what's hopefully final version, there will be new date issues I think
# 7 aug incorporating real human data, although the humans are IDd with computer vision it's very reliable so we're using it! same date issues as last version i hope


# let's see what kind of mess we're working with
mammals_unclean <- read_csv(here("data/WI_data_7aug2024/classifications_7aug2024.csv"))
mammals_prev <- read_csv(here("data/WI_data_31jul2024/image_classifications_31jul2024.csv")) 


#unique(mammals_unclean$behavior)

mammals <- mammals_prev %>%
  filter(class == "Mammalia") %>% 
  #filter(!identified_by == "Computer vision") %>%  # need to retain these because they're people pics
  
  # ugh annoying unclean inconsistent capitals
  mutate(filename = tolower(filename)) %>% 
  mutate(common_name = tolower(common_name)) %>% 
  
  # ugh ugh the filenames where i forgot to put a "_"
  mutate(filename = str_replace(filename, "pp17apr22", "pp_17apr22")) %>% 
  mutate(filename = str_replace(filename, "boatcam17apr22", "boat_17apr22")) %>% 
  mutate(filename = str_replace(filename, "pb28apr22", "pb_28apr22")) %>% 
  mutate(filename = str_replace(filename, "gov17apr22", "gov_17apr22")) %>% 
  mutate(filename = str_replace(filename, "nvs28apr22", "nvs_28apr22")) %>% 
  mutate(filename = str_replace(filename, "pl17apr22", "pl_17apr22")) %>%  
  mutate(filename = str_replace(filename, "cove17apr22", "cove_17apr22")) %>% 
  mutate(filename = str_replace(filename, "bc17apr22", "bc_17apr22")) %>%
  mutate(filename = str_replace(filename, "bc28apr22", "bc_28apr22")) %>%
  mutate(filename = str_replace(filename, "pd28apr22", "pd_28apr22")) %>% 
  mutate(filename = str_replace(filename, "gov_12nov22", "big_12nov22")) %>%
  mutate(filename = str_replace(filename, "gov_17sep", "big_17sep")) %>%

    # also inconsistent sitenames:
  mutate(sitename = case_when(deployment_id == "Boneyard 09/07/2022" ~ "Boneyard Cam",
                              deployment_id == "Old Fencepost Cam April 2022" ~ "Old Fencepost Cam",
                              deployment_id == "Percos Post March 2022" ~ "Percos Post Cam",
                              deployment_id == "seawall cam" ~ "Seawall Cam",
                              deployment_id == "Boathouse May - July 2022" ~ "Boathouse Cam",
                              deployment_id == "Saucito" ~ "Saucito Cam",
                              .default = as.character(deployment_id))) %>% 
  
  mutate(deployment_id = paste0(str_extract(filename, "[^_]*_[^_]*"))) %>%  # return only the first 2 substrings (e.g. cam name and dmy) of the filenames - this is very sloppy and I hate it but oh well
  
  #English translation:
    # [^_]* = as many non-underscore characters as possible
    # _ = an underscore
    # [^_]* = as many non-underscore characters as possible
    # [...] is a character class. [abc] means "a or b or c", and [^abc] means anything but a or b or c.
  
  # editing the csv messed up the dates which were so nice before
  #mutate(timestamp = as.POSIXct(timestamp, tz=Sys.timezone()))# %>% # THIS IS SUPER INCONSISTENT, SOMETIMES THE DATA IMPORTS WITH NORMAL DATES, SOMETIMES NOT
  
  ## main thing to do is clean up the behavior column which is a mess. Specifically need to separate all of the "holding" etc behaviors from the item
  mutate(behavior = tolower(behavior)) %>% 
  mutate(behavior_clean = str_replace(behavior, "carrying", "carrying;")) %>% 
  mutate(behavior_clean = str_replace(behavior_clean, "holding food", "holding;")) %>%
  mutate(behavior_clean = str_replace(behavior_clean, "holding ", "holding;")) %>% 
  mutate(behavior_clean = str_replace(behavior_clean, "eating ", "eating;")) %>%
  mutate(behavior_clean = str_replace(behavior_clean, "foraging", "foragin")) %>% # i think this is the only way to do this given the nature of this typo
  mutate(behavior_clean = str_replace(behavior_clean, "foragin", "nose to ground")) %>%
  mutate(behavior_clean = str_replace(behavior_clean, "flighting", "fighting")) %>%
  mutate(behavior_clean = str_remove(behavior_clean, "\\[")) %>%
  mutate(behavior_clean = str_remove(behavior_clean, "\\]")) %>%
# there are many more behaviors to clean but for now, moving on to cleaning identifications because we are likely not going to focus on behavior


  separate(behavior, c("behavior_clean", "prey_or_item"), sep = ";", remove = FALSE) %>% 
  separate(behavior, c("behavior1", "behavior2"), sep = ",", remove = TRUE) %>%  # separates out behaviors if there are multiple
  
  uncount(number_of_objects, # "uncounts" i.e. makes specified number of copies of the row based on the # in how_many
          # we are basically multipling the number captures of animals by the number of animals in the capture - this is fine...
          
          .remove = FALSE #check your work by including .remove = FALSE... this keeps the column where the #of copies to make was stored. default is to delete it
          ) %>% 
 
# going to make some assumptions about IDs here:
  mutate(common_name = case_when(common_name == "canis species" ~ "coyote",
                        common_name == "canine family" ~ "coyote",
                        common_name == "cervus species" ~ "mule deer",
                        common_name == "cervidae family" ~ "mule deer",
                        common_name == "cetartiodactyla order" ~ "mule deer",
                        common_name == "odocoileus species" ~ "Mule Deer",
                        common_name == "elk" ~ "mule deer", # def not an elk
                        common_name == "even-toed ungulate" ~ "mule deer",
                        common_name == "pronghorn" ~ "mule deer", 
                        common_name == "vulpes species" ~ "grey fox",
                        common_name == "lepus species" ~ "brush rabbit", 
                        common_name == "domestic cat" ~ "bobcat", # double checked, def a bobcat
                        common_name == "cat family" ~ "bobcat", 
                        common_name == "eurasian lynx" ~ "bobcat",
                        common_name == "lynx species" ~ "bobcat",
                        common_name == "domestic pig" ~ "feral pig", 
                        common_name == "wild boar" ~ "feral pig",
                        common_name == "sus species" ~ "feral pig",
                        common_name == "neotoma species" ~ "rat/mouse",
                        common_name == "martes species" ~ "weasel",
                        common_name == "ursus species" ~ "american black bear",
                        common_name == "nutria" ~ "rat/mouse", # for now, until we determine if that thing is actually a nutria!
                        common_name == "western gray squirrel" ~ "california ground squirrel",
                        common_name == "brown rat" ~ "rat/mouse",
                        common_name == "woodrat or rat or mouse species" ~ "rat/mouse",
                        common_name == "rodent" ~ "rat/mouse",
                        common_name == "california mouse" ~ "rat/mouse",
                        common_name == "house mouse" ~ "rat/mouse",
                        common_name == "muridae family" ~ "rat/mouse",
                        common_name == "geomyidae family" ~ "gopher",
                        common_name == "kit fox" ~ "coyote", # def not a kit fox 
                        common_name == "white-tailed deer" ~ "mule deer",
                        common_name == "domestic cattle" & deployment_id == "Jalama 2" ~ "feral hog", # went and checked and computer IDd some pigs as cows. fixing for now in post, will fix later in WI [4aug23]
                        common_name == "white-tailed jackrabbit" ~ "coyote", # based on pics these are always yotes
                        common_name == "black-tailed jackrabbit" ~ "coyote", # checked and all jackrabbit IDs are coyotes
                        common_name == "mammal" ~ "unidentified mammal",
                        common_name == "carnivorous mammal" ~ "unidentified mammal",
                        common_name == "human-camera trapper" ~ "human",
                        common_name == "human-pedestrian" ~ "human",
                        common_name == "human - biker" ~ "human",
                        TRUE ~ as.character(common_name)
                        )
         ) %>% 
# need to remove domestics, marine mammals, sheep, mouflon?? and other weird IDs KEEP HUMANS, WILL REMOVE LATER
  filter(common_name != "mouflon", 
         common_name != "domestic sheep",
         common_name != "domestic cattle",
         common_name != "domestic donkey",
         common_name != "equus species",
         common_name != "puma", # at this moment, this is not a puma, it's a dog
         common_name != "domestic cow",
         common_name != "domestic horse",
         common_name != "californian sea lion",
         common_name != "harbor seal",
         common_name != "perissodactyla order",
         common_name != "bovidae family"
#         common_name != "domestic dog",
#         common_name != "human",
#         common_name != "human - camera trapper",
#         common_name != "human - biker",
#         common_name != "human",
#         common_name != "human-camera trapper",
#         common_name != "human-pedestrian",
         ) 

spp <- unique(mammals$common_name)
spp <- unique(mammals$common_name2)

```

#### Fix Messed Up Dates in Dataset
```{r fixing dates}

# we will first pull out just the incorrect dates, which are luckily all before 2020
fix_dates <- mammals %>% 
    ## make the dates nice
  mutate(year = year(timestamp)) %>% 
  filter(year == 2017 | 
           year == 2018 | 
           year == 2020 | 
           year == 2021 |
           deployment_id == "boat_16oct22" |
           deployment_id == "boat_17apr22" |
           deployment_id == "boat_17sep22" |
           deployment_id == "boat_5apr22" )

  #filter(year<=2021) # for now we have to make this more complicated because WI randomly changed dates of some deployments

# now we need to figure out what the corrections are, and for that we will need this dataset: https://docs.google.com/spreadsheets/d/1-gLml8H2stC2Va9H42tWLylx-5f1r8m4BDMKnsBUHl8/edit?usp=sharing (downloaded csv of one tab)
timestamp_corrections1 <- read_csv(here("data/incorrect_timestamps.csv")) 

timestamp_corrections <- timestamp_corrections1 %>% 
  rename(deployment_id = filename) %>% 
  mutate(real_start = mdy_hm(real_start)) %>% 
  mutate(recorded_start = mdy_hm(recorded_start)) %>% 
  mutate(real_end = mdy_hm(real_end)) %>% 
  mutate(recorded_end = mdy_hm(recorded_end)) %>% 
  mutate(correction = difftime(real_start, recorded_start, units = "mins")) %>%  #gives us the # minutes to add to the recorded time to correct the timestamp
  dplyr::select(deployment_id, correction)

#test_correct <- timestamp_corrections %>% 
#  mutate(calculated_end = recorded_end + correction) # it works!!

# now we can apply to the timestamp on each incorrect date, I hope!

## by matching on deployment ID
fixed_dates <- left_join(fix_dates, timestamp_corrections, by = join_by(deployment_id)) %>%  # pop the correction time in to the deployments it applies to
  # overwrite timestamp column with correct date
  mutate(timestamp = timestamp + correction) %>% 
  dplyr::select(!c(correction, year)) # take these bad bois out
  

# join the two datasets back together!
mammals_correctdates <- mammals %>% 
    filter(year(timestamp) == 2022 | 
           year(timestamp) == 2023 ) %>% 
    filter(deployment_id != "boat_16oct22") %>% 
    filter(deployment_id != "boat_17apr22") %>% 
    filter(deployment_id != "boat_17sep22") %>% 
    filter(deployment_id != "boat_5apr22" ) # need to remove the bad dates from the original dataset
  #filter(year(timestamp)>2021) # no longer this simple

#mammals_incorrectdates <- mammals %>% 
#  filter(year(timestamp)<=2021) # just to check (right now same # rows as 'fixed dates' so that's great)

## and do some downstream cleaning like removing useless columns
mammals_fixed_dates <- rbind(mammals_correctdates, fixed_dates) %>% 
  dplyr::select(sitename, deployment_id, filename, identified_by, class, order, family, genus, species, common_name, timestamp, number_of_objects, behavior1, behavior2, prey_or_item) %>% 
  mutate(day = day(timestamp)) %>% 
  mutate(month = month(timestamp, label = TRUE)) %>% 
  mutate(year = year(timestamp)) %>% 
  mutate(date = date(timestamp)) %>% 
  mutate(time = format(timestamp, '%T')) %>% 
  #separate(timestamp, c("date", "time"), sep = " ") %>% this creates blanks were the time is 0:00:00
  mutate(common_name = tolower(common_name))

```

#### We should add in some metadata for sorting and analysis purposes
```{r add in metadata}

######################## "monthly" deployment data ######################## 

deployment_data_original <- read_csv(here("data/cam_deployment_data_FIXED_DATES_29jul2024.csv")) %>% 
  clean_names() %>% 
  rename(sitename = camera_name, 
         maint_month = month,
         deployment_id = filename_prefix,
         property = location) %>% 
  
  # need to rename some file prefixes to match what is in mammals post-cleaning (ie adding underscores)
  mutate(deployment_id = str_replace(deployment_id, "pp17apr22", "pp_17apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "boatcam17apr22", "boat_17apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "pb28apr22", "pb_28apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "gov17apr22", "gov_17apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "nvs28apr22", "nvs_28apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "pl17apr22", "pl_17apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "cove17apr22", "cove_17apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "bc28apr22", "bc_28apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "bc17apr22", "bc_17apr22")) %>%
  mutate(deployment_id = str_replace(deployment_id, "pd28apr22", "pd_28apr22")) %>% 
  filter(!is.na(deployment_id)) %>% # will sort out deployments that don't have an ID because I manually added these in
  
  # need to fix days active column:
  mutate(day_end = mdy_hm(day_end)) %>% 
  mutate(day_start = mdy_hm(day_start)) %>% 
  mutate(days_active_calcd = difftime(day_end, day_start, units = "days")) %>% 
  
  filter(exclude != "y") # also need to take out the deployments I found a reason to exclude (tipped to sky etc)
  
# also need to figure out how many days active per MONTH
## need to remember WHY i did it this way when i'd already calculated it below using min(date) and max(date) by month...

deployment_data <- deployment_data_original %>%   
  mutate(each_date = map2(day_start, day_end, ~seq(from = .x, to = .y, by = "day"))) %>% # creates a list of all dates between each start and end date (each row)
  unnest(each_date) %>% # expands that list to be one row per date
  mutate(month = month(each_date, label = TRUE)) %>%  # extracts month from the dates
  group_by(deployment_id, month) %>% 
  mutate(days_per_month = n()) %>% 
# works if i want to keep all rows i just created but i dont, i want to summarize
  distinct(deployment_id, month, .keep_all = TRUE) %>%  # lets me keep only rows that are unique for combo of deployment id and month, while keeping all other columns
  dplyr::select(!each_date) %>%  # no longer need this
  ungroup()

############# create a dataframe for each date at each site that the camera was active, will be useful later for adding in zeros ############
all_days_sampled <- deployment_data_original %>%   
  #filter(exclude == "n") %>%  # need to take out the excluded days! turns out i already did this
  mutate(each_date = map2(day_start, day_end, ~seq(from = .x, to = .y, by = "day"))) %>% # creates a list of all dates between each start and end date (each row)
  unnest(each_date) %>% # expands that list to be one row per date
  mutate(month = month(each_date, label = TRUE)) %>% 
  mutate(date = date(each_date)) %>% 
  select(property, sitename, deployment_id, month, date)


######################## add on human data to montly deployment data #########################

humans_est <- read_csv(here("data/human_activity.csv")) %>%  # current best estimate of sitely, monthly human activity (made 8/3/2023)
  mutate(month = month(dmy(date), label = TRUE)) %>% 
  mutate(year = year(dmy(date))) %>% 
  rename(sitename = site,
         n_human_shots = n,
         human_seconds = seconds) %>% # NOT corrected by sampling effort
  dplyr::select(sitename, year, month, n_human_shots, human_seconds)

deployment_data_humans <- left_join(deployment_data, humans_est, by = join_by(sitename, month)) %>% 
  # fill in zeros for where there "is no human activity" - apparently, must check this
  mutate(human_seconds = case_when(is.na(human_seconds) ~ 0,
                                   TRUE ~ as.numeric(human_seconds))) %>% 
  ungroup() %>% 
  mutate(sitename = fct_relevel(sitename, c("Boathouse Cam", "Ladrones Cam", "Saucito Cam", "Water Canyon Cam", "Not Water Canyon Cam", "Morida Cam","RIZ Cam", "RIZ Cam Backup", "RIZ Cam S Canyon", "Sudden Canyon Cam", "Old Fencepost Cam", "Jolluru Cam", "Short Canyon Cam", "Jalama Cam", "Jalama 2 Cam", "Cojalama Cam", "Cojo Gate Cam", "Cojo Canyon Cam", "Seawall Cam", "Black Canyon Cam", "North Beach Fort Cam", "North Vista Spring Cam", "North Beach Canyon Cam", "Boneyard Cam", "Cove Cam", "Govies Cliff Cam", "Big Cojo Cam", "Percos Boat Cam", "Percos Driftwood Cam", "Wood Canyon Cam", "Percos Beach Cam", "Percos Log Cam", "Percos Post Cam", "Damsite Creek Cam")))
  
  # turns out there are some duplicates of months that have different human estimates, so we should sum these by month (easy to do using group_by (site, month) and mutate)
  # we do this downstream when we make deployment data into montly data
  

######################## site metadata #########################

metadata <- read_csv(here("data/site_metadata.csv")) %>% # NOT camera_metadata.csv (site_metadata is in camtrapR format)
  clean_names() %>% 
  filter(!str_detect(camera_id, "2")) %>%  # removing second cameras from the metadata (these were placed when the originals were stolen, but in the same place)
  dplyr::select(station, utm_x, utm_y, cam_brand, property, habitat_adjacent, habitat_secondary, iz_type, burst_settings) %>% 
  rename(sitename = station)

#### add in distance to Jalama (as center of human activity) using package geosphere ####

jalama_loc <- tibble(utm_x = 34.510480, utm_y = -120.501467) 
jalama_mtx <- cbind(jalama_loc$utm_y, jalama_loc$utm_x) # dist function requires that both distances be in matrix form, this is one way to do that apparently
# help doc example doesn't say this is necessary but oh well

metadata2 <- metadata %>% 
  mutate(dist_jalama = (distVincentyEllipsoid(cbind(utm_y, utm_x), jalama_mtx))/1000) %>% # dist returns shortest distance in meters as default
  filter(sitename != "North Jalama") # for now, so list matches the data we're actually using

######################## combine and add metadata to mammals data #########################

# this combines deployment (now we've made it monthly) records with metadata for each site
metadata_by_deployment <- left_join(deployment_data_humans, metadata2, by = join_by(property, sitename)) %>% 
  dplyr::select(property, sitename, deployment_id, dist_jalama, imgs_on_sd, days_active_calcd, days_per_month, month, exclude, utm_x, utm_y, habitat_adjacent, habitat_secondary, iz_type, burst_settings, cam_brand, n_human_shots, human_seconds) %>% 
  filter(exclude == "n") %>% 
  filter(!is.na(deployment_id)) # filters out all the deployments that haven't been reviewed because i don't put in filename prefix if it hasn't been reviewed on WI

# cross checking that all deployments in the metadata are present in the mammals df and vice versa
deployments_from_mammalsdf <- mammals_fixed_dates %>% 
  distinct(deployment_id, month, .keep_all = TRUE)

missing_deployments <- anti_join(deployments_from_mammalsdf, metadata_by_deployment, by = c("deployment_id", "month"))  # anti_join() return all rows from x without a match in y  ]

# for some reason this dataframe construction is duplicating some rows .... causing downstream issues, so we are going to use a dataframe BY MONTH instead and then match to mammals using combo of site and month

metadata_by_month <- metadata_by_deployment %>% 
  distinct(sitename, month, days_per_month, .keep_all = TRUE) %>% # only unique combinations of site, month, and days deployed because some months were split over deployments
  # so now we need to sum days deployed when they're split across multiple deployments but in the same month:
  group_by(property, sitename, dist_jalama, utm_x, utm_y, habitat_adjacent, habitat_secondary, iz_type, burst_settings, cam_brand, # none of these change over the course of the project
           month) %>% # these are how we want to sum days per month (sums across deployments)
  summarise(trapnights = sum(days_per_month),
            total_human_sec = sum(human_seconds)) %>% 
  unite(sitemonth_id, c("sitename", "month"), sep = ".", remove = FALSE) %>% 
  ungroup() %>% 
  mutate(humans_per_trapnight = total_human_sec/trapnights) %>% 
  mutate(sitename = fct_relevel(sitename, c("Boathouse Cam", "Ladrones Cam", "Saucito Cam", "Water Canyon Cam", "Not Water Canyon Cam", "Morida Cam","RIZ Cam", "RIZ Cam Backup", "RIZ Cam S Canyon", "Sudden Canyon Cam", "Old Fencepost Cam", "Jolluru Cam", "Short Canyon Cam", "Jalama Cam", "Jalama 2 Cam", "Cojalama Cam", "Cojo Gate Cam", "Cojo Canyon Cam", "Seawall Cam", "Black Canyon Cam", "North Beach Fort Cam", "North Vista Spring Cam", "North Beach Canyon Cam", "Boneyard Cam", "Cove Cam", "Govies Cliff Cam", "Big Cojo Cam", "Percos Boat Cam", "Percos Driftwood Cam", "Wood Canyon Cam", "Percos Beach Cam", "Percos Log Cam", "Percos Post Cam", "Damsite Creek Cam"))) %>% 
  select(!c(cam_brand))

########## FINAL dataset of mammal activity ########## 
mammals_clean <- left_join(mammals_fixed_dates, metadata_by_month, by = join_by(sitename, month)) %>% 
  #rename(sitename = sitename.x) %>%  #got two of these in the join
  #dplyr::select(!sitename.y) %>% 
  filter(!is.na(property)) %>%  # right now (23 jul 2024) this filters out HROA data because there's not metadata for those cams
  filter(common_name != "human",
         common_name != "domestic dog") # finally time to rule these out bc they're contained in human/trapnight estimate

# the above SHOULD pop on monthly metadata to each observation based on its observed month and site

####### TABLING BELOW FOR NOW UNTIL WI DATA IS MORE COMPLETE #######
#humans_clean <- left_join(mammals_fixed_dates, metadata_by_month, by = join_by(sitename, month)) %>% 
  #rename(sitename = sitename.x) %>%  #got two of these in the join
  #dplyr::select(!sitename.y) %>% 
  #filter(!is.na(property)) %>%
#  filter(common_name == "human"|
#         common_name == "domestic dog")

#humans_summary <- humans_clean %>% 
#  group_by(sitename, burst_settings, month, days_per_month) %>% 
#  summarise(n = n()) %>% 
#  mutate(seconds = n/burst_settings) %>% 
#  mutate(seconds_per_trapnight = seconds/days_per_month) %>% 
#  ungroup()
#write_csv(humans_summary, here("data/humans_summary.csv"))

# saving this for reproducibility
#write_csv(rbind(mammals_clean, humans_clean), here("data/classifications_w_metadata_full_9aug2024.csv"))

```

## Planned Initial Analyses
- response variable: animal activity (count of occurrences) across all species and across entire study with month as random effect
- effect variables of interest: distance from Jalama, human activity, species
      - maybe above or below Point Conception (barrier to human movement from Jalama)
- random variables: month
- will need to include an offset of days active per site
 
## Bin Data Appropriately
counts by site > month (losing daily or weekly variation or variation due to species)
counts by 24hr period, results in zero inflation, but that might be ok
counts by 7-day surveys seems like the best compromise

### By Day (Easy)
```{r bin by day}
activity_by_day <- mammals_clean %>% 
  ungroup() %>% 
  count(property, sitename, month, date, sitemonth_id, dist_jalama, habitat_adjacent, habitat_secondary, iz_type, total_human_sec, humans_per_trapnight, trapnights, burst_settings, common_name) %>%  # need all the covariates in here, including trap nights in case we want to adjust counts before summing? idk
  # count() combines group_by and summarize(n_distinct) so we get a count of the rows in this combo of variables, in the order we specify
  mutate(captures_adj = ceiling(n/burst_settings)) %>%  # adjusts by burst (divides by 8 or 10) and rounds up
  select(!n)


# we will also make a matrix version (all species have a column) becuase it's easier to add on days where nothing was taken later on
activity_by_day_matrix <- activity_by_day %>% 
  pivot_wider(names_from = common_name, 
              values_from = captures_adj, 
              values_fill = 0, # adds zeros where appropriate
              names_prefix = "sp_") 

# stopping before adding in zeros because we're not really going to use this dataframe

```


### By 7-day "Surveys"
can use floor_date() from lubridate I think "floor_date() takes a date-time object and rounds it down to the nearest boundary of the specified time unit."
need to also use "all days sampled" dataset to make sure we get the days where nothing was found in there - easier to also start with the by-day binned dataset
```{r bin by 7day survey}

activity_7day1 <- activity_by_day_matrix %>% 
  full_join(., all_days_sampled) %>%  # if we leave out by = then it should just join on all common columns

  # trying fill instead of join for other variables:
  group_by(property, sitename, month) %>%  # becayse all of our covariates are measured on the site:month scale, they should be the same within these groups and we can fill from previous values
  fill(sitemonth_id:burst_settings) %>%  # default is down which is fine i think
  arrange(sitename, date) # so i can make sure all the dates are there

# this got more complicated than i thought and we still need to bind metadata to the rows that had no animla photos (so no way to populate the date rows with metadata from previous rows)

# take out any row with NAs in the metadata
activity_7day_wmeta <- activity_7day1 %>% 
  filter(!is.na(habitat_adjacent)) # gotta take these out

# pulling NAs out and attempting a join again on just those
activity_7day_nometa <- activity_7day1 %>% 
  filter(is.na(habitat_adjacent)) %>% 
  select(!sitemonth_id:burst_settings) %>%  # need ot pull these cols out so i can replace them in the bind (filling NAs on a bunch of columns like this like with coalesce is onerous)
  left_join(., metadata_by_month)# %>% # a full 
  
  
  # now that all the metadata is in place, we can mash the two sub-frames back together fill NAs in the species columns with zeros
activity_7day <- rbind(activity_7day_wmeta, activity_7day_nometa) %>% 
  select(!c(deployment_id, utm_x, utm_y)) %>%  # i don't need these rn and they have NAs so bye
  # replace zeros
  mutate(across(starts_with("sp_"), ~replace_na(., 0))) %>%  # dont forget the tilde
  # now we can start collapsing things into 7-day chunks
  mutate(week = week(date)) %>%
  
  # create a survey ID the represents the week of "collection" (on a 52 week calendar)
  unite(survey_id, c(sitename, week), remove = FALSE) %>% 
  select(!week) %>% # we wnat to keep sitename but not week
  
  # now we can pivot longer again before we sum all the counts
  pivot_longer(cols = 14:28,
               names_to = "common_name", 
               names_prefix = "sp_", 
               values_to = "daily_seconds") %>% # since we already adjusted by burst settings before matrix, these are seconds and not counts
  
  ## need to FIND A WAY to get active days per week, looks like we gotta skip summarise and group_by > mutate
  group_by(sitename, survey_id) %>%  # gotta keep all covariates
  mutate(weekly_trapnights = n_distinct(date)) %>% # will this work with group_by? it should
  ungroup() %>%  
  
  # group again but now by species
  group_by(property, sitename, dist_jalama, habitat_adjacent, habitat_secondary, iz_type, month, total_human_sec, humans_per_trapnight, survey_id, weekly_trapnights, common_name) %>% #
  summarise(activity_seconds = sum(daily_seconds)) %>% 
  ungroup() %>% 
  mutate(seconds_per_trapnight = activity_seconds/weekly_trapnights) %>% # cna only do this for animals since humans were estimated on a monthly basis, not weekly
  arrange(sitename, survey_id)


  ##### below is floor_date, a really good way to do this but i decided i want to group by assigning lubridate:week
  #group_by(sitename) %>%  #only grouping by sitename to allow the 7 day chunks to pass across months and just kinda see how that goes?
  #mutate(survey_start = floor_date(date, "7 days")) # "survey" here is our 7day chunk that were making sup

```


### By Month
```{r binning and summarizing data by month}

######### make table of species by month ###########
species_month <- mammals_clean %>%
  count(month, year, common_name) %>%  # makes a summary table of # images per species per month
# includes count of empties
  rename(n_imgs = n) # rename n to be more informative


######### make table of animal activity ( = incidences adjusted by burst number and corrected for by trap nights) by date and site and species ###########
activity_site_month_sp_1 <- mammals_clean %>% 
  ungroup()%>%
  group_by(property,sitename, dist_jalama, month, trapnights, burst_settings, total_human_sec, humans_per_trapnight, common_name) %>% 
  summarize(total_captures = n()
    #total_captures = sum(number_of_objects), #  this isn't returning the same # as counting rows because sometimes the value isn't 1!!!
            ) %>% # had a hard time with the final columns so I'm tossing it out into a mutate:
  mutate(activity_seconds = ceiling(total_captures/burst_settings))%>%  # adjusts # of captures by the number of images taken in a burst (8 or 10), ceiling rounds UP)
  # need to make trapnights numeric integers in order to adjust by them
  mutate(trapnights = case_when(trapnights == "0.000000 days" ~ 1,
                                TRUE ~ as.numeric(trapnights))) %>% 
  mutate(trapnights = case_when(trapnights == 0 ~ 1,
                                TRUE ~ as.numeric(trapnights))) %>%
  mutate(seconds_by_trapnight = activity_seconds/trapnights) %>% 

  unite("sitemonth_id", sitename, month, sep = ".", remove = FALSE) %>% 
  mutate(protection_rank = case_when(property == "JLDP" ~ 2, # actively managed
                                     property == "VSFB" ~ 1, # not managed but not developed
                                     property == "JBCP" ~ 0  # mildly developed recreational area
                                     )) %>% 
  mutate(season = case_when(month %in% c("Apr", "May", "Jun", "Jul", "Aug", "Sep") ~ "dry",
                            month %in% c("Oct", "Nov", "Dec", "Jan", "Feb", "Mar") ~ "wet")) %>% 
  relocate(protection_rank, .after = property) %>% 
  relocate(season, .before = month)


######### need to add in the zeros for all spp and humans that this dataframe doesn't reflect ###########
### easiest way to do that is to make a matrix

# one for raw seconds
activity_wide <- activity_site_month_sp_1 %>% 
  dplyr::select(!c(total_captures, seconds_by_trapnight)) %>%  # we need to take out some columns that only go with each indiv. species
  pivot_wider(names_from = common_name, names_prefix = "sp_", # column name, prefix with sp_ so we can act on all columns if we need to!
              values_from = activity_seconds, 
              values_fill = 0) %>% 
  ungroup()

# one for adjusted seconds
activity_wide_adj <- activity_site_month_sp_1 %>% # this one lets us look at captures adjusted by effort
  dplyr::select(!c(total_captures, activity_seconds)) %>%  # we need to take out some columns that only go with each indiv. species
  pivot_wider(names_from = common_name, names_prefix = "sp_", # column name, prefix with sp_ so we can act on all columns if we need to!
              values_from = seconds_by_trapnight, 
              values_fill = 0) %>% 
  ungroup()
 
# we want to keep the above dataframes for later matrix stuff, otherwise we could just pipe in pivoting them longer again 
activity_site_month_sp_2 <- activity_wide %>% 
  pivot_longer(cols = sp_coyote:sp_gopher,
               names_to = "common_name",
               names_prefix = "sp_", # this should remove the prefix I added, i think
               values_to = "activity_seconds")

activity_site_month_sp_adj <- activity_wide_adj %>% 
  pivot_longer(cols = sp_coyote:sp_gopher,
               names_to = "common_name",
               names_prefix = "sp_", # this should remove the prefix I added, i think
               values_to = "seconds_by_trapnight")

activity_site_month_sp <- full_join(activity_site_month_sp_2, activity_site_month_sp_adj) # will join based on all shared columns, so should only tack on the seconds by trapnight col
```

### Visualize Data ~ Month and Site
```{r plots of mean activity by month}

# by month, sites averaged
month_violin <- ggplot(activity_7day, aes(x = as.factor(month), y = activity_seconds)) +
  geom_violin()

# by month, adjusted by trapnights
month_adj_box <- ggplot(activity_7day, aes(x = as.factor(month), y = seconds_per_trapnight))+
  geom_boxplot()

month_adj_violin <- ggplot(activity_7day, aes(x = as.factor(month), y = seconds_per_trapnight)) +
  geom_violin()

# by month, with species colored different
month_species <- ggplot(activity_7day, aes(x = as.factor(month), y = seconds_per_trapnight)) +
  geom_point(aes(color = common_name)) 

# add a running mean per species
monthly_mean_spp <- activity_7day %>% 
  group_by(month, common_name) %>% 
  summarise(mean_activity = mean(seconds_per_trapnight),
            sd_activity = sd(seconds_per_trapnight)) %>% 
  ungroup() %>% 
  ggplot(., aes(x = month, y = mean_activity, group = common_name, colour = common_name)) +
  #geom_errorbar(aes(ymin = mean_activity - sd_activity, ymax = mean_activity + sd_activity)) + # better to just plot the scatter in this case
  geom_path() +
  geom_point( data = activity_7day, aes(x = month, y = seconds_per_trapnight, colour = common_name))

# based on this, looks like the peak in the summer is slightly a function of trapping effort (aka is less pronounced when we adjust by # trapnights)

activity_7day$sitename <- factor(activity_7day$sitename, levels = c("Boathouse Cam", "Ladrones Cam", "Saucito Cam", "Water Canyon Cam", "Not Water Canyon Cam", "Morida Cam","RIZ Cam", "RIZ Cam Backup", "RIZ Cam S Canyon", "Sudden Canyon Cam", "Old Fencepost Cam", "Jolluru Cam", "Short Canyon Cam", "Jalama Cam", "Jalama 2 Cam", "Cojalama Cam", "Cojo Gate Cam", "Cojo Canyon Cam", "Seawall Cam", "Black Canyon Cam", "North Beach Fort Cam", "North Vista Spring Cam", "North Beach Canyon Cam", "Boneyard Cam", "Cove Cam", "Govies Cliff Cam", "Big Cojo Cam", "Percos Boat Cam", "Percos Boat Rodents", "Percos Driftwood Cam", "Wood Canyon Cam", "Percos Beach Cam", "Percos Log Cam", "Percos Post Cam", "Damsite Creek Cam"))

# by site, months averaged
site_scatter <- ggplot(activity_7day, aes(x = sitename, y = activity_seconds)) +
  geom_point(aes(color = month))

# by site, months averaged, adjusted by effort
site_bar <- ggplot(activity_7day, aes(x = sitename, y = seconds_per_trapnight)) +
  geom_bar(aes(fill = month), stat = "identity")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 7))

# by property/ protection level, everything else grouped:

#protection_scatter <- ggplot(activity_7day, aes(x = protection_rank, y = activity_seconds)) +
#  geom_point(aes(color = property))


```

### Visualize Data ~ Season
```{r wet vs dry}

# should probably pull precip data and see if it actually matches what we've called wet and dry

##### all activity compared between wet and dry season ##### 
season_boxplot <- ggplot(activity_site_month_sp, aes(x = season, y = seconds_by_trapnight)) +
  #geom_boxplot()
  geom_jitter()

##### HUMAN activity compared between wet and dry season ##### 
human_season_boxplot <- activity_site_month_sp %>% 
  distinct(season, sitename, month, humans_per_trapnight) %>% # because we currently have a row per species for each site and month
  ggplot(., aes(x = season, y = humans_per_trapnight)) +
  #geom_boxplot()
  geom_jitter()

#####  wet vs dry season, split into species #####
spp_season_boxplot <- ggplot(activity_site_month_sp, aes(x = season, y = seconds_by_trapnight)) +
  geom_boxplot(aes(fill = common_name))

top3_season_boxplot <- activity_site_month_sp %>% 
  filter(common_name %in% c("coyote", "feral pig", "mule deer")) %>% 
  ggplot(., aes(x = season, y = seconds_by_trapnight)) +
  geom_boxplot(aes(fill = common_name))

#####  wet vs dry season, split into site #####
site_season_boxplot <- ggplot(activity_site_month_sp, aes(x = season, y = seconds_by_trapnight)) +
  geom_boxplot(aes(fill = sitename))

##### wet vs dry season, with human activity on x axis #####
human_season_scatter <- ggplot(activity_site_month_sp, aes(x = humans_per_trapnight, y = seconds_by_trapnight)) +
  geom_point()+
  facet_wrap(~ season)
  

```


### More visualizations

```{r visualize with scatter}

######### scatter plot of mammal activity by site by month ~ distance from jalama #########

activity_by_dist_plot <- ggplot(activity_7day, aes(x = dist_jalama, y = seconds_per_trapnight)) +
  geom_point(aes(color = common_name))+
  geom_smooth()

######### scatter plot of mammal activity by site ~ human activity #########

activity_by_humans_plot <- ggplot(activity_7day, aes(x = human_seconds, y = seconds_per_trapnight)) +
  geom_point(aes(color = common_name))+
  geom_smooth()

######### scatter plot of coyote activity by site by month ~ distance from jalama #########

#coyote_dist_plot <- ggplot(coyote_activity, aes(x = dist_jalama, y = activity_seconds)) + 
#  geom_point()

######### check out distribution of data #########

bysite <- ggplot(activity_7day, aes(x = seconds_per_trapnight)) +
  geom_density(aes(color = sitename, fill = sitename), alpha = 0.3) + # Note: just to show what the geom_violin shows
  theme_classic() +
  #scale_x_continuous(expand = c(0,0), limits = c(0,3e6)) +
  #scale_y_continuous(expand = c(0,0)) +
  labs(x = "Seconds of Wildlife Activity Per Trapnight", y = "Density")

bydays_sampled <- ggplot(activity_7day, aes(x = weekly_trapnights, y = seconds_per_trapnight)) +
  geom_point() + # Note: just to show what the geom_violin shows
  theme_classic() +
  #scale_x_continuous(expand = c(0,0), limits = c(0,3e6)) +
  #scale_y_continuous(expand = c(0,0)) +
  labs(y = "Captures (adjusted by burst settings)", x = "Days Camera Was Active per WEEK")

# days active and captures don't LOOK correlated but let's check
cor.test(~ seconds_per_trapnight + weekly_trapnights, data = activity_7day, method=c("spearman"))
# yup, correlated

# we should also check for correlation between human activity and distance from jalama

humans_inspace <- ggplot(activity_7day, aes(x = dist_jalama, y = human_seconds)) +
  geom_point()

cor.test(~ human_seconds + dist_jalama, data = activity_7day, method=c("spearman"))
cor.test(~ human_seconds + dist_jalama, data = activity_7day, method=c("pearson"))
cor(activity_7day$dist_jalama, activity_7day$human_seconds)


```


## Next run a GLMM with a negative binomial distribution: wildlife activity ~ distance from jalama * species * human activity + random effect of site and property
- actually taking out species as a factor because i will model the three most active species on their own later

### Trying to run models with response variable binned into month and adding in a covariate for season
We will do rainy season as oct - march and dry season as april - sept

```{r glmm activity model selection month sample}

# we know we need a mixed model because we need a random nesting effect of property and site x month
# variables of interest are monthly human seconds, season, distance from jalama, species, and interactions between all
# number of trapnights logged as an offset -- not sure why we log this but other papers do it (because there's a log link for the rest of the function)

incomplete <- activity_site_month_sp %>% 
  filter(!complete.cases(.)) #make sure every row has all data (no NAs)

# real random effect here, in this nested design, would be SITE as the thing that is repeatedly sampled, but site = distance from jalama and is the variable we are interested in...
# week/survey_id is also nested within site (and repeatedly sampled), so is a random variable, but it's the level of observation atm since we binned (summed) all occurences into weekly sums - i don't believe we need to include the level of sampling as an error term

# many models standardize their explanatory variables... why?
## found the answer here: https://stats.stackexchange.com/questions/35071/what-is-rank-deficiency-and-how-to-deal-with-it/35077#35077
### "The trick is usually to use common units, but on some problems even that is an issue when variables vary by too many orders of magnitude. More important is to scale your numbers to be similar in magnitude."

############### VARS ############### 

# RESPONSE variable is activity_seconds, offset is weekly_trapnights
# SAMPLES are 4 weeks x 9 months x 30 sites per species (although a lot of zeros)
# RANDOM variable is survey_id
# potential EXPLANATORY variables are:
## property: VSFB, JLDP, Jalama ## rank deficient
## season
## common_name (species) ## rank deficient
## iz_type ## rank deficient
## dist_jalama
## human_seconds

######### NULL MODELS ######### 

#glmm_null <- glmer.nb(activity_seconds ~ 1, 
#                     data = activity_site_month_sp, 
#                     family = nbinom2(link = "log"), 
#                     offset = log10(trapnights), 
#                         na.action = "na.fail")

glmm_null_randoms <- glmmTMB(activity_seconds ~ 1 
                             + (1|sitename), 
                             data = activity_site_month_sp, 
                             family = nbinom2(link = "log"), 
                             offset = log10(trapnights), 
                         na.action = "na.fail")

######### FULL MODELS ######### 

glmm_full_inx <- glmmTMB(activity_seconds ~ season * scale(total_human_sec) * scale(dist_jalama)
                          + (1|sitename), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail") 

glmm_full_add <- glmmTMB(activity_seconds ~ season + total_human_sec + dist_jalama
                          + (1|sitename) , 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_full_seasxhum <- glmmTMB(activity_seconds ~ season * total_human_sec + dist_jalama
                          + (1|sitename) , 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_full_seasxdist <- glmmTMB(activity_seconds ~ season * dist_jalama + total_human_sec 
                          + (1|sitename) , 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_full_distxhum <- glmmTMB(activity_seconds ~ season + dist_jalama * total_human_sec 
                          + (1|sitename) , 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

######### MULTIPLE COVARIATE MODELS ######### 

glmm_1x <- glmmTMB(activity_seconds ~ dist_jalama * total_human_sec 
                          + (1|sitename) + (1|property), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail") # necessary for dredge to work

glmm_1a <- glmmTMB(activity_seconds ~ dist_jalama + total_human_sec 
                          + (1|sitename) + (1|property), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_2x <- glmmTMB(activity_seconds ~ season * total_human_sec 
                          + (1|sitename) + (1|property), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail") # necessary for dredge to work

glmm_2a <- glmmTMB(activity_seconds ~ season + total_human_sec 
                          + (1|sitename) + (1|property), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_3x <- glmmTMB(activity_seconds ~ season * dist_jalama 
                          + (1|sitename) + (1|property), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail") # necessary for dredge to work

glmm_3a <- glmmTMB(activity_seconds ~ season + dist_jalama 
                          + (1|sitename) + (1|property), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

######### SINGLE COVARIATE MODELS ######### 

glmm_season <- glmmTMB(activity_seconds ~ season 
                          + (1|sitename) + (1|property), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_dist <- glmmTMB(activity_seconds ~ dist_jalama
                          + (1|sitename) + (1|property), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_human <- glmmTMB(activity_seconds ~ total_human_sec 
                          + (1|sitename) + (1|property), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

####### attempting to make a model table ####### 
bbmle::AICctab(glmm_full_inx, glmm_full_add, glmm_full_seasxhum, glmm_full_seasxdist, glmm_full_distxhum, glmm_1x, glmm_1a, glmm_2x, glmm_2a, glmm_3x, glmm_3a, glmm_dist, glmm_human, glmm_season)

#cand.set <- c(glmm_full_inx, glmm_full_add, glmm_1x, glmm_1a, glmm_2x, glmm_2a, glmm_3x, glmm_3a, glmm_dist, glmm_human, glmm_season)

#cand.set <- c(glmm_null_randoms, glmm_season)
#model_names <- c('null', 'season')

#model_names <- c( 'full crossed model', 'full additive model', 'dist_jalama * total_human_sec ', 'dist_jalama + total_human_sec ','season * total_human_sec', 'season + total_human_sec', 'season * dist_jalama', 'season + dist_jalama', 'dist_jalama', 'human_sec', 'season')


#AICcmodavg::aictab(cand.set, second.ord = TRUE, nobs = NULL, sort = TRUE, c.hat = 1)
#AIC(cand.set)
#AICctab(cand.set, mnames = model_names, base = TRUE, weights = FALSE, logLik = TRUE, nobs = NULL)



simulationOutput <- simulateResiduals(fittedModel = glmm_full_off)
plot(simulationOutput)
plotResiduals(simulationOutput)
testZeroInflation(simulationOutput)
#testTemporalAutocorrelation(simulationOutput)

### trying a zero inflated version, apparently this is how you code it in?? source:
# https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html#interpreting-residuals-and-recognizing-misspecification-problems

#glmm_full_ZI <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds * common_name 
#                          + (1|sitename) + (1|property) + offset(log10(trapnights)), 
#                         data = activity_site_month_sp, 
#                         family = nbinom2(link = "log"), 
#                         ziformula = ~ dist_jalama + human_seconds + common_name,
#                         na.action = "na.fail")
#summary(glmm_full_ZI)
#res<- simulateResiduals(glmm_full_ZI, plot = T)

plot(allEffects(glmm_full_off))
r.squaredGLMM(glmm_full_off)

```

## First time we did this, we used the data binned by month. Now, 9/3, we're using data binned by week
So going through and replacing df: site_month_sp with _7day (y from here on out will be )
```{r glmm activity model selection 7 day sample}

# we know we need a mixed model because we need a random nesting effect of property and site x month
# variables of interest are monthly human seconds, season, distance from jalama, species, and interactions between all
# number of trapnights logged as an offset -- not sure why we log this but other papers do it (because there's a log link for the rest of the function)

incomplete <- activity_7day %>% 
  filter(!complete.cases(.)) #make sure every row has all data (no NAs)

# real random effect here, in this nested design, would be SITE as the thing that is repeatedly sampled, but site = distance from jalama and is the variable we are interested in...
# week/survey_id is also nested within site (and repeatedly sampled), so is a random variable, but it's the level of observation atm since we binned (summed) all occurences into weekly sums - i don't believe we need to include the level of sampling as an error term

# many models standardize their explanatory variables... why?
## found the answer here: https://stats.stackexchange.com/questions/35071/what-is-rank-deficiency-and-how-to-deal-with-it/35077#35077
### "The trick is usually to use common units, but on some problems even that is an issue when variables vary by too many orders of magnitude. More important is to scale your numbers to be similar in magnitude."

############### VARS ############### 

# RESPONSE variable is activity_seconds, offset is weekly_trapnights
# SAMPLES are 4 weeks x 9 months x 30 sites per species (although a lot of zeros)
# RANDOM variable is survey_id
# potential EXPLANATORY variables are:
## property: VSFB, JLDP, Jalama ## rank deficient
## month (going to see if this is sig and then replace it with an explanatory variable)
## common_name (species) ## rank deficient
## iz_type ## rank deficient
## dist_jalama
## human_seconds

glmm_null <- glmmTMB(activity_seconds ~ 1, 
                     data = activity_7day, 
                     family = nbinom2(link = "log"), 
                     offset = log10(weekly_trapnights), 
                         na.action = "na.fail")

glmm_null_randoms <- glmmTMB(activity_seconds ~ 1 + (1|sitename), 
                             data = activity_7day, 
                             family = nbinom2(link = "log"), 
                             offset = log10(weekly_trapnights), 
                         na.action = "na.fail")

glmm_full_inx <- glmmTMB(activity_seconds ~ month + human_seconds + dist_jalama +
                         #inxs
                           (month * human_seconds) +
                           (month * dist_jalama) +
                           (dist_jalama * human_seconds) +
                          + (1|sitename), 
                         data = activity_7day, 
                         family = nbinom2(link = "log"), 
                         offset = log10(weekly_trapnights), 
                         na.action = "na.fail") # necessary for dredge to work

glmm_full_add <- 


glmm_1 <- glmmTMB(activity_seconds ~ dist_jalama + human_seconds 
                          + (1|sitename) + (1|property), 
                         data = activity_7day, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_2 <- glmmTMB(activity_seconds ~ dist_jalama
                          + (1|sitename) + (1|property), 
                         data = activity_7day, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_3 <- glmmTMB(activity_seconds ~ human_seconds 
                          + (1|sitename) + (1|property), 
                         data = activity_7day, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_full_off <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds #* common_name 
                          + (1|sitename) + (1|property), 
                         data = activity_7day, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail") # necessary for dredge to work

#glmm_5 <- glmmTMB(activity_seconds ~ dist_jalama + human_seconds + protection_rank #* common_name 
#                          + (1|sitename), 
#                         data = activity_7day, 
#                         family = nbinom2(link = "log"), 
#                         offset = log10(trapnights), 
#                         na.action = "na.fail") 

#glmm_full_off <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds * protection_rank #* common_name 
#                          + (1|sitename), 
#                         data = activity_7day, 
#                         family = nbinom2(link = "log"), 
#                         offset = log10(trapnights), 
#                         na.action = "na.fail") # necessary for dredge to work

activity_models <- c(glmm_full_off, glmm_1, glmm_2, glmm_3, glmm_null_randoms, glmm_null_randoms1, glmm_null_randoms2)
model_names <- c('glmm_full_off', 'glmm_1', 'glmm_2', 'glmm_3', 'glmm_null_rndms', 'glmm_null_rndms1','glmm_null_rndms2')
#aictab(cand.set = activity_models)

summary(glmm_full_off)
#offset(activity_7day$days_active)

simulationOutput <- simulateResiduals(fittedModel = glmm_full_off)
plot(simulationOutput)
plotResiduals(simulationOutput)
testZeroInflation(simulationOutput)
#testTemporalAutocorrelation(simulationOutput)

### trying a zero inflated version, apparently this is how you code it in?? source:
# https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html#interpreting-residuals-and-recognizing-misspecification-problems

#glmm_full_ZI <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds * common_name 
#                          + (1|sitename) + (1|property) + offset(log10(trapnights)), 
#                         data = activity_7day, 
#                         family = nbinom2(link = "log"), 
#                         ziformula = ~ dist_jalama + human_seconds + common_name,
#                         na.action = "na.fail")
#summary(glmm_full_ZI)
#res<- simulateResiduals(glmm_full_ZI, plot = T)

plot(allEffects(glmm_full_off))
r.squaredGLMM(glmm_full_off)

```


## Time to switch to by-species analysis
### First we can look at which species and explanatory variables are correlated
```{r correlation plot}
# first we need a wide form of the data so we have the right info for every row!
# good thing we already made one

activity_corrs1 <- activity_wide %>% 
  select(!burst_settings)
activity_corrs <- cor(select_if(activity_corrs1, is.numeric))  # cor and corrplot can only deal with numeric values, makes sense
activity_corrs2 <- cor(select_if(activity_wide_adj, is.numeric)) # same thing but adjusted for effort

corrplot(activity_corrs,
         type = "upper",
         #method = "ellipse",
         tl.col = "black",
         tl.cex = 0.5)

corrplot(activity_corrs2,
         type = "upper",
         method = "ellipse",
         tl.col = "black",
         tl.cex = 0.5)
#large dark circles = large positive correlations
#small light circles = small negative correlations

```

### Let's model coyote activity first since they're the most common
```{r coyote glmm}

# already made a yote df, need to update it tho
coyote_activity <- activity_site_month_sp %>% 
  filter(common_name == "coyote")

# how does the data look
ggplot(coyote_activity, aes(x = activity_seconds)) +
  geom_histogram()

# poisson diagnostics NOT good, def a neg binom situation
#coyote_full <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds #+ (1|sitename) 
#                     + (1|property), # swapping from glmmTMB to glmer to see if it'll let us make an AIC table
#                         data = coyote_activity, 
#                         family = poisson(link = "log"), 
#                         offset = log10(trapnights), 
#                         #na.action = "na.fail",# necessary for dredge to work
#                       na.action = "na.exclude"
#                       ) 

coyote_full <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds + (1|sitename) 
                     + (1|property), # swapping from glmmTMB to glmer to see if it'll let us make an AIC table
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 


coyote_null_rndms <- glmmTMB(activity_seconds ~ 1 + (1|sitename) + (1|property), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_null_rndms1 <- glmmTMB(activity_seconds ~ 1 + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_null_rndms2 <- glmmTMB(activity_seconds ~ 1 + (1|property), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_null <- glmmTMB(activity_seconds ~ 1, 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_add <- glmmTMB(activity_seconds ~ dist_jalama + human_seconds + (1|sitename) + (1|property), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                       na.action = "na.exclude"
                       )


coyote_1 <- glmmTMB(activity_seconds ~ dist_jalama + (1|sitename) + (1|property), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_2 <- glmmTMB(activity_seconds ~ human_seconds + (1|sitename) + (1|property), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 


# test assumptions on full model
simulationOutput_yote <- simulateResiduals(fittedModel = coyote_full)
plot(simulationOutput_yote)
plotResiduals(simulationOutput_yote)
testZeroInflation(simulationOutput_yote)

#coyote_models <- c(coyote_full, coyote_1, coyote_2, coyote_null_rndms, coyote_null_rndms1, coyote_null_rndms2)
#coyote_names <- c('coyote_full', 'coyote_1', 'coyote_2', 'coyote_null_rndms', 'coyote_null_rndms1','coyote_null_rndms2')
#summary(coyote_models)
#aictab(cand.set = coyote_models)
#AIC(coyote_models)

plot(allEffects(coyote_add))
r.squaredGLMM(coyote_add)
```
### now pigs
```{r pig glmm}

# already made a yote df, need to update it tho
pig_activity <- activity_site_month_sp %>% 
  filter(common_name == "feral pig")

# how does the data look
ggplot(pig_activity, aes(x = activity_seconds)) +
  geom_histogram()

pig_full <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds + (1|sitename) + (1|property), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 


pig_null_rndms <- glmmTMB(activity_seconds ~ 1 + (1|sitename) + (1|property), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_null_rndms1 <- glmmTMB(activity_seconds ~ 1 + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_null_rndms2 <- glmmTMB(activity_seconds ~ 1 + (1|property), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_null <- glmmTMB(activity_seconds ~ 1, 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_add <- glmmTMB(activity_seconds ~ dist_jalama + human_seconds + (1|sitename) + (1|property), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_1 <- glmmTMB(activity_seconds ~ dist_jalama + (1|sitename) + (1|property), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_2 <- glmmTMB(activity_seconds ~ human_seconds + (1|sitename) + (1|property), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

summary(pig_full)

# test assumptions on full model
simulationOutput_pig <- simulateResiduals(fittedModel = pig_full)
plot(simulationOutput_pig)
plotResiduals(simulationOutput_pig)
testZeroInflation(simulationOutput_pig)

plot(allEffects(pig_full))
r.squaredGLMM(pig_full)

#pig_models <- c(pig_full, pig_null, pig_1, pig_2, pig_null_rndms, pig_null_rndms1, pig_null_rndms2)
#pig_names <- c('pig_full', 'pig_null', 'pig_1', 'pig_2', 'pig_null_rndms', 'pig_null_rndms1','pig_null_rndms2')

```
### finally for deer
```{r deer models}

# already made a yote df, need to update it tho
deer_activity <- activity_site_month_sp %>% 
  filter(common_name == "mule deer")

# how does the data look
ggplot(deer_activity, aes(x = activity_seconds)) +
  geom_histogram()

deer_full <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds + (1|sitename) + (1|property), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 


deer_null_rndms <- glmmTMB(activity_seconds ~ 1 + (1|sitename) + (1|property), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_null_rndms1 <- glmmTMB(activity_seconds ~ 1 + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_null_rndms2 <- glmmTMB(activity_seconds ~ 1 + (1|property), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_null <- glmmTMB(activity_seconds ~ 1, 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_add <- glmmTMB(activity_seconds ~ dist_jalama + human_seconds + (1|sitename) + (1|property), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_1 <- glmmTMB(activity_seconds ~ dist_jalama + (1|sitename) + (1|property), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_2 <- glmmTMB(activity_seconds ~ human_seconds + (1|sitename) + (1|property), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

summary(deer_1)
summary(deer_full)

plot(allEffects(deer_add))
r.squaredGLMM(deer_add)

```
# now we look at richness with the same predictor variables
```{r richness model}

# we can use the activity_wide matrix to calculate richness
richness_matrix <- activity_wide %>% 
  select(!c(property, protection_rank, sitename, dist_jalama, month, human_seconds, trapnights, burst_settings)) %>% 
  column_to_rownames("sitemonth_id")  #this is important for separating site names from count data (for matrix reasons)

# sp.count from vegan calculates richness
sp.count <- specnumber(richness_matrix) # vector

rich_df <- sp.count %>% 
  enframe()%>% 
  rename(sitemonth_id = name,
         richness = value)

# vegan::diversity() will calculate shannon (default), simpson, and fisher indices
shannondiv <- diversity(richness_matrix) 
# again creates named vector 

# attach it back to the dataframe
diversity_df <- shannondiv %>% 
  # convert named vector to dataframe
  enframe() %>% 
  rename(sitemonth_id = name,
         sh_diversity = value) %>% 
  
# join with metadata: this joins shandiv_df to site_type matching shandiv_df$name to site_type$site
  full_join(., activity_site_month_sp, by = join_by(sitemonth_id)) %>%
  full_join(., rich_df, by = join_by(sitemonth_id)) %>% 
  distinct(sitename, month, sh_diversity, .keep_all = TRUE)

# how does richness data look?
ggplot(diversity_df, aes(x = richness)) +
  geom_histogram()

# how does shanvid data look?
ggplot(diversity_df, aes(x = sh_diversity)) +
  geom_histogram()
# wow it's almost normal, but with a bunch of zeros

################# model time ###################

richness_pois <- glmmTMB(richness ~ dist_jalama * human_seconds + (1|sitename) + (1|property), 
                         data = diversity_df, 
                         family = poisson(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

richness_nb <- glmmTMB(richness ~ dist_jalama * human_seconds + (1|sitename) + (1|property), 
                         data = diversity_df, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

richness_gaus <- glmmTMB(richness ~ dist_jalama * human_seconds + (1|sitename) + (1|property), 
                         data = diversity_df, 
                         family = gaussian(link = "identity"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

richness_null_rndms <- glmmTMB(richness ~ 1 + (1|sitename) + (1|property), 
                         data = diversity_df, 
                         family = gaussian(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

richness_null_rndms1 <- glmmTMB(richness ~ 1 + (1|sitename), 
                         data = diversity_df, 
                         family = gaussian(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

richness_null_rndms2 <- glmmTMB(richness ~ 1 + (1|property), 
                         data = diversity_df, 
                         family = gaussian(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

richness_null <- glmmTMB(richness ~ 1, 
                         data = diversity_df, 
                         family = gaussian(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

richness_add <- glmmTMB(richness ~ dist_jalama + human_seconds + (1|sitename) + (1|property), 
                         data = diversity_df, 
                         family = gaussian(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

richness_1 <- glmmTMB(richness ~ dist_jalama + (1|sitename) + (1|property), 
                         data = diversity_df, 
                         family = gaussian(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

richness_2 <- glmmTMB(richness ~ human_seconds + (1|sitename) + (1|property), 
                         data = diversity_df, 
                         family = gaussian(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 
  

# having trouble with model fit, doesn't predict the data well even though they are counts (of species)
simulationOutput_rich <- simulateResiduals(fittedModel = richness_gaus)
plot(simulationOutput_rich)
plotResiduals(simulationOutput_rich)
testZeroInflation(simulationOutput_rich)

#summary(richness_1)
#summary(richness_add)
summary(richness_gaus)
summary(richness_add)

r.squaredGLMM(richness_add)
r.squaredGLMM(richness_gaus)

plot(allEffects(richness_1))
plot(allEffects(richness_add))
plot(allEffects(richness_nb))
plot(allEffects(richness_gaus))

```
### Finally we can make some pretty prediction figures
```{r activity prediction graph}

all_activity_response <- #plot( # plot function is helpful but i want ot ggplot
  data.frame(predict_response(glmm_full_off, terms = c("dist_jalama", "human_seconds [0, 100, 200]"))) # old syntax was ggpredict, now it's predict_response, brackets indicate desired values of factor
  #, 
                          #facets = TRUE, 
                          #show_data = TRUE)

activity_plot <- ggplot(all_activity_response, aes (x, predicted, colour = group)) +
  geom_line()+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), linetype = 0, alpha = 0.1)+
  xlab("Distance From Jalama Beach (km)")+
  ylab("Predicted Mammal Activity (seconds)")+
  scale_fill_discrete(name = "", 
                      breaks = c(0,100,200),
                       labels = c("No Human Activity", "Moderate Human Activity (100 seconds)", "High Human Activity (200 seconds)"))+
  scale_color_discrete(name = "",
                       breaks = c(0,100,200),
                       labels = c("No Human Activity", "Moderate Human Activity (100 seconds)", "High Human Activity (200 seconds)"))+
  #theme(legend.title = element_blank())+
  theme_classic()
  #geom_jitter(data = activity_site_month_sp, aes(x = dist_jalama, y = activity_seconds, colour = property))

#png(here("figures/total_activity_model.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#activity_plot
#dev.off()

r.squaredGLMM(glmm_full_off)
r.squaredGLMM(glmm_2)

```
```{r coyotes prediction graph}

############# ~ DISTANCE #############
coyote_dist_response <- data.frame(predict_response(coyote_add, terms = c("dist_jalama"))) 

coyote_dist_plot <- ggplot(coyote_dist_response, aes (x, predicted)) +
  geom_line(colour = "#1C8356")+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1, fill = "#1C8356")+
  xlab("Distance From Jalama Beach (km)")+
  ylab("Predicted Coyote Activity (seconds)")+
  theme_classic()
  #geom_jitter(data = coyote_activity, aes(x = dist_jalama, y = activity_seconds, colour = property))

#png(here("figures/coyote_activity_dist.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#coyote_dist_plot
#dev.off()

############# ~ HUMAN #############
coyote_human_response <- data.frame(predict_response(coyote_add, terms = c("human_seconds"))) 

coyote_human_plot <- ggplot(coyote_human_response, aes (x, predicted)) +
  geom_line(colour = "#1C8356")+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1, fill = "#1C8356")+
  xlab("Human Activity (seconds)")+
  ylab("Predicted Coyote Activity (seconds)")+
  theme_classic()
  #geom_jitter(data = coyote_activity, aes(x = dist_jalama, y = activity_seconds, colour = property))

#png(here("figures/coyote_activity_human.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#coyote_human_plot
#dev.off()

r.squaredGLMM(coyote_add)
r.squaredGLMM(coyote_1)

```
```{r deer prediction graph}

############# ~ DISTANCE #############
deer_dist_response <- data.frame(predict_response(deer_add, terms = c("dist_jalama"))) 

deer_dist_plot <- ggplot(deer_dist_response, aes (x, predicted)) +
  geom_line(colour = "#1CBE4F")+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1, fill = "#1CBE4F")+
  xlab("Distance From Jalama Beach (km)")+
  ylab("Predicted Deer Activity (seconds)")+
  theme_classic()
  #geom_jitter(data = deer_activity, aes(x = dist_jalama, y = activity_seconds, colour = property))

#png(here("figures/deer_activity_dist.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#deer_dist_plot
#dev.off()

############# ~ HUMAN #############
deer_human_response <- data.frame(predict_response(deer_add, terms = c("human_seconds"))) 

deer_human_plot <- ggplot(deer_human_response, aes (x, predicted)) +
  geom_line(colour = "#1CBE4F")+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1, fill = "#1CBE4F")+
  xlab("Human Activity (seconds)")+
  ylab("Predicted Deer Activity (seconds)")+
  theme_classic()
  #geom_jitter(data = deer_activity, aes(x = dist_jalama, y = activity_seconds, colour = property))

#png(here("figures/deer_activity_human.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#deer_human_plot
#dev.off()

r.squaredGLMM(deer_add)
r.squaredGLMM(deer_1)

```

```{r pig prediction graph}

############# ~ DISTANCE #############
pig_response_dist <- data.frame(predict_response(pig_full, terms = "dist_jalama"))

pig_dist_plot <- ggplot(pig_response_dist, aes (x, predicted)) +
  geom_line(colour = "#16FF32")+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1, fill = "#16FF32")+
  xlab("Distance From Jalama Beach (km)")+
  ylab("Predicted Pig Activity (seconds)")+
  theme_classic()
  #geom_jitter(data = deer_activity, aes(x = dist_jalama, y = activity_seconds, colour = property))

#png(here("figures/pig_activity_dist.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#pig_dist_plot
#dev.off()

############# ~ HUMAN #############
pig_response_humans <- data.frame(predict_response(pig_full, terms = "human_seconds"))

pig_human_plot <- ggplot(pig_response_humans, aes (x, predicted)) +
  geom_line(colour = "#16FF32")+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1, fill = "#16FF32")+
  xlab("Human Activity (seconds)")+
  ylab("Predicted Pig Activity (seconds)")+
  theme_classic()
  #geom_jitter(data = deer_activity, aes(x = dist_jalama, y = activity_seconds, colour = property))

#png(here("figures/pig_activity_human.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#pig_human_plot
#dev.off()

############# ~ DISTANCE * HUMAN #############
# make a histogram to pick relevant levels of human activity
human_levels <- ggplot(activity_site_month_sp, aes(x = human_seconds))+
  geom_histogram(bins = 30)

pig_response <- data.frame(predict_response(pig_full, terms = c("dist_jalama", "human_seconds [0, 100, 300]")))

pig_inxn_plot <- ggplot(pig_response, aes(x, predicted, linetype = group)) +
  geom_line(colour = "#16FF32")+
  #geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), linetype = 0, alpha = 0.1)+
  xlab("Distance From Jalama Beach (km)")+
  ylab("Predicted Pig Activity (seconds)")+
  scale_linetype_discrete(name = "", 
                      breaks = c(0,100,300),
                       labels = c("No Human Activity", "Moderate Human Activity (100 seconds)", "High Human Activity (300 seconds)"))+
  #scale_fill_discrete(name = "",
  #                     breaks = c(0,100,300),
  #                     labels = c("No Human Activity", "Moderate Human Activity (100 seconds)", "High Human Activity (300 seconds)"))+
  theme_classic()
  #geom_jitter(data = pig_activity, aes(x = dist_jalama, y = activity_seconds, colour = property))

#png(here("figures/pig_activity_inxn.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#pig_inxn_plot
#dev.off()

r.squaredGLMM(pig_full)
r.squaredGLMM(pig_1)

```
#### Making a combo plot for all species responses
```{r combo species activity}
# using same colors as before:
IDcolors <- alphabet.colors(15)
# coyote is 'forest' #1C8356
# pig is 'green' #16FF32
# deer is 'jade' #1CBE4F

pig_response_dist <- data.frame(predict_response(pig_full, terms = "dist_jalama"))
pig_response_humans <- data.frame(predict_response(pig_full, terms = "human_seconds"))

all_spp_humans <- ggplot()+ # something is funky so i'm having to include 'mapping =' in front of aes() to get it to recognize
  geom_line(coyote_human_response, mapping = aes(log10(x), log10(predicted)), colour = "#1C8356")+
  #geom_ribbon(coyote_human_response, mapping = aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1)+
  
  geom_line(pig_response_humans, mapping = aes(x = log10(x), y = log10(predicted)), colour = "#16FF32")+
  #geom_ribbon(pig_response_humans, mapping = aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1)+
  
  geom_line(deer_human_response, mapping = aes(x = log10(x), y = log10(predicted)), colour = "#1CBE4F")+
  #geom_ribbon(deer_human_response,mapping =  aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1)+
  
  xlab("Distance From Jalama Beach (km)")+
  ylab("Predicted Mammal Activity (seconds)")+
  theme_classic()

# ok this sucks even when log transformed

```
#### Finally plotting richness predictions
```{r richenss response}

############# ~ DISTANCE #############
richness_dist_response <- data.frame(predict_response(richness_gaus, terms = c("dist_jalama"))) 

richness_dist_plot <- ggplot(richness_dist_response, aes (x, predicted)) +
  geom_line()+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1)+
  geom_jitter(data = diversity_df, aes(x = dist_jalama, y = richness, colour = property))+
  xlab("Distance From Jalama Beach (km)")+
  ylab("Predicted Richness")+
  ylim(0, 6) +
  theme_classic()
  

#png(here("figures/richness_activity_dist.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#richness_dist_plot
#dev.off()

############# ~ HUMAN #############
richness_human_response <- data.frame(predict_response(richness_add, terms = c("human_seconds"))) 

richness_human_plot <- ggplot(richness_human_response, aes (x, predicted)) +
  geom_line(colour = "#1CBE4F")+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1, fill = "#1CBE4F")+
  xlab("Human Activity (seconds)")+
  ylab("Predicted richness Activity (seconds)")+
  theme_classic()
  #geom_jitter(data = richness_activity, aes(x = dist_jalama, y = activity_seconds, colour = property))

#png(here("figures/richness_activity_human.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#richness_human_plot
#dev.off()

r.squaredGLMM(richness_add)
r.squaredGLMM(richness_1)
r.squaredGLMM(richness_gaus)

```



# Leaving the world of Frequentist Stats now...
#### We need to bin the data into 1 day "surveys" so that we can run a multi-species occupancy model using spOccupancy and have multiple samples per month, also need to plug in zeros
```{r make new dataframe with 1 day samples}
# marisa is using a different dataset for the bayesian stuff that I made her - it includes daily human counts (although a lot fewer than our estimates suggest)
## this is the one I made for her
mammals_humans <- read_csv(here("data/classifications_w_humans_metadata_full_9aug2024.csv")) %>% 
  mutate(date = mdy(date)) %>% 
  rename(trapnights = days_per_month)

all_days_sampled # will need this dataframe

all_days_sampled2 <- all_days_sampled %>% 
  mutate(date = date(each_date)) %>% # hopefully this column will let us full_join on date
  mutate(year = year(each_date)) %>% 
  mutate(day = day(each_date)) %>% 
  mutate(number_of_objects = 0) %>% 
  rename(timestamp = each_date) %>% 
  unite("sitemonth_id", c(sitename, month), sep = ".", remove = FALSE) %>% 
  select(property, deployment_id, sitename, month, sitemonth_id, year, day, date, number_of_objects, timestamp)  %>% 
  #mutate(check = gl(7, 1, length = length(site_sample_id))) # gl is apparently a way to generate a sequence that i'll never remember (7 integers, repeat each integer once, until end of df) don't need this anymore

# still need to add in metadata to this so it's filled in correctly during the full bind, including dist, habitats, lat lon, iz type, burst settings, humans, and trapnights
  left_join(., metadata_by_month, by = join_by(property, sitename, month, sitemonth_id)) %>% 
  select(!c(human_seconds, n_human_shots)) # will calculate this from human data in dataframe


mammals_occupancy_full <- mammals_humans %>% 
  full_join(., all_days_sampled2 #, by = join_by(property, deployment_id, sitename, month, sitemonth_id, year, day, date, number_of_objects, timestamp) # so many columns to bind on, but it should automatically select all the similar ones
            ) 

#write_csv(mammals_occupancy, "mammals_occupancy.csv")

mammals_occupancy_byday <- mammals_occupancy_full %>% 
  group_by(property, sitename, dist_jalama, utm_x, utm_y, habitat_adjacent, habitat_secondary, iz_type, burst_settings, # site specific grouping vars
           month, sitemonth_id, trapnights, # month specific grouping vars
           date, # day specific grouping vars
           common_name # need sums by species
           ) %>% 
  summarise(raw_daily_counts = sum(number_of_objects)) %>% 
  
  # now we need to pivot to remove NA rows and insert zeroes
  
  pivot_wider(names_from = common_name, 
              #names_sep = "_",
              names_prefix = "sp_",
              values_from = raw_daily_counts, 
              values_fill = 0) %>% 
  select(!sp_NA) %>% # we no longer need this columns now that all the zeros are filled in, so we should be able to pivot longer now and get a count of spp for every day sampled
  pivot_longer(cols = starts_with("sp_"),
               names_to = "common_name",
               names_prefix = "sp_",
               values_to = "raw_daily_counts") %>% 
  mutate(binary_occupancy = case_when(raw_daily_counts > 1 ~ 1,
                                      raw_daily_counts == 0 ~ 0)) # creates a binary column for occupancy

write_csv(mammals_occupancy_byday, here("data/mammals_daily_occupancy.csv"))

```


# graveyard

### Visualize Data ~ Point Conception
```{r plot diffs above and below PC, eval=FALSE, include=FALSE}

pc_violin <- ggplot(activity_site_month_sp, aes(x = point_conception, y = activity_seconds)) +
  geom_violin()

pc_boxplot <- ggplot(activity_site_month_sp, aes(x = point_conception, y = activity_seconds)) +
  geom_boxplot()

```
#### Run a T-Test
```{r point conception t test, eval=FALSE, include=FALSE}

activity_site_month_ttest <- activity_site_month_sp %>% 
  filter(!point_conception == "north of Jalama")

t.test(activity_seconds ~ point_conception, data = activity_site_month_ttest)

```
#### Full community t-test was non-sig so investigating species by species//some groups
```{r pc hypothesis with spp and groups, eval=FALSE, include=FALSE}

######### make table of animal activity ( = incidences adjusted by burst number and corrected for by trap nights) by site and month ###########

activity_site_month_sp <- mammals_clean %>% # first append monthly trapnights to full dataset
  group_by(sitename, month, dist_jalama, days_active, burst_settings, utm_x, utm_y, common_name) %>% 
  summarize(total_captures = n(), # count number of rows (observations) in each category
            ) %>% # had a hard time with the other columns so I'm tossing them out into a mutate:
  mutate(activity_seconds = ceiling(total_captures/burst_settings)) %>%  # adjusts # of captures by the number of images taken in a burst (8 or 10)), rounds them UP to integers for glms
  mutate(seconds_by_trapnight = activity_seconds/as.numeric(days_active)) %>% 
  # we're interested in whether sites are above or below point conception (south of jalama) # because people can't really walk past it from jalama, so we will mutate in a column
  mutate(point_conception = case_when(utm_y < -120.452725 & utm_y > -120.506037 ~ "above", #lon of PC and south of north jalama cam
                                      utm_y > -120.452725 ~ "below",
                                      utm_y < -120.506037 ~ "north of Jalama")) # lon of north jalama cam
  
  
######################################### visualize ###############################################

pc_violin_byspp <- ggplot(activity_site_month_sp, aes(x = point_conception, y = activity_seconds)) +
  geom_violin()+
  facet_wrap(~common_name)

pc_boxplot_byspp <- ggplot(activity_site_month_sp, aes(x = point_conception, y = activity_seconds)) +
  geom_boxplot()+
  facet_wrap(~common_name)

########################################### coyotes only #############################################
coyote_activity <- activity_site_month_sp %>% 
  filter(common_name == "coyote") #%>% 
  filter(!point_conception == "north of Jalama")

t.test(activity_seconds ~ point_conception, data = coyote_activity)


```

## We're going to change gears and use some ready-made packages to understand activity levels

### Specifically the 'activity' package
```{r playing with activity}
require(activity)

### following this guide: https://bookdown.org/c_w_beirne/wildCo-Data-Analysis/activity.html


# adding solar time to the dataset to account for sunrise and sunset (Vazquez, Carmen, et al. “Comparing diel activity patterns of wildlife across latitudes and seasons: Time transformations using day length.” Methods in Ecology and Evolution 10.12 (2019): 2057-2066.)
#mammals_predictors1 <- mammals_clean %>% 
#  unite("timestamp", c(date, time))

#solart <- solartime(ymd_hms(mammals_predictors1$timestamp, tz="UTC"),
#                           mammals_predictors1$utm_x, 
#                           mammals_predictors1$utm_y,
#                           tz = 7, # an offset in numeric hours to UTC
#                           format = "%Y-%m-%d %H:%M:%S")

#mammals_predictors <- mammals_predictors1 %>% 
#  mutate(solar = solart$solar) %>% 
#  mutate(clock = solart$clock)

#plot(mammals_predictors$solar, mammals_predictors$clock)

# Fit an activity model
#m1 <- fitact(mammals_predictors$solar[mammals_predictors$common_name=="coyote"], sample="model", reps=100, show = TRUE) # reps are number of bootstraps, we're sampling the data instead of the model, show = TRUE shows a progress bar while bootstrapping
#plot(m1)

```








