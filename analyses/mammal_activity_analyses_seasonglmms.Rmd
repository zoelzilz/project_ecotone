---
title: "Mammunity vs Humans"
output: html_document
date: "2024-07-23"
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
# setting all chunks not to display anything but output so i can easily knit and send to hillary

library(tidyverse)
library(car)
library(ggpubr)
library(ggprism) # paste pvalues
library(geosphere)
library(lubridate)
library(hms)
library(janitor)
library(kableExtra)
library(formattable)
library(webshot2)
library(vegan)
library(here)
library(RColorBrewer)
library(rstatix)
library(Polychrome) #package to make palettes witha shit ton of distinct colors
library(MuMIn)
library(glmmTMB)
library(lme4)
library(corrplot)
library(DHARMa)
library(AICcmodavg)
library(ggeffects)
library(effects)
library(bbmle)
library(prism)
library(zoo)

```

# Catch All Markdown for Mammal Community and Activity Summary Figures and Stats
### Behind the Scenes
#### Pull and Clean Data
```{r data import and cleaning}
#### data versioning log ###
# 23 jul 2024: had to manually paste in some NVS photos with the correct date 
# 31 jul using what's hopefully final version, there will be new date issues I think
# 7 aug incorporating real human data, although the humans are IDd with computer vision it's very reliable so we're using it! same date issues as last version i hope


# let's see what kind of mess we're working with
mammals_unclean <- read_csv(here("data/WI_data_7aug2024/classifications_7aug2024.csv"))
mammals_prev <- read_csv(here("data/WI_data_31jul2024/image_classifications_31jul2024.csv")) 


#unique(mammals_unclean$behavior)

mammals <- mammals_prev %>%
  filter(class == "Mammalia") %>% 
  #filter(!identified_by == "Computer vision") %>%  # need to retain these because they're people pics
  
  # ugh annoying unclean inconsistent capitals
  mutate(filename = tolower(filename)) %>% 
  mutate(common_name = tolower(common_name)) %>% 
  
  # ugh ugh the filenames where i forgot to put a "_"
  mutate(filename = str_replace(filename, "pp17apr22", "pp_17apr22")) %>% 
  mutate(filename = str_replace(filename, "boatcam17apr22", "boat_17apr22")) %>% 
  mutate(filename = str_replace(filename, "pb28apr22", "pb_28apr22")) %>% 
  mutate(filename = str_replace(filename, "gov17apr22", "gov_17apr22")) %>% 
  mutate(filename = str_replace(filename, "nvs28apr22", "nvs_28apr22")) %>% 
  mutate(filename = str_replace(filename, "pl17apr22", "pl_17apr22")) %>%  
  mutate(filename = str_replace(filename, "cove17apr22", "cove_17apr22")) %>% 
  mutate(filename = str_replace(filename, "bc17apr22", "bc_17apr22")) %>%
  mutate(filename = str_replace(filename, "bc28apr22", "bc_28apr22")) %>%
  mutate(filename = str_replace(filename, "pd28apr22", "pd_28apr22")) %>% 
  mutate(filename = str_replace(filename, "gov_12nov22", "big_12nov22")) %>%
  mutate(filename = str_replace(filename, "gov_17sep", "big_17sep")) %>%

    # also inconsistent sitenames:
  mutate(sitename = case_when(deployment_id == "Boneyard 09/07/2022" ~ "Boneyard Cam",
                              deployment_id == "Old Fencepost Cam April 2022" ~ "Old Fencepost Cam",
                              deployment_id == "Percos Post March 2022" ~ "Percos Post Cam",
                              deployment_id == "seawall cam" ~ "Seawall Cam",
                              deployment_id == "Boathouse May - July 2022" ~ "Boathouse Cam",
                              deployment_id == "Saucito" ~ "Saucito Cam",
                              .default = as.character(deployment_id))) %>% 
  
  mutate(deployment_id = paste0(str_extract(filename, "[^_]*_[^_]*"))) %>%  # return only the first 2 substrings (e.g. cam name and dmy) of the filenames - this is very sloppy and I hate it but oh well
  
  #English translation:
    # [^_]* = as many non-underscore characters as possible
    # _ = an underscore
    # [^_]* = as many non-underscore characters as possible
    # [...] is a character class. [abc] means "a or b or c", and [^abc] means anything but a or b or c.
  
  # editing the csv messed up the dates which were so nice before
  #mutate(timestamp = as.POSIXct(timestamp, tz=Sys.timezone()))# %>% # THIS IS SUPER INCONSISTENT, SOMETIMES THE DATA IMPORTS WITH NORMAL DATES, SOMETIMES NOT
  
  ## main thing to do is clean up the behavior column which is a mess. Specifically need to separate all of the "holding" etc behaviors from the item
  mutate(behavior = tolower(behavior)) %>% 
  mutate(behavior_clean = str_replace(behavior, "carrying", "carrying;")) %>% 
  mutate(behavior_clean = str_replace(behavior_clean, "holding food", "holding;")) %>%
  mutate(behavior_clean = str_replace(behavior_clean, "holding ", "holding;")) %>% 
  mutate(behavior_clean = str_replace(behavior_clean, "eating ", "eating;")) %>%
  mutate(behavior_clean = str_replace(behavior_clean, "foraging", "foragin")) %>% # i think this is the only way to do this given the nature of this typo
  mutate(behavior_clean = str_replace(behavior_clean, "foragin", "nose to ground")) %>%
  mutate(behavior_clean = str_replace(behavior_clean, "flighting", "fighting")) %>%
  mutate(behavior_clean = str_remove(behavior_clean, "\\[")) %>%
  mutate(behavior_clean = str_remove(behavior_clean, "\\]")) %>%
# there are many more behaviors to clean but for now, moving on to cleaning identifications because we are likely not going to focus on behavior


  separate(behavior, c("behavior_clean", "prey_or_item"), sep = ";", remove = FALSE) %>% 
  separate(behavior, c("behavior1", "behavior2"), sep = ",", remove = TRUE) %>%  # separates out behaviors if there are multiple
  
  uncount(number_of_objects, # "uncounts" i.e. makes specified number of copies of the row based on the # in how_many
          # we are basically multipling the number captures of animals by the number of animals in the capture - this is fine...
          
          .remove = FALSE #check your work by including .remove = FALSE... this keeps the column where the #of copies to make was stored. default is to delete it
          ) %>% 
 
# going to make some assumptions about IDs here:
  mutate(common_name = case_when(common_name == "canis species" ~ "coyote",
                        common_name == "canine family" ~ "coyote",
                        common_name == "cervus species" ~ "mule deer",
                        common_name == "cervidae family" ~ "mule deer",
                        common_name == "cetartiodactyla order" ~ "mule deer",
                        common_name == "odocoileus species" ~ "Mule Deer",
                        common_name == "elk" ~ "mule deer", # def not an elk
                        common_name == "even-toed ungulate" ~ "mule deer",
                        common_name == "pronghorn" ~ "mule deer", 
                        common_name == "vulpes species" ~ "grey fox",
                        common_name == "lepus species" ~ "brush rabbit", 
                        common_name == "domestic cat" ~ "bobcat", # double checked, def a bobcat
                        common_name == "cat family" ~ "bobcat", 
                        common_name == "eurasian lynx" ~ "bobcat",
                        common_name == "lynx species" ~ "bobcat",
                        common_name == "domestic pig" ~ "feral pig", 
                        common_name == "wild boar" ~ "feral pig",
                        common_name == "sus species" ~ "feral pig",
                        common_name == "neotoma species" ~ "rat/mouse",
                        common_name == "martes species" ~ "weasel",
                        common_name == "ursus species" ~ "american black bear",
                        common_name == "nutria" ~ "rat/mouse", # for now, until we determine if that thing is actually a nutria!
                        common_name == "western gray squirrel" ~ "california ground squirrel",
                        common_name == "brown rat" ~ "rat/mouse",
                        common_name == "woodrat or rat or mouse species" ~ "rat/mouse",
                        common_name == "rodent" ~ "rat/mouse",
                        common_name == "california mouse" ~ "rat/mouse",
                        common_name == "house mouse" ~ "rat/mouse",
                        common_name == "muridae family" ~ "rat/mouse",
                        common_name == "geomyidae family" ~ "gopher",
                        common_name == "kit fox" ~ "coyote", # def not a kit fox 
                        common_name == "white-tailed deer" ~ "mule deer",
                        common_name == "domestic cattle" & deployment_id == "Jalama 2" ~ "feral hog", # went and checked and computer IDd some pigs as cows. fixing for now in post, will fix later in WI [4aug23]
                        common_name == "white-tailed jackrabbit" ~ "coyote", # based on pics these are always yotes
                        common_name == "black-tailed jackrabbit" ~ "coyote", # checked and all jackrabbit IDs are coyotes
                        common_name == "mammal" ~ "unidentified mammal",
                        common_name == "carnivorous mammal" ~ "unidentified mammal",
                        common_name == "human-camera trapper" ~ "human",
                        common_name == "human-pedestrian" ~ "human",
                        common_name == "human - biker" ~ "human",
                        TRUE ~ as.character(common_name)
                        )
         ) %>% 
# need to remove domestics, marine mammals, sheep, mouflon?? and other weird IDs KEEP HUMANS, WILL REMOVE LATER
  filter(common_name != "mouflon", 
         common_name != "domestic sheep",
         common_name != "domestic cattle",
         common_name != "domestic donkey",
         common_name != "equus species",
         common_name != "puma", # at this moment, this is not a puma, it's a dog
         common_name != "domestic cow",
         common_name != "domestic horse",
         common_name != "californian sea lion",
         common_name != "harbor seal",
         common_name != "perissodactyla order",
         common_name != "bovidae family"
#         common_name != "domestic dog",
#         common_name != "human",
#         common_name != "human - camera trapper",
#         common_name != "human - biker",
#         common_name != "human",
#         common_name != "human-camera trapper",
#         common_name != "human-pedestrian",
         ) 

spp <- unique(mammals$common_name)
spp <- unique(mammals$common_name2)

```

#### Fix Messed Up Dates in Dataset
```{r fixing dates}

# we will first pull out just the incorrect dates, which are luckily all before 2020
fix_dates <- mammals %>% 
    ## make the dates nice
  mutate(year = year(timestamp)) %>% 
  filter(year == 2017 | 
           year == 2018 | 
           year == 2020 | 
           year == 2021 |
           deployment_id == "boat_16oct22" |
           deployment_id == "boat_17apr22" |
           deployment_id == "boat_17sep22" |
           deployment_id == "boat_5apr22" )

  #filter(year<=2021) # for now we have to make this more complicated because WI randomly changed dates of some deployments

# now we need to figure out what the corrections are, and for that we will need this dataset: https://docs.google.com/spreadsheets/d/1-gLml8H2stC2Va9H42tWLylx-5f1r8m4BDMKnsBUHl8/edit?usp=sharing (downloaded csv of one tab)
timestamp_corrections1 <- read_csv(here("data/incorrect_timestamps.csv")) 

timestamp_corrections <- timestamp_corrections1 %>% 
  rename(deployment_id = filename) %>% 
  mutate(real_start = mdy_hm(real_start)) %>% 
  mutate(recorded_start = mdy_hm(recorded_start)) %>% 
  mutate(real_end = mdy_hm(real_end)) %>% 
  mutate(recorded_end = mdy_hm(recorded_end)) %>% 
  mutate(correction = difftime(real_start, recorded_start, units = "mins")) %>%  #gives us the # minutes to add to the recorded time to correct the timestamp
  dplyr::select(deployment_id, correction)

#test_correct <- timestamp_corrections %>% 
#  mutate(calculated_end = recorded_end + correction) # it works!!

# now we can apply to the timestamp on each incorrect date, I hope!

## by matching on deployment ID
fixed_dates <- left_join(fix_dates, timestamp_corrections, by = join_by(deployment_id)) %>%  # pop the correction time in to the deployments it applies to
  # overwrite timestamp column with correct date
  mutate(timestamp = timestamp + correction) %>% 
  dplyr::select(!c(correction, year)) # take these bad bois out
  

# join the two datasets back together!
mammals_correctdates <- mammals %>% 
    filter(year(timestamp) == 2022 | 
           year(timestamp) == 2023 ) %>% 
    filter(deployment_id != "boat_16oct22") %>% 
    filter(deployment_id != "boat_17apr22") %>% 
    filter(deployment_id != "boat_17sep22") %>% 
    filter(deployment_id != "boat_5apr22" ) # need to remove the bad dates from the original dataset
  #filter(year(timestamp)>2021) # no longer this simple

#mammals_incorrectdates <- mammals %>% 
#  filter(year(timestamp)<=2021) # just to check (right now same # rows as 'fixed dates' so that's great)

## and do some downstream cleaning like removing useless columns
mammals_fixed_dates <- rbind(mammals_correctdates, fixed_dates) %>% 
  dplyr::select(sitename, deployment_id, filename, identified_by, class, order, family, genus, species, common_name, timestamp, number_of_objects, behavior1, behavior2, prey_or_item) %>% 
  mutate(day = day(timestamp)) %>% 
  mutate(month = month(timestamp, label = TRUE)) %>% 
  mutate(year = year(timestamp)) %>% 
  mutate(date = date(timestamp)) %>% 
  mutate(time = format(timestamp, '%T')) %>% 
  #separate(timestamp, c("date", "time"), sep = " ") %>% this creates blanks were the time is 0:00:00
  mutate(common_name = tolower(common_name))

```

#### We should add in some metadata for sorting and analysis purposes
```{r add in metadata}

######################## "monthly" deployment data ######################## 

#deployment_data_original <- read_csv(here("data/cam_deployment_data_FIXED_DATES_29jul2024.csv")) %>% 
deployment_data_original <- read_csv(here("data/cam_deployment_data_FIXED_DATES_9sep2024.csv")) %>% # switched to this most recent version of metadata on sep 17... hoping it doesn't break evreything
  clean_names() %>% 
  rename(sitename = camera_name, 
         maint_month = month,
         deployment_id = filename_prefix,
         property = location) %>% 
  
  # need to rename some file prefixes to match what is in mammals post-cleaning (ie adding underscores)
  mutate(deployment_id = str_replace(deployment_id, "pp17apr22", "pp_17apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "boatcam17apr22", "boat_17apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "pb28apr22", "pb_28apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "gov17apr22", "gov_17apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "nvs28apr22", "nvs_28apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "pl17apr22", "pl_17apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "cove17apr22", "cove_17apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "bc28apr22", "bc_28apr22")) %>% 
  mutate(deployment_id = str_replace(deployment_id, "bc17apr22", "bc_17apr22")) %>%
  mutate(deployment_id = str_replace(deployment_id, "pd28apr22", "pd_28apr22")) %>% 
  filter(!is.na(deployment_id)) %>% # will sort out deployments that don't have an ID because I manually added these in
  
  # need to fix days active column:
  mutate(day_end = mdy_hm(day_end)) %>% 
  mutate(day_start = mdy_hm(day_start)) %>% 
  mutate(days_active_calcd = difftime(day_end, day_start, units = "days")) %>% 
  
  filter(exclude != "y") # also need to take out the deployments I found a reason to exclude (tipped to sky etc)
  
# also need to figure out how many days active per MONTH
## need to remember WHY i did it this way when i'd already calculated it below using min(date) and max(date) by month...

deployment_data <- deployment_data_original %>%   
  mutate(each_date = map2(day_start, day_end, ~seq(from = .x, to = .y, by = "day"))) %>% # creates a list of all dates between each start and end date (each row)
  unnest(each_date) %>% # expands that list to be one row per date
  mutate(month = month(each_date, label = TRUE)) %>%  # extracts month from the dates
  group_by(deployment_id, month) %>% 
  mutate(days_per_month = n()) %>% 
# works if i want to keep all rows i just created but i dont, i want to summarize
  distinct(deployment_id, month, .keep_all = TRUE) %>%  # lets me keep only rows that are unique for combo of deployment id and month, while keeping all other columns
  dplyr::select(!each_date) %>%  # no longer need this
  ungroup()

############# create a dataframe for each date at each site that the camera was active, will be useful later for adding in zeros ############
all_days_sampled <- deployment_data_original %>%   
  #filter(exclude == "n") %>%  # need to take out the excluded days! turns out i already did this
  mutate(each_date = map2(day_start, day_end, ~seq(from = .x, to = .y, by = "day"))) %>% # creates a list of all dates between each start and end date (each row)
  unnest(each_date) %>% # expands that list to be one row per date
  mutate(month = month(each_date, label = TRUE)) %>% 
  mutate(date = date(each_date)) %>% 
  select(property, sitename, deployment_id, month, date)


######################## add on human data to montly deployment data #########################

humans_est <- read_csv(here("data/human_activity.csv")) %>%  # current best estimate of sitely, monthly human activity (made 8/3/2023)
  mutate(month = month(dmy(date), label = TRUE)) %>% 
  mutate(year = year(dmy(date))) %>% 
  rename(sitename = site,
         n_human_shots = n,
         human_seconds = seconds) %>% # NOT corrected by sampling effort
  dplyr::select(sitename, year, month, n_human_shots, human_seconds)

deployment_data_humans <- left_join(deployment_data, humans_est, by = join_by(sitename, month)) %>% 
  # fill in zeros for where there "is no human activity" - apparently, must check this
  mutate(human_seconds = case_when(is.na(human_seconds) ~ 0,
                                   TRUE ~ as.numeric(human_seconds))) %>% 
  ungroup() %>% 
  mutate(sitename = fct_relevel(sitename, c("Boathouse Cam", "Ladrones Cam", "Saucito Cam", "Water Canyon Cam", "Not Water Canyon Cam", "Morida Cam","RIZ Cam", "RIZ Cam Backup", "RIZ Cam S Canyon", "Sudden Canyon Cam", "Old Fencepost Cam", "Jolluru Cam", "Short Canyon Cam", "Jalama Cam", "Jalama 2 Cam", "Cojalama Cam", "Cojo Gate Cam", "Cojo Canyon Cam", "Seawall Cam", "Black Canyon Cam", "North Beach Fort Cam", "North Vista Spring Cam", "North Beach Canyon Cam", "Boneyard Cam", "Cove Cam", "Govies Cliff Cam", "Big Cojo Cam", "Percos Boat Cam", "Percos Driftwood Cam", "Wood Canyon Cam", "Percos Beach Cam", "Percos Log Cam", "Percos Post Cam", "Damsite Creek Cam")))
  
  # turns out there are some duplicates of months that have different human estimates, so we should sum these by month (easy to do using group_by (site, month) and mutate)
  # we do this downstream when we make deployment data into montly data
  

######################## site metadata #########################

metadata <- read_csv(here("data/site_metadata.csv")) %>% # NOT camera_metadata.csv (site_metadata is in camtrapR format)
  clean_names() %>% 
  filter(!str_detect(camera_id, "2")) %>%  # removing second cameras from the metadata (these were placed when the originals were stolen, but in the same place)
  dplyr::select(station, utm_x, utm_y, cam_brand, property, habitat_adjacent, habitat_secondary, iz_type, burst_settings) %>% 
  rename(sitename = station)

#### add in distance to Jalama (as center of human activity) using package geosphere ####

jalama_loc <- tibble(utm_x = 34.510480, utm_y = -120.501467) 
jalama_mtx <- cbind(jalama_loc$utm_y, jalama_loc$utm_x) # dist function requires that both distances be in matrix form, this is one way to do that apparently
# help doc example doesn't say this is necessary but oh well

metadata2 <- metadata %>% 
  mutate(dist_jalama = (distVincentyEllipsoid(cbind(utm_y, utm_x), jalama_mtx))/1000) %>% # dist returns shortest distance in meters as default
  filter(sitename != "North Jalama") # for now, so list matches the data we're actually using

######################## combine and add metadata to mammals data #########################

# this combines deployment (now we've made it monthly) records with metadata for each site
metadata_by_deployment <- left_join(deployment_data_humans, metadata2, by = join_by(property, sitename)) %>% 
  dplyr::select(property, sitename, deployment_id, dist_jalama, imgs_on_sd, days_active_calcd, days_per_month, month, exclude, utm_x, utm_y, habitat_adjacent, habitat_secondary, iz_type, burst_settings, cam_brand, n_human_shots, human_seconds) %>% 
  filter(exclude == "n") %>% 
  filter(!is.na(deployment_id)) # filters out all the deployments that haven't been reviewed because i don't put in filename prefix if it hasn't been reviewed on WI

# cross checking that all deployments in the metadata are present in the mammals df and vice versa
deployments_from_mammalsdf <- mammals_fixed_dates %>% 
  distinct(deployment_id, month, .keep_all = TRUE)

missing_deployments <- anti_join(deployments_from_mammalsdf, metadata_by_deployment, by = c("deployment_id", "month"))  # anti_join() return all rows from x without a match in y  ]

# for some reason this dataframe construction is duplicating some rows .... causing downstream issues, so we are going to use a dataframe BY MONTH instead and then match to mammals using combo of site and month

metadata_by_month <- metadata_by_deployment %>% 
  distinct(sitename, month, days_per_month, .keep_all = TRUE) %>% # only unique combinations of site, month, and days deployed because some months were split over deployments
  # so now we need to sum days deployed when they're split across multiple deployments but in the same month:
  group_by(property, sitename, dist_jalama, utm_x, utm_y, habitat_adjacent, habitat_secondary, iz_type, burst_settings, cam_brand, # none of these change over the course of the project
           month) %>% # these are how we want to sum days per month (sums across deployments)
  summarise(trapnights = sum(days_per_month),
            total_human_sec = sum(human_seconds)) %>% 
  unite(sitemonth_id, c("sitename", "month"), sep = ".", remove = FALSE) %>% 
  ungroup() %>% 
  mutate(humans_per_trapnight = total_human_sec/trapnights) %>% 
  mutate(sitename = fct_relevel(sitename, c("Boathouse Cam", "Ladrones Cam", "Saucito Cam", "Water Canyon Cam", "Not Water Canyon Cam", "Morida Cam","RIZ Cam", "RIZ Cam Backup", "RIZ Cam S Canyon", "Sudden Canyon Cam", "Old Fencepost Cam", "Jolluru Cam", "Short Canyon Cam", "Jalama Cam", "Jalama 2 Cam", "Cojalama Cam", "Cojo Gate Cam", "Cojo Canyon Cam", "Seawall Cam", "Black Canyon Cam", "North Beach Fort Cam", "North Vista Spring Cam", "North Beach Canyon Cam", "Boneyard Cam", "Cove Cam", "Govies Cliff Cam", "Big Cojo Cam", "Percos Boat Cam", "Percos Driftwood Cam", "Wood Canyon Cam", "Percos Beach Cam", "Percos Log Cam", "Percos Post Cam", "Damsite Creek Cam"))) %>% 
  select(!c(cam_brand))

########## FINAL dataset of mammal activity ########## 
mammals_clean <- left_join(mammals_fixed_dates, metadata_by_month, by = join_by(sitename, month)) %>% 
  #rename(sitename = sitename.x) %>%  #got two of these in the join
  #dplyr::select(!sitename.y) %>% 
  filter(!is.na(property)) %>%  # right now (23 jul 2024) this filters out HROA data because there's not metadata for those cams
  filter(common_name != "human",
         common_name != "domestic dog") # finally time to rule these out bc they're contained in human/trapnight estimate

# the above SHOULD pop on monthly metadata to each observation based on its observed month and site

####### TABLING BELOW FOR NOW UNTIL WI DATA IS MORE COMPLETE #######
#humans_clean <- left_join(mammals_fixed_dates, metadata_by_month, by = join_by(sitename, month)) %>% 
  #rename(sitename = sitename.x) %>%  #got two of these in the join
  #dplyr::select(!sitename.y) %>% 
  #filter(!is.na(property)) %>%
#  filter(common_name == "human"|
#         common_name == "domestic dog")

#humans_summary <- humans_clean %>% 
#  group_by(sitename, burst_settings, month, days_per_month) %>% 
#  summarise(n = n()) %>% 
#  mutate(seconds = n/burst_settings) %>% 
#  mutate(seconds_per_trapnight = seconds/days_per_month) %>% 
#  ungroup()
#write_csv(humans_summary, here("data/humans_summary.csv"))

# saving this for reproducibility
#write_csv(rbind(mammals_clean, humans_clean), here("data/classifications_w_metadata_full_9aug2024.csv"))

```
### Pull in precipitation data
Goal is to generate mean precip for previous 90 days for each month, starting the first day of that month
Using r package prism to access Oregon State Prism data
JK it's easier to just download it for our one cell of interest from the online portal
```{r precip data from prism}

# read in downloaded prism data
precip_daily_orig <- read_csv(here("data/PRISM_ppt_daily_4km_20211201_20230401_34.4576_-120.4652.csv"))

# clean it
precip_monthly <- precip_daily_orig %>% # this was dumb, i already had this, oh well, need montly for next step
  clean_names() %>% 
  mutate(date = mdy(date)) %>% 
  mutate(month = month(date, label = TRUE),
         year = year(date)) %>% 
  group_by(year, month) %>% 
  summarize(ppt_month = sum(ppt_inches)) %>% 
  ungroup()

# package zoo has a function for a 3-part rolling mean
precip_rollmean <- precip_monthly %>% 
  mutate(months = rollapply(month_year,3,function(x){
    paste(substr(x,1,1),collapse = '')
  },align='right',fill=NA)) %>% 
  mutate_at(vars(ppt_month),~rollmean(.,k=3,align = 'right',fill=NA)) %>% 
  mutate(prev3_months = lag(months),
         prev3_mean_ppt = lag(ppt_month))

precip <- precip_rollmean %>% 
  select(year, month, prev3_mean_ppt)

#prism_set_dl_dir("/Users/zoe/Documents_Local/GitHub/project_ecotone/data") # necessary for prism to work

# idk I think daily averages are the best? 
#get_prism_dailys(type = "ppt", minDate = "2022-01-01", maxDate = "2022-04-01") #Must be specified in a valid iso-8601 (e.g. YYYY-MM-DD) 

```



## Planned Initial Analyses
- response variable: animal activity (count of occurrences) across all species and across entire study with month as random effect
- effect variables of interest: distance from Jalama, human activity, species
      - maybe above or below Point Conception (barrier to human movement from Jalama)
- random variables: month
- will need to include an offset of days active per site
 
# Bin Data Appropriately
counts by site > month (losing daily or weekly variation or variation due to species)
counts by 24hr period, results in zero inflation, but that might be ok
counts by 7-day surveys seems like the best compromise

### By Day (Easy)
```{r bin by day}
activity_by_day <- mammals_clean %>% 
  ungroup() %>% 
  count(property, sitename, month, date, sitemonth_id, dist_jalama, habitat_adjacent, habitat_secondary, iz_type, total_human_sec, humans_per_trapnight, trapnights, burst_settings, common_name) %>%  # need all the covariates in here, including trap nights in case we want to adjust counts before summing? idk
  # count() combines group_by and summarize(n_distinct) so we get a count of the rows in this combo of variables, in the order we specify
  mutate(captures_adj = ceiling(n/burst_settings)) %>%  # adjusts by burst (divides by 8 or 10) and rounds up
  select(!n)


# we will also make a matrix version (all species have a column) becuase it's easier to add on days where nothing was taken later on
activity_by_day_matrix <- activity_by_day %>% 
  pivot_wider(names_from = common_name, 
              values_from = captures_adj, 
              values_fill = 0, # adds zeros where appropriate
              names_prefix = "sp_")%>% 
  # now we add on the days that the camera was active but nothing was captured
  
  full_join(., all_days_sampled) %>%  # if we leave out by = then it should just join on all common columns

  # trying fill instead of join for other variables:
  group_by(property, sitename, month) %>%  # becayse all of our covariates are measured on the site:month scale, they should be the same within these groups and we can fill from previous values
  fill(sitemonth_id:burst_settings) %>%  # default is down which is fine i think
  arrange(sitename, date) # so i can make sure all the dates are there

# this got more complicated than i thought and we still need to bind metadata to the rows that had no animla photos (so no way to populate the date rows with metadata from previous rows)

# take out any row with NAs in the metadata
activity_day_wmeta <- activity_by_day_matrix %>% 
  filter(!is.na(habitat_adjacent)) # gotta take these out

# pulling NAs out and attempting a join again on just those
activity_day_nometa <- activity_by_day_matrix %>% 
  filter(is.na(habitat_adjacent)) %>% 
  select(!sitemonth_id:burst_settings) %>%  # need ot pull these cols out so i can replace them in the bind (filling NAs on a bunch of columns like this like with coalesce is onerous)
  left_join(., metadata_by_month)# %>% # a full 
  
  
  # now that all the metadata is in place, we can mash the two sub-frames back together fill NAs in the species columns with zeros
activity_day_matrix <- rbind(activity_day_wmeta, activity_day_nometa) %>% 
  select(!c(deployment_id, utm_x, utm_y)) %>%  # i don't need these rn and they have NAs so bye
  # replace zeros
  mutate(across(starts_with("sp_"), ~replace_na(., 0)))

write_csv(activity_day_matrix, here("data/activity_day_matrix.csv"))

```


### By 7-day "Surveys"
can use floor_date() from lubridate I think "floor_date() takes a date-time object and rounds it down to the nearest boundary of the specified time unit."
need to also use "all days sampled" dataset to make sure we get the days where nothing was found in there - easier to also start with the by-day binned dataset
```{r bin by 7day survey}

activity_7day1 <- activity_by_day_matrix %>% 
  full_join(., all_days_sampled) %>%  # if we leave out by = then it should just join on all common columns

  # trying fill instead of join for other variables:
  group_by(property, sitename, month) %>%  # becayse all of our covariates are measured on the site:month scale, they should be the same within these groups and we can fill from previous values
  fill(sitemonth_id:burst_settings) %>%  # default is down which is fine i think
  arrange(sitename, date) # so i can make sure all the dates are there

# this got more complicated than i thought and we still need to bind metadata to the rows that had no animla photos (so no way to populate the date rows with metadata from previous rows)

# take out any row with NAs in the metadata
activity_7day_wmeta <- activity_7day1 %>% 
  filter(!is.na(habitat_adjacent)) # gotta take these out

# pulling NAs out and attempting a join again on just those
activity_7day_nometa <- activity_7day1 %>% 
  filter(is.na(habitat_adjacent)) %>% 
  select(!sitemonth_id:burst_settings) %>%  # need ot pull these cols out so i can replace them in the bind (filling NAs on a bunch of columns like this like with coalesce is onerous)
  left_join(., metadata_by_month)# %>% # a full 
  
  
  # now that all the metadata is in place, we can mash the two sub-frames back together fill NAs in the species columns with zeros
activity_7day <- rbind(activity_7day_wmeta, activity_7day_nometa) %>% 
  select(!c(deployment_id, utm_x, utm_y)) %>%  # i don't need these rn and they have NAs so bye
  # replace zeros
  mutate(across(starts_with("sp_"), ~replace_na(., 0))) %>%  # dont forget the tilde
  # now we can start collapsing things into 7-day chunks
  mutate(week = week(date)) %>%
  
  # create a survey ID the represents the week of "collection" (on a 52 week calendar)
  unite(survey_id, c(sitename, week), remove = FALSE) %>% 
  select(!week) %>% # we wnat to keep sitename but not week
  
  # now we can pivot longer again before we sum all the counts
  pivot_longer(cols = 14:28,
               names_to = "common_name", 
               names_prefix = "sp_", 
               values_to = "daily_seconds") %>% # since we already adjusted by burst settings before matrix, these are seconds and not counts
  
  ## need to FIND A WAY to get active days per week, looks like we gotta skip summarise and group_by > mutate
  group_by(sitename, survey_id) %>%  # gotta keep all covariates
  mutate(weekly_trapnights = n_distinct(date)) %>% # will this work with group_by? it should
  ungroup() %>%  
  
  # group again but now by species
  group_by(property, sitename, dist_jalama, habitat_adjacent, habitat_secondary, iz_type, month, total_human_sec, humans_per_trapnight, survey_id, weekly_trapnights, common_name) %>% #
  summarise(activity_seconds = sum(daily_seconds)) %>% 
  ungroup() %>% 
  mutate(seconds_per_trapnight = activity_seconds/weekly_trapnights) %>% # cna only do this for animals since humans were estimated on a monthly basis, not weekly
  arrange(sitename, survey_id)


  ##### below is floor_date, a really good way to do this but i decided i want to group by assigning lubridate:week
  #group_by(sitename) %>%  #only grouping by sitename to allow the 7 day chunks to pass across months and just kinda see how that goes?
  #mutate(survey_start = floor_date(date, "7 days")) # "survey" here is our 7day chunk that were making sup

```


### By Month
```{r binning and summarizing data by month}

######### make table of species by month ###########
species_month <- mammals_clean %>%
  count(month, year, common_name) %>%  # makes a summary table of # images per species per month
# includes count of empties
  rename(n_imgs = n) # rename n to be more informative


######### make table of animal activity ( = incidences adjusted by burst number and corrected for by trap nights) by date and site and species ###########
activity_site_month_sp_1 <- mammals_clean %>% 
  ungroup()%>%
  group_by(property,sitename, dist_jalama, year, month, trapnights, burst_settings, total_human_sec, humans_per_trapnight, common_name) %>% 
  summarize(total_captures = n()
    #total_captures = sum(number_of_objects), #  this isn't returning the same # as counting rows because sometimes the value isn't 1!!!
            ) %>% # had a hard time with the final columns so I'm tossing it out into a mutate:
  mutate(activity_seconds = ceiling(total_captures/burst_settings))%>%  # adjusts # of captures by the number of images taken in a burst (8 or 10), ceiling rounds UP)
  # need to make trapnights numeric integers in order to adjust by them
  mutate(trapnights = case_when(trapnights == "0.000000 days" ~ 1,
                                TRUE ~ as.numeric(trapnights))) %>% 
  mutate(trapnights = case_when(trapnights == 0 ~ 1,
                                TRUE ~ as.numeric(trapnights))) %>%
  mutate(seconds_by_trapnight = activity_seconds/trapnights) %>% 

  unite("sitemonth_id", sitename, month, sep = ".", remove = FALSE) %>% 
  mutate(protection_rank = case_when(property == "JLDP" ~ 2, # actively managed
                                     property == "VSFB" ~ 1, # not managed but not developed
                                     property == "JBCP" ~ 0  # mildly developed recreational area
                                     )) %>% 
  mutate(season = case_when(month %in% c("Apr", "May", "Jun", "Jul", "Aug", "Sep") ~ "dry",
                            month %in% c("Oct", "Nov", "Dec", "Jan", "Feb", "Mar") ~ "wet")) %>% 
  relocate(protection_rank, .after = property) %>% 
  relocate(season, .before = month)


######### need to add in the zeros for all spp and humans that this dataframe doesn't reflect ###########
### easiest way to do that is to make a matrix

# one for raw seconds
activity_wide <- activity_site_month_sp_1 %>% 
  dplyr::select(!c(total_captures, seconds_by_trapnight)) %>%  # we need to take out some columns that only go with each indiv. species
  pivot_wider(names_from = common_name, names_prefix = "sp_", # column name, prefix with sp_ so we can act on all columns if we need to!
              values_from = activity_seconds, 
              values_fill = 0) %>% 
  ungroup()

# one for adjusted seconds
activity_wide_adj <- activity_site_month_sp_1 %>% # this one lets us look at captures adjusted by effort
  dplyr::select(!c(total_captures, activity_seconds)) %>%  # we need to take out some columns that only go with each indiv. species
  pivot_wider(names_from = common_name, names_prefix = "sp_", # column name, prefix with sp_ so we can act on all columns if we need to!
              values_from = seconds_by_trapnight, 
              values_fill = 0) %>% 
  ungroup()
 
# we want to keep the above dataframes for later matrix stuff, otherwise we could just pipe in pivoting them longer again 
activity_site_month_sp_2 <- activity_wide %>% 
  pivot_longer(cols = sp_coyote:sp_gopher,
               names_to = "common_name",
               names_prefix = "sp_", # this should remove the prefix I added, i think
               values_to = "activity_seconds")

activity_site_month_sp_adj <- activity_wide_adj %>% 
  pivot_longer(cols = sp_coyote:sp_gopher,
               names_to = "common_name",
               names_prefix = "sp_", # this should remove the prefix I added, i think
               values_to = "seconds_by_trapnight")

activity_site_month_sp <- full_join(activity_site_month_sp_2, activity_site_month_sp_adj) %>%  # will join based on all shared columns, so should only tack on the seconds by trapnight col
  # finally add on lagged precip data
  full_join(., precip) %>% 
  na.omit()
  
```

### Visualize Data ~ Month and Site
```{r plots of mean activity by month}

# by month, sites averaged
month_violin <- ggplot(activity_7day, aes(x = as.factor(month), y = activity_seconds)) +
  geom_violin()

# by month, adjusted by trapnights
month_adj_box <- ggplot(activity_7day, aes(x = as.factor(month), y = seconds_per_trapnight))+
  geom_boxplot()

month_adj_violin <- ggplot(activity_7day, aes(x = as.factor(month), y = seconds_per_trapnight)) +
  geom_violin()

# by month, with species colored different
month_species <- ggplot(activity_7day, aes(x = as.factor(month), y = seconds_per_trapnight)) +
  geom_point(aes(color = common_name)) 

# add a running mean per species
monthly_mean_spp <- activity_7day %>% 
  group_by(month, common_name) %>% 
  summarise(mean_activity = mean(seconds_per_trapnight),
            sd_activity = sd(seconds_per_trapnight)) %>% 
  ungroup() %>% 
  ggplot(., aes(x = month, y = mean_activity, group = common_name, colour = common_name)) +
  #geom_errorbar(aes(ymin = mean_activity - sd_activity, ymax = mean_activity + sd_activity)) + # better to just plot the scatter in this case
  geom_path() +
  geom_point( data = activity_7day, aes(x = month, y = seconds_per_trapnight, colour = common_name))

# based on this, looks like the peak in the summer is slightly a function of trapping effort (aka is less pronounced when we adjust by # trapnights)

activity_7day$sitename <- factor(activity_7day$sitename, levels = c("Boathouse Cam", "Ladrones Cam", "Saucito Cam", "Water Canyon Cam", "Not Water Canyon Cam", "Morida Cam","RIZ Cam", "RIZ Cam Backup", "RIZ Cam S Canyon", "Sudden Canyon Cam", "Old Fencepost Cam", "Jolluru Cam", "Short Canyon Cam", "Jalama Cam", "Jalama 2 Cam", "Cojalama Cam", "Cojo Gate Cam", "Cojo Canyon Cam", "Seawall Cam", "Black Canyon Cam", "North Beach Fort Cam", "North Vista Spring Cam", "North Beach Canyon Cam", "Boneyard Cam", "Cove Cam", "Govies Cliff Cam", "Big Cojo Cam", "Percos Boat Cam", "Percos Boat Rodents", "Percos Driftwood Cam", "Wood Canyon Cam", "Percos Beach Cam", "Percos Log Cam", "Percos Post Cam", "Damsite Creek Cam"))

# by site, months averaged
site_scatter <- ggplot(activity_7day, aes(x = sitename, y = activity_seconds)) +
  geom_point(aes(color = month))

# by site, months averaged, adjusted by effort
site_bar <- ggplot(activity_7day, aes(x = sitename, y = seconds_per_trapnight)) +
  geom_bar(aes(fill = month), stat = "identity")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 7))

# by property/ protection level, everything else grouped:

#protection_scatter <- ggplot(activity_7day, aes(x = protection_rank, y = activity_seconds)) +
#  geom_point(aes(color = property))


```

### Visualize Data ~ Season
```{r wet vs dry}

# should probably pull precip data and see if it actually matches what we've called wet and dry
# it does

##### all activity compared between wet and dry season ##### 
season_boxplot <- ggplot(activity_site_month_sp, aes(x = season, y = seconds_by_trapnight)) +
  #geom_boxplot()
  geom_jitter()

##### HUMAN activity compared between wet and dry season ##### 
human_season_boxplot <- activity_site_month_sp %>% 
  distinct(season, sitename, month, humans_per_trapnight) %>% # because we currently have a row per species for each site and month
  ggplot(., aes(x = season, y = humans_per_trapnight)) +
  #geom_boxplot()
  geom_jitter()

#####  wet vs dry season, split into species #####
spp_season_boxplot <- ggplot(activity_site_month_sp, aes(x = season, y = seconds_by_trapnight)) +
  geom_boxplot(aes(fill = common_name))

top3_season_boxplot <- activity_site_month_sp %>% 
  filter(common_name %in% c("coyote", "feral pig", "mule deer")) %>% 
  ggplot(., aes(x = season, y = seconds_by_trapnight)) +
  geom_boxplot(aes(fill = common_name))

#####  wet vs dry season, split into site #####
site_season_boxplot <- ggplot(activity_site_month_sp, aes(x = season, y = seconds_by_trapnight)) +
  geom_boxplot(aes(fill = sitename))

##### wet vs dry season, with human activity on x axis #####
human_season_scatter <- ggplot(activity_site_month_sp, aes(x = humans_per_trapnight, y = seconds_by_trapnight)) +
  geom_point()+
  facet_wrap(~ season)
  
############# actual lagged precipitation data ##############

##### prev_precip ~ animal activity #####

precip_bar <- activity_site_month_sp %>% 
  distinct(year, month, prev3_mean_ppt) %>% 
  unite(year_month, c(year, month), sep = "_") %>% 
  mutate(year_month = fct_relevel(year_month, c("2022_Mar", "2022_Apr", "2022_May", "2022_Jun", "2022_Jul", "2022_Aug", "2022_Sep",  "2022_Oct", "2022_Nov", "2022_Dec",  "2023_Mar", "2023_Apr"))) %>%  # i'm just too tired to code a way to force these to be in order otherwise
  ggplot(., aes(x = year_month, y = prev3_mean_ppt))+
  geom_bar(stat = "identity")+
  ylab("Mean Precipitation (in) from Previous 3 Months")

precip_scatter <-ggplot(activity_site_month_sp, aes(x = prev3_mean_ppt, y = seconds_by_trapnight, colour = common_name))+
  geom_point()
precip_scatter

```


### More visualizations looking for correlations

```{r visualize with scatter}

######### scatter plot of mammal activity by site by month ~ distance from jalama #########

activity_by_dist_plot <- ggplot(activity_7day, aes(x = dist_jalama, y = seconds_per_trapnight)) +
  geom_point(aes(color = common_name))+
  geom_smooth()

######### scatter plot of mammal activity by site ~ human activity #########

activity_by_humans_plot <- ggplot(activity_7day, aes(x = human_seconds, y = seconds_per_trapnight)) +
  geom_point(aes(color = common_name))+
  geom_smooth()

######### scatter plot of coyote activity by site by month ~ distance from jalama #########

#coyote_dist_plot <- ggplot(coyote_activity, aes(x = dist_jalama, y = activity_seconds)) + 
#  geom_point()

######### check out distribution of data #########

bysite <- ggplot(activity_7day, aes(x = seconds_per_trapnight)) +
  geom_density(aes(color = sitename, fill = sitename), alpha = 0.3) + # Note: just to show what the geom_violin shows
  theme_classic() +
  #scale_x_continuous(expand = c(0,0), limits = c(0,3e6)) +
  #scale_y_continuous(expand = c(0,0)) +
  labs(x = "Seconds of Wildlife Activity Per Trapnight", y = "Density")

bydays_sampled <- ggplot(activity_7day, aes(x = weekly_trapnights, y = seconds_per_trapnight)) +
  geom_point() + # Note: just to show what the geom_violin shows
  theme_classic() +
  #scale_x_continuous(expand = c(0,0), limits = c(0,3e6)) +
  #scale_y_continuous(expand = c(0,0)) +
  labs(y = "Captures (adjusted by burst settings)", x = "Days Camera Was Active per WEEK")

# days active and captures don't LOOK correlated but let's check
cor.test(~ seconds_per_trapnight + weekly_trapnights, data = activity_7day, method=c("spearman"))
# yup, correlated


```

### couple human stats
```{r}

# we should also check for correlation between human activity and distance from jalama

humans_inspace_raw <- ggplot(activity_site_month_sp, aes(x = dist_jalama, y = total_human_sec)) +
  geom_point()

humans_inspace_perngiht <- ggplot(activity_site_month_sp, aes(x = dist_jalama, y = humans_per_trapnight)) +
  geom_point()


cor.test(~ humans_per_trapnight + dist_jalama, data = activity_site_month_sp, method=c("spearman"))
cor.test(~ humans_per_trapnight + dist_jalama, data = activity_site_month_sp, method=c("pearson"))
cor(activity_site_month_sp$dist_jalama, activity_site_month_sp$humans_per_trapnight)

# can do a kruskal wallis i think... to compare between months
# no, KW is like anova, needs more than two samples; for pairwise, we do Dunn
human_season_pairwise <- activity_site_month_sp %>% 
  distinct(sitename, season, month, humans_per_trapnight) %>%  # theres a record of human activity for every species within month
  ungroup() %>% 
  dunn_test(humans_per_trapnight ~ season, p.adjust.method = "bonferroni")

```


# Next run a GLMM with a negative binomial distribution: wildlife activity ~ distance from jalama * species * human activity + random effect of site and property
- actually taking out species as a factor because i will model the three most active species on their own later

### Trying to run models with response variable binned into month and adding in a covariate for season
We will do rainy season as oct - march and dry season as april - sept

```{r glmm activity model selection month sample}

# we know we need a mixed model because we need a random nesting effect of property and site x month
# variables of interest are monthly human seconds, season, distance from jalama, species, and interactions between all
# number of trapnights logged as an offset -- not sure why we log this but other papers do it (because there's a log link for the rest of the function)

incomplete <- activity_site_month_sp %>% 
  filter(!complete.cases(.)) #make sure every row has all data (no NAs)

# real random effect here, in this nested design, would be SITE as the thing that is repeatedly sampled, but site = distance from jalama and is the variable we are interested in...
# week/survey_id is also nested within site (and repeatedly sampled), so is a random variable, but it's the level of observation atm since we binned (summed) all occurences into weekly sums - i don't believe we need to include the level of sampling as an error term

# many models standardize their explanatory variables... why?
## found the answer here: https://stats.stackexchange.com/questions/35071/what-is-rank-deficiency-and-how-to-deal-with-it/35077#35077
### "The trick is usually to use common units, but on some problems even that is an issue when variables vary by too many orders of magnitude. More important is to scale your numbers to be similar in magnitude."

############### VARS ############### 

# RESPONSE variable is activity_seconds, offset is weekly_trapnights
# SAMPLES are 4 weeks x 9 months x 30 sites per species (although a lot of zeros)
# RANDOM variable is survey_id
# potential EXPLANATORY variables are:
## property: VSFB, JLDP, Jalama ## rank deficient
## season
## common_name (species) ## rank deficient
## iz_type ## rank deficient
## dist_jalama
## human_seconds

######### NULL MODELS ######### 

#glmm_null <- glmer.nb(activity_seconds ~ 1, 
#                     data = activity_site_month_sp, 
#                     family = nbinom2(link = "log"), 
#                     offset = log10(trapnights), 
#                         na.action = "na.fail")

glmm_null_randoms <- glmmTMB(activity_seconds ~ 1 
                             + (1|sitename), 
                             data = activity_site_month_sp, 
                             family = nbinom2(link = "log"), 
                             offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_null_zi <- update(glmm_null_randoms, ziformula = ~1)

######### FULL MODELS ######### 

glmm_full_inx_ppt <- glmmTMB(activity_seconds ~ prev3_mean_ppt * humans_per_trapnight * dist_jalama
                          + (1|sitename), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail") 

glmm_full_inx <- glmmTMB(activity_seconds ~ season * humans_per_trapnight * dist_jalama
                          + (1|sitename), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail") 

glmm_full_inx_zi <- update(glmm_full_inx, ziformula = ~1) # we're adding on a single zero inflation parameter to all observations (not a hurdle) - treats zero counts as a mixture of structural and sampling zeros, just giving the bernoulli formula an intercept

glmm_full_add <- glmmTMB(activity_seconds ~ season + humans_per_trapnight + dist_jalama
                          + (1|sitename) , 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_full_add_zi <- update(glmm_full_add, ziformula = ~1)

glmm_full_seasxhum <- glmmTMB(activity_seconds ~ season * humans_per_trapnight + dist_jalama
                          + (1|sitename) , 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_full_seasxhum_zi <- update(glmm_full_seasxhum, ziformula = ~1)

glmm_full_seasxdist <- glmmTMB(activity_seconds ~ season * dist_jalama + humans_per_trapnight 
                          + (1|sitename) , 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_full_seasxdist_zi <- update(glmm_full_seasxdist, ziformula = ~1)

glmm_full_distxhum <- glmmTMB(activity_seconds ~ season + dist_jalama * humans_per_trapnight 
                          + (1|sitename) , 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_full_distxhum_zi <- update(glmm_full_distxhum, ziformula = ~1) # we're adding on a single zero inflation parameter to all observations (not a hurdle) - treats zero counts as a mixture of structural and sampling zeros, just giving the bernoulli formula an intercept

#glmm_full_distxhum_hurdle <- update(glmm_full_distxhum, ziformula = ~.) # we're adding on a single zero inflation parameter to all observations (not a hurdle), assumes that there is a difference between structural and sampling zeros

######### MULTIPLE COVARIATE MODELS ######### 

# just to see:
#glmm_distxseas_distxhum <- glmmTMB(activity_seconds ~ (dist_jalama * total_human_sec) + (dist_jalama * season)
#                          + (1|sitename), 
#                         data = activity_site_month_sp, 
#                         family = nbinom2(link = "log"), 
#                         offset = log10(trapnights), 
#                         na.action = "na.fail") # necessary for dredge to work

glmm_1x <- glmmTMB(activity_seconds ~ dist_jalama * humans_per_trapnight 
                          + (1|sitename), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail") # necessary for dredge to work

glmm_1x_z<- update(glmm_1x, ziformula = ~1)

glmm_1a <- glmmTMB(activity_seconds ~ dist_jalama + humans_per_trapnight 
                          + (1|sitename), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_1a_z<- update(glmm_1a, ziformula = ~1)

glmm_2x <- glmmTMB(activity_seconds ~ season * humans_per_trapnight 
                          + (1|sitename), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail") # necessary for dredge to work

glmm_2x_z<- update(glmm_2x, ziformula = ~1)

glmm_2a <- glmmTMB(activity_seconds ~ season + humans_per_trapnight 
                          + (1|sitename), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_2a_z<- update(glmm_2a, ziformula = ~1)

glmm_3x <- glmmTMB(activity_seconds ~ season * dist_jalama 
                          + (1|sitename), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail") 

glmm_3x_z<- update(glmm_3x, ziformula = ~1)

glmm_3a <- glmmTMB(activity_seconds ~ season + dist_jalama 
                          + (1|sitename) , 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_3a_z<- update(glmm_3a, ziformula = ~1)

######### SINGLE COVARIATE MODELS ######### 

glmm_season <- glmmTMB(activity_seconds ~ season 
                          + (1|sitename), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_season_z<- update(glmm_season, ziformula = ~1)

glmm_dist <- glmmTMB(activity_seconds ~ dist_jalama
                          + (1|sitename), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_dist_z<- update(glmm_dist, ziformula = ~1)

glmm_human <- glmmTMB(activity_seconds ~ humans_per_trapnight 
                          + (1|sitename) , 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_human_z<- update(glmm_human, ziformula = ~1)

####### attempting to make a model table ####### 
bbmle::AICtab(glmm_full_inx, glmm_full_inx_zi, 
                                    glmm_full_add, glmm_full_add_zi,
                                    glmm_full_seasxhum, glmm_full_seasxhum_zi,
                                    glmm_full_seasxdist, glmm_full_seasxdist_zi,
                                    glmm_full_distxhum, glmm_full_distxhum_zi,
                                    glmm_1x, glmm_1a, glmm_1x_z, glmm_1a_z, 
                                    glmm_2x, glmm_2a, glmm_2x_z, glmm_2a_z,
                                    glmm_3x, glmm_3a,  glmm_3a_z, glmm_3x_z, 
                                    glmm_dist, glmm_dist_z,
                                    glmm_human, glmm_human_z,
                                    glmm_season, glmm_season_z,
                                    glmm_null_randoms, glmm_null_zi, logLik = TRUE,  base = TRUE)

activity_aic_table <- bbmle::AICtab( glmm_full_inx_zi, # only including zero inflated in the final table
                                    glmm_full_add_zi,
                                     glmm_full_seasxhum_zi,
                                     glmm_full_seasxdist_zi,
                                     glmm_full_distxhum_zi,
                                     glmm_1x_z, glmm_1a_z, 
                                     glmm_2x_z, glmm_2a_z,
                                     glmm_3a_z, glmm_3x_z, 
                                     glmm_dist_z,
                                     glmm_human_z,
                                     glmm_season_z,
                                     glmm_null_zi, logLik = TRUE,  base = TRUE)

activity_aic_table <- as.tibble(activity_aic_table) %>% 
  rownames_to_column()

####### model diagnostics ####### 
simulationOutput <- simulateResiduals(fittedModel = glmm_full_inx_ppt)
simulationOutput <- simulateResiduals(fittedModel = glmm_3x_z)
#residuals(simulationOutput)
plot(simulationOutput)
plotResiduals(simulationOutput)
testQuantiles(simulationOutput)
testDispersion(simulationOutput)
testZeroInflation(simulationOutput)
plot(x = simulationOutput$observedResponse, y = simulationOutput$fittedPredictedResponse)
#testSpatialAutocorrelation(simulationOutput, x = activity_site_month_sp$dist_jalama, y = activity_site_month_sp$activity_seconds)


####### plotting and R2 of best models ####### 

plot(allEffects(glmm_3x_z))
r.squaredGLMM(glmm_3x_z)

```




## Time to switch to by-species analysis
### First we can look at which species and explanatory variables are correlated
```{r correlation plot}
# first we need a wide form of the data so we have the right info for every row!
# good thing we already made one

activity_corrs1 <- activity_wide %>% 
  select(!burst_settings)
activity_corrs <- cor(select_if(activity_corrs1, is.numeric))  # cor and corrplot can only deal with numeric values, makes sense
activity_corrs2 <- cor(select_if(activity_wide_adj, is.numeric)) # same thing but adjusted for effort

corrplot(activity_corrs,
         type = "upper",
         #method = "ellipse",
         tl.col = "black",
         tl.cex = 0.5)

corrplot(activity_corrs2,
         type = "upper",
         method = "ellipse",
         tl.col = "black",
         tl.cex = 0.5)
#large dark circles = large positive correlations
#small light circles = small negative correlations

```

### Let's model coyote activity first since they're the most common
```{r coyote glmm}

# already made a yote df, need to update it tho
coyote_activity <- activity_site_month_sp %>% 
  filter(common_name == "coyote")

# how does the data look
ggplot(coyote_activity, aes(x = activity_seconds)) +
  geom_histogram()

# doesn't look liek AS many zeros so i think we can model with out zero inflation

# poisson diagnostics NOT good, def a neg binom situation
#coyote_full <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds #+ (1|sitename) 
#                     + (1|property), # swapping from glmmTMB to glmer to see if it'll let us make an AIC table
#                         data = coyote_activity, 
#                         family = poisson(link = "log"), 
#                         offset = log10(trapnights), 
#                         #na.action = "na.fail",# necessary for dredge to work
#                       na.action = "na.exclude"
#                       ) 

coyote_full <- glmmTMB(activity_seconds ~ dist_jalama * humans_per_trapnight * season + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 


coyote_null_rndms <- glmmTMB(activity_seconds ~ 1 + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_add <- glmmTMB(activity_seconds ~ dist_jalama + humans_per_trapnight + season 
                      + (1|sitename) , 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                       na.action = "na.exclude"
                       )


coyote_dist <- glmmTMB(activity_seconds ~ dist_jalama + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_human <- glmmTMB(activity_seconds ~ humans_per_trapnight + (1|sitename) , 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_distxhum <- glmmTMB(activity_seconds ~ dist_jalama * humans_per_trapnight + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_distahum <- glmmTMB(activity_seconds ~ dist_jalama + humans_per_trapnight + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_full_distxhum <- glmmTMB(activity_seconds ~ dist_jalama * humans_per_trapnight + season + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_humxseas <- glmmTMB(activity_seconds ~ humans_per_trapnight * season + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_humaseas <- glmmTMB(activity_seconds ~ humans_per_trapnight + season + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_full_humxseas <- glmmTMB(activity_seconds ~ dist_jalama + humans_per_trapnight * season + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_distxseas <- glmmTMB(activity_seconds ~ dist_jalama * season + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_distxseas_zi <- update(coyote_distxseas, ziformula = ~1) 

coyote_distaseas <- glmmTMB(activity_seconds ~ dist_jalama + season + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_full_distxseas <- glmmTMB(activity_seconds ~ humans_per_trapnight + dist_jalama * season + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

####### make a model table ####### 
bbmle::AICtab(coyote_distxseas_zi, coyote_full, coyote_null_rndms, coyote_add, coyote_dist, coyote_human, coyote_distxhum, coyote_distahum, coyote_full_distxhum, coyote_humxseas, coyote_humaseas, coyote_full_humxseas, coyote_distxseas, coyote_distaseas, coyote_full_distxseas,  base = TRUE)


####### difference between top model and zi counterpart ########
#anova(coyote_distxseas, coyote_distxseas_zi) # giving weird results


####### average top models ########
coyote_avg <- model.avg(coyote_dist, coyote_distxseas, coyote_distaseas, coyote_distahum)

# test assumptions on full model
simulationOutput_yote <- simulateResiduals(fittedModel = coyote_distaseas) # best model
plot(simulationOutput_yote)
plotResiduals(simulationOutput_yote)
plotQQunif(simulationOutput_yote)
testQuantiles(simulationOutput_yote)
testZeroInflation(simulationOutput_yote)

#coyote_models <- c(coyote_full, coyote_1, coyote_2, coyote_null_rndms, coyote_null_rndms1, coyote_null_rndms2)
#coyote_names <- c('coyote_full', 'coyote_1', 'coyote_2', 'coyote_null_rndms', 'coyote_null_rndms1','coyote_null_rndms2')
#summary(coyote_models)
#aictab(cand.set = coyote_models)
#AIC(coyote_models)

plot(allEffects(coyote_distxseas)) # best model
# A negative interaction coefficient means that the effect of the combined action of two predictors is less then the sum of the individual effects.
r.squaredGLMM(coyote_distxseas)
```
### now pigs
```{r pig glmm}

# already made a yote df, need to update it tho
pig_activity <- activity_site_month_sp %>% 
  filter(common_name == "feral pig")

# how does the data look
ggplot(pig_activity, aes(x = activity_seconds)) +
  geom_histogram(binwidth = 1)

pig_full <- glmmTMB(activity_seconds ~ dist_jalama * humans_per_trapnight * season + (1|sitename), # not really considering third order interactions but it's interesting, should prob take these out of the table
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_full_zi <- update(pig_full, ziformula = ~1)

pig_full_pois <- update(pig_full_zi, family = "poisson") # i think this overfits somehow becaue r2 is .9999 but AIC is like 2x worse


pig_null_rndms <- glmmTMB(activity_seconds ~ 1 + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_null_rndms_zi <- update(pig_null_rndms, ziformula = ~1)

pig_add <- glmmTMB(activity_seconds ~ dist_jalama + humans_per_trapnight + season 
                      + (1|sitename) , 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                       na.action = "na.exclude"
                       )

pig_add_zi <- update(pig_add, ziformula = ~1)


pig_dist <- glmmTMB(activity_seconds ~ dist_jalama + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_dist_zi <- update(pig_dist, ziformula = ~1)

pig_human <- glmmTMB(activity_seconds ~ humans_per_trapnight + (1|sitename) , 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_human_zi <- update(pig_human, ziformula = ~1)

pig_distxhum <- glmmTMB(activity_seconds ~ dist_jalama * humans_per_trapnight + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_distxhum_zi <- update(pig_distxhum, ziformula = ~1)

pig_distahum <- glmmTMB(activity_seconds ~ dist_jalama + humans_per_trapnight + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_distahum_zi <- update(pig_distahum, ziformula = ~1)

pig_full_distxhum <- glmmTMB(activity_seconds ~ dist_jalama * humans_per_trapnight + season + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_full_distxhum_zi <- update(pig_full_distxhum, ziformula = ~1)

pig_humxseas <- glmmTMB(activity_seconds ~ humans_per_trapnight * season + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_humxseas_zi <- update(pig_humxseas, ziformula = ~1)

pig_humaseas <- glmmTMB(activity_seconds ~ humans_per_trapnight + season + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_humaseas_zi <- update(pig_humaseas, ziformula = ~1)

pig_full_humxseas <- glmmTMB(activity_seconds ~ dist_jalama + humans_per_trapnight * season + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_full_humxseas_zi <- update(pig_full_humxseas, ziformula = ~1)

pig_distxseas <- glmmTMB(activity_seconds ~ dist_jalama * season + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_distxseas_zi <- update(pig_distxseas, ziformula = ~1)

pig_distaseas <- glmmTMB(activity_seconds ~ dist_jalama + season + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_distaseas_zi <- update(pig_distaseas, ziformula = ~1)


pig_full_distxseas <- glmmTMB(activity_seconds ~ humans_per_trapnight + dist_jalama * season + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_full_distxseas_zi <- update(pig_full_distxseas, ziformula = ~1)


####### make a model table ####### 
bbmle::AICtab(pig_full_zi, pig_full, pig_null_rndms, pig_add, pig_dist, pig_human, pig_distxhum, pig_distahum, pig_full_distxhum, pig_humxseas, pig_humaseas, pig_full_humxseas, pig_distxseas, pig_distaseas, pig_full_distxseas,  base = TRUE)

bbmle::AICtab(pig_full_zi, pig_full,
              pig_null_rndms, pig_null_rndms_zi, 
              pig_add, pig_dist, pig_human, pig_add_zi, pig_dist_zi, pig_human_zi,
              pig_distxhum, pig_distahum, pig_full_distxhum, pig_distxhum_zi, pig_distahum_zi, pig_full_distxhum_zi, 
              pig_humxseas, pig_humaseas, pig_full_humxseas, pig_humxseas_zi, pig_humaseas_zi, pig_full_humxseas_zi, 
              pig_distxseas, pig_distaseas, pig_full_distxseas,  pig_distxseas_zi, pig_distaseas_zi, pig_full_distxseas_zi,  
              base = TRUE)

##### hard to pick between full and full ZI

anova(pig_full, pig_distxseas)

####### average top models ########
pig_avg <- model.avg(pig_dist, pig_distxseas, pig_distaseas, pig_distahum)

# test assumptions on full model
simulationOutput_pig <- simulateResiduals(fittedModel = pig_humaseas)
plot(simulationOutput_pig)
plotResiduals(simulationOutput_pig)
testDispersion(simulationOutput_pig)

testZeroInflation(simulationOutput_pig)

plot(allEffects(pig_full_zi))
r.squaredGLMM(pig_full_zi)
```
### finally for deer
```{r deer models}

# already made a yote df, need to update it tho
deer_activity <- activity_site_month_sp %>% 
  filter(common_name == "mule deer")

# how does the data look
ggplot(deer_activity, aes(x = activity_seconds)) +
  geom_histogram(binwidth = 1)

deer_full <- glmmTMB(activity_seconds ~ dist_jalama * humans_per_trapnight * season + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_full_zi <- update(deer_full, ziformula = ~1)


deer_null_rndms <- glmmTMB(activity_seconds ~ 1 + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_null_rndms_zi <- update(deer_null_rndms, ziformula = ~1)

deer_add <- glmmTMB(activity_seconds ~ dist_jalama + humans_per_trapnight + season 
                      + (1|sitename) , 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                       na.action = "na.exclude"
                       )
deer_add_zi <- update(deer_add, ziformula = ~1)


deer_dist <- glmmTMB(activity_seconds ~ dist_jalama + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_dist_zi <- update(deer_dist, ziformula = ~1)

deer_human <- glmmTMB(activity_seconds ~ humans_per_trapnight + (1|sitename) , 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_human_zi <- update(deer_human, ziformula = ~1)


deer_distxhum <- glmmTMB(activity_seconds ~ dist_jalama * humans_per_trapnight + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_distxhum_zi <- update(deer_distxhum, ziformula = ~1)

deer_distahum <- glmmTMB(activity_seconds ~ dist_jalama + humans_per_trapnight + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_distahum_zi <- update(deer_distahum, ziformula = ~1)

deer_full_distxhum <- glmmTMB(activity_seconds ~ dist_jalama * humans_per_trapnight + season + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_full_distxhum_zi <- update(deer_full_distxhum, ziformula = ~1)

deer_humxseas <- glmmTMB(activity_seconds ~ humans_per_trapnight * season + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_humxseas_zi <- update(deer_humxseas, ziformula = ~1)

deer_humaseas <- glmmTMB(activity_seconds ~ humans_per_trapnight + season + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_humaseas_zi <- update(deer_humaseas, ziformula = ~1)


deer_full_humxseas <- glmmTMB(activity_seconds ~ dist_jalama + humans_per_trapnight * season + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_full_humxseas_zi <- update(deer_full_humxseas, ziformula = ~1)


deer_distxseas <- glmmTMB(activity_seconds ~ dist_jalama * season + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_distxseas_zi <- update(deer_distxseas, ziformula = ~1)


deer_distaseas <- glmmTMB(activity_seconds ~ dist_jalama + season + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_distaseas_zi <- update(deer_distaseas, ziformula = ~1)


deer_full_distxseas <- glmmTMB(activity_seconds ~ humans_per_trapnight + dist_jalama * season + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_full_distxseas_zi <- update(deer_full_distxseas, ziformula = ~1)


####### make a model table ####### 
bbmle::AICtab(deer_full, 
              deer_null_rndms, 
              deer_add, deer_dist, deer_human, 
              deer_distxhum, deer_distahum, deer_full_distxhum,
              deer_humxseas, deer_humaseas, deer_full_humxseas, 
              deer_distxseas, deer_distaseas, deer_full_distxseas, base = TRUE)

bbmle::AICtab(deer_full, deer_full_zi,
              deer_null_rndms, deer_null_rndms_zi, 
              deer_add, deer_dist, deer_human, deer_add_zi, deer_dist_zi, deer_human_zi, 
              deer_distxhum, deer_distahum, deer_full_distxhum, deer_distxhum_zi, deer_distahum_zi, deer_full_distxhum_zi,
              deer_humxseas, deer_humaseas, deer_full_humxseas, deer_humxseas_zi, deer_humaseas_zi, deer_full_humxseas_zi, 
              deer_distxseas, deer_distaseas, deer_full_distxseas, deer_distxseas_zi, deer_distaseas_zi, deer_full_distxseas_zi,  base = TRUE)
              
####### average top models ########
deer_avg <- model.avg(deer_dist, deer_distxseas, deer_distaseas, deer_distahum)

# test assumptions on full model
simulationOutput_deer <- simulateResiduals(fittedModel = deer_dist)
plot(simulationOutput_deer)
plotResiduals(simulationOutput_deer)
testZeroInflation(simulationOutput_deer)

plot(allEffects(deer_distxhum))
r.squaredGLMM(deer_distaseas)

#### is the nested (simple) model better than the complex model? ####
# we can use lmtest::lrtest to compare the log-likelihoods using loglikelihood ratio calculations and chi2 test
lrtest(deer_dist, deer_distxhum, deer_distahum, deer_distaseas) # all within 2 AIC
# first three are not significantly different from each other, adding season significantly decreases performance

```


## now we look at richness/diversity with the same predictor variables
### Getting Diversity Estimates w/ Relative Activity Index (seconds/100 days)
```{r richness estimates}

# we can use the activity_site_month_sp matrix to calculate richness
richness_matrix <- activity_site_month_sp %>% 
  select(sitemonth_id, common_name, seconds_by_trapnight) %>% 
  mutate(rel_abund_index100 = ceiling(seconds_by_trapnight*100)) %>%  # i dont think shannon div handles fractions well, so we will do this to create integers
  select(-seconds_by_trapnight) %>% 
  pivot_wider(names_from = common_name, 
              values_from = rel_abund_index100,
              values_fill = 0) %>% 
  column_to_rownames("sitemonth_id")  #this is important for separating site names from count data (for matrix reasons)


# sp.count from vegan calculates richness
sp.count <- specnumber(richness_matrix) # vector

rich_df <- sp.count %>% 
  enframe()%>% 
  rename(sitemonth_id = name,
         richness = value)

# vegan::diversity() will calculate shannon (default), simpson, and fisher indices
shannondiv <- diversity(richness_matrix, index = "shannon") # default is shannon but i want ot remember
# again creates named vector 

div_df <- shannondiv %>% 
  enframe() %>% 
  rename(sitemonth_id = name, 
         shandiv = value)

# attach shannon diversity it back to the dataframe
diversity_df <- rich_df %>% 
# join with metadata: this joins shandiv_df to site_type matching shandiv_df$name to site_type$site
  full_join(., activity_site_month_sp, by = join_by(sitemonth_id)) %>%
  distinct(sitename, month, richness, .keep_all = TRUE)

# how does diversity data look?
ggplot(diversity_df, aes(x = log10(richness+1))) +
  geom_histogram()

# shandiv has:
# zero inflated gaussian, that's cool!
# not sure that's a residual distribution that exists though... might be a zero inflated gamma?
# it's not, because after doing some research, you can't take the log of zero and that fucks up a lot of model distributions like poisson that are designed for count data

# back to richness!

######## lets visualize in response to some stuff: ######## 

##### richness ~ human activity ##### 
richxhum <- ggplot(diversity_df, aes( x = humans_per_trapnight, y = richness))+
  geom_point()

richxdist <- ggplot(diversity_df, aes( x = dist_jalama, y = richness))+
  geom_point()

richxseason <- ggplot(diversity_df, aes( x = season, y = richness))+
  geom_boxplot()+
  geom_jitter(aes(color = humans_per_trapnight))

rich_seasxhum <- ggplot(diversity_df, aes( x = humans_per_trapnight, y = richness))+
  geom_point()+
  facet_wrap(~season)

mean(diversity_df$richness)
sd(diversity_df$richness)


```

### Modeling Richness

```{r diversity model}

################# let's include effort as a random effect ###################
# we know it's there, so we need ot allocate differences between samples to it, but we don't care to visualize it #

richness_full_pois <- glmmTMB(richness ~ dist_jalama * humans_per_trapnight * season + (1|sitename) + (1|trapnights), # this model is underdispersed and overfits zeros. (more zeros than expected)
                         data = diversity_df, 
                         family = compois(link = "log")
                       ) 

richness_null_rndms <- glmmTMB(richness ~ 1 + (1|sitename) + (1|trapnights),, 
                         data = diversity_df, 
                         family = compois(link = "log")
                       ) 

richness_full_seasxhum <- glmmTMB(richness ~ dist_jalama + humans_per_trapnight * season + (1|sitename) + (1|trapnights),, 
                         data = diversity_df, 
                         family = compois(link = "log")
                       ) 

richness_full_distxhum <- glmmTMB(richness ~ dist_jalama * humans_per_trapnight + season + (1|sitename) + (1|trapnights), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       ) 

richness_full_distxseas <- glmmTMB(richness ~ dist_jalama * season + humans_per_trapnight + (1|sitename)+ (1|trapnights), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       )

richness_full_add <- glmmTMB(richness ~ dist_jalama + season + humans_per_trapnight + (1|sitename) + (1|trapnights), 
                         data = diversity_df, 
                         offset = log10(trapnights)
                       )


richness_distxseas <- glmmTMB(richness ~ dist_jalama * season + (1|sitename) + (1|trapnights), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       )

richness_distaseas <- glmmTMB(richness ~ dist_jalama + season + (1|sitename)+ (1|trapnights), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       )

richness_distxhum <- glmmTMB(richness ~ dist_jalama * humans_per_trapnight + (1|sitename) + (1|trapnights), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       ) 

richness_distahum <- glmmTMB(richness ~ dist_jalama + humans_per_trapnight + (1|sitename) + (1|trapnights), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       )

richness_humxseas <- glmmTMB(richness ~ season * humans_per_trapnight + (1|sitename)+ (1|trapnights), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       )

richness_humaseas <- glmmTMB(richness ~ season + humans_per_trapnight + (1|sitename) + (1|trapnights), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       )

richness_human <- glmmTMB(richness ~ humans_per_trapnight + (1|sitename)+ (1|trapnights), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       )

richness_season <- glmmTMB(richness ~ season + (1|sitename)+ (1|trapnights), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       )

richness_distance <- glmmTMB(richness ~ dist_jalama + (1|sitename) + (1|trapnights), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       )


################# glm time ###################


richness_full_pois <- glmmTMB(richness ~ dist_jalama * humans_per_trapnight * season + (1|sitename), # this model is underdispersed and overfits zeros. (more zeros than expected)
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       ) 

richness_null_rndms <- glmmTMB(richness ~ 1 + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

richness_full_seasxhum <- glmmTMB(richness ~ dist_jalama + humans_per_trapnight * season + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       ) 

richness_full_distxhum <- glmmTMB(richness ~ dist_jalama * humans_per_trapnight + season + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       ) 

richness_full_distxseas <- glmmTMB(richness ~ dist_jalama * season + humans_per_trapnight + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       )

richness_full_add <- glmmTMB(richness ~ dist_jalama + season + humans_per_trapnight + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       )


richness_distxseas <- glmmTMB(richness ~ dist_jalama * season + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       )

richness_distaseas <- glmmTMB(richness ~ dist_jalama + season + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       )

richness_distxhum <- glmmTMB(richness ~ dist_jalama * humans_per_trapnight + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       ) 

richness_distahum <- glmmTMB(richness ~ dist_jalama + humans_per_trapnight + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       )

richness_humxseas <- glmmTMB(richness ~ season * humans_per_trapnight + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       )

richness_humaseas <- glmmTMB(richness ~ season + humans_per_trapnight + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       )

richness_human <- glmmTMB(richness ~ humans_per_trapnight + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       )

richness_season <- glmmTMB(richness ~ season + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       )

richness_distance <- glmmTMB(richness ~ dist_jalama + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log"), 
                         offset = log10(trapnights)
                       )

richness_distance0 <- glmmTMB(richness ~ dist_jalama + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       )

richness_distxseas0 <- glmmTMB(richness ~ dist_jalama * season + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       )

richness_distaseas0 <- glmmTMB(richness ~ dist_jalama + season + (1|sitename), 
                         data = diversity_df, 
                         family = compois(link = "log")
                       )


####### AIC table ########
bbmle::AICtab(richness_full, richness_null_rndms, #richness_nb, #richness_full_gaus,
              richness_full_add, richness_full_distxhum, richness_full_distxseas, richness_full_seasxhum, 
              richness_distahum, richness_distxhum, 
              richness_distaseas, richness_distxseas, 
              richness_humaseas, richness_humxseas, 
              richness_human, richness_season, richness_distance, 
              # best three models w/o offset
              #richness_distaseas0, richness_distance0, richness_distxseas0,
              base = TRUE)


# having trouble with model fit, doesn't predict the data well even though they are counts (of species)
simulationOutput_rich <- simulateResiduals(fittedModel = richness_distaseas)
plot(simulationOutput_rich)
plotResiduals(simulationOutput_rich)
testDispersion(simulationOutput_rich)
testZeroInflation(simulationOutput_rich)
countOnes <- function(x) sum(x == 1) # 
testGeneric(simulationOutput_rich, summary = countOnes) # tests for 1 iflation
performance::check_overdispersion(simulationOutput_rich)


#summary(richness_1)
#summary(richness_add)
summary(richness_distance)
summary(richness_distxseas)
summary(richness_full_distxseas)

# top 3 models
r.squaredGLMM(richness_distance)
r.squaredGLMM(richness_distxseas) # highest r2
r.squaredGLMM(richness_full_distxseas) 

#plot(effects::allEffects(richness_distance)) # somehow effects:: is breaking when we use a genpois distribution. something about mu gets funky
#plot(allEffects(richness_distxseas))
#plot(allEffects(richness_distaseas))
#plot(allEffects(richness_gaus))

fct_relevel(f)
fct_relevel(f, "a")

plot(x = predict(richness_distxseas), y = (diversity_df$richness))

```


# Finally we can make some pretty prediction figures
```{r activity prediction graphs}
 # going to start by mutating the raw dataframe a bit for plotting
activity_site_month_sp_plot <- activity_site_month_sp %>% 
    mutate(human_activity_rank = case_when(total_human_sec < 0 ~ "none",
                                           total_human_sec > 0 & total_human_sec < 100   ~ "moderate",
                                           total_human_sec > 100 ~ "high"))

all_activity_ZI <-
  data.frame(predict_response(glmm_3x_z, terms = c("dist_jalama [0:12]", "season")))

activity_plot_ZI <- 
  ggplot(all_activity_ZI, aes (x, predicted, colour = group)) +
  geom_line()+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), linetype = 0, alpha = 0.1)+
  xlab("Distance From Jalama Beach Campground (km)")+
  ylab("Predicted Mammal Activity (seconds/day)")+
  scale_fill_manual(name = "", 
                    values = c("wet" = "dodgerblue1", "dry" = "gold"),
                      breaks = c("wet", "dry"),
                       labels = c("Wet Season (Oct-Mar)", "Dry Season (Apr-Sep)"))+
  scale_color_manual(name = "",
                     values = c("wet" = "dodgerblue1", "dry" = "gold"),
                       breaks = c("wet", "dry"),
                       labels = c("Wet Season (Oct-Mar)", "Dry Season (Apr-Sep)"))+
  #theme(legend.title = element_blank())+
  theme_classic()+
  geom_jitter(data = activity_site_month_sp_plot, aes(x = dist_jalama, y = seconds_by_trapnight, colour = season), shape = 1) # seconds per trapnight
  #scale_color_discrete(name = "",
                       #breaks = c(,3,6,9),
                       #labels = c("At Jalama Beach Campground", "3 Kilometers Away", "6 Kilometers Away", "9 Kilometers Away")
#                       )



#png(here("figures/mammal_activity_seasxdist.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#activity_plot_ZI
#dev.off()

```

### coyotes
```{r coyotes prediction graph}

## for fun
dotwhisker::dwplot(coyote_distxseas) # plotting effect size + 95 ci on a dot whisker plot

############# ~ DISTANCE #############
coyote_dist_response <- data.frame(predict_response(coyote_distxseas, terms = c("dist_jalama [0:12]", "season"))) %>% 
  rename(season = group)

coyote_dist_plot <- ggplot(coyote_dist_response, aes(x, predicted, linetype = season)) +
  geom_line(aes(colour = season))+
  scale_color_manual(name = "", 
                    values = c("wet" = "dodgerblue1", "dry" = "gold")
                      )+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = season), alpha = 0.2)+
  scale_fill_manual(name = "", 
                    values = c("wet" = "dodgerblue1", "dry" = "gold"),
                      breaks = c("wet", "dry"),
                       labels = c("Wet Season (Oct-Mar)", "Dry Season (Apr-Sep)"))+
  xlab("Distance From Jalama Beach Campground (km)")+
  ylab("Predicted Coyote Activity (seconds)")+
  theme_classic() +
  geom_jitter(data = coyote_activity, aes(x = dist_jalama, y = seconds_by_trapnight, color = season), shape = 1)+
  scale_color_manual(name = "", 
                    values = c("wet" = "dodgerblue1", "dry" = "gold")
                      )+
  # remove some legends:
  guides(color = "none", linetype = "none")

#png(here("figures/coyote_activity_inxn.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#coyote_dist_plot
#dev.off()

############# ~ SEASON #############
coyote_season_response <- data.frame(predict_response(coyote_add, terms = c("season"))) 

coyote_season_plot <- ggplot(coyote_season_response, aes(x, predicted)) +
  geom_point( size = 4)+
  geom_errorbar(aes(x, ymin = conf.low, ymax = conf.high))+
  #scale_color_manual(name = "", 
  #                  values = c("wet" = "dodgerblue1", "dry" = "gold")
  #                    )+
  # remove some legends:
  #guides(color = "none")
  xlab("Season")+
  ylab("Predicted Coyote Activity (seconds/day)")+
  theme_classic()+
  geom_jitter(data = coyote_activity, aes(x = season, y = seconds_by_trapnight, color = dist_jalama), shape = 1)+
  scale_color_continuous(type = "viridis", 
                         name = "") # add in Distance from Jalama Beach Campground (km) in post

#png(here("figures/coyote_activity_season.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#coyote_season_plot
#dev.off()

r.squaredGLMM(coyote_add)
r.squaredGLMM(coyote_1)

```
### pigs
```{r pig prediction graph}

############# ~ DISTANCE #############
#pig_response_dist <- data.frame(predict_response(pig_full, terms = c("dist_jalama", "total_human_sec [0, 100, 200]", "season")))

#pig_dist_plot <- ggplot(pig_response_dist, aes (x, predicted, colour = group)) +
#  geom_line()+
#  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1)+
#  xlab("Distance From Jalama Beach (km)")+
#  ylab("Predicted Pig Activity (seconds)")+
#  facet_wrap(~facet)
#  theme_classic()
  #geom_jitter(data = pig_activity, aes(x = dist_jalama, y = activity_seconds, colour = property))

#png(here("figures/pig_activity_dist.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#pig_dist_plot
#dev.off()

############# ~ HUMAN #############
#pig_response_humans <- data.frame(predict_response(pig_full, terms = "human_seconds"))

#pig_human_plot <- ggplot(pig_response_humans, aes (x, predicted)) +
#  geom_line(colour = "#16FF32")+
#  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1, fill = "#16FF32")+
#  xlab("Human Activity (seconds)")+
#  ylab("Predicted Pig Activity (seconds)")+
#  theme_classic()
  #geom_jitter(data = deer_activity, aes(x = dist_jalama, y = activity_seconds, colour = property))

#png(here("figures/pig_activity_human.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#pig_human_plot
#dev.off()

############# ~ DISTANCE * HUMAN #############
# make a histogram to pick relevant levels of human activity
human_levels <- ggplot(activity_site_month_sp, aes(x = humans_per_trapnight))+
  geom_histogram(bins = 30)

pig_response <- data.frame(predict_response(pig_full_zi, terms = c( "dist_jalama [0:12]", "season"), type = "zero_inflated"))

pig_response_full <- data.frame(predict_response(pig_full_zi, terms = c( "dist_jalama [0:10]", "humans_per_trapnight [0, 20, 40]", "season"), type = "zero_inflated")) %>% 
  rename(season = facet)




pig_resp_humxseason <- data.frame(predict_response(pig_full_zi, terms = c("humans_per_trapnight[0:50]", "season"), type = "zero_inflated")) %>% 
  rename(season = group)

pig_humxseason_plot <- ggplot(pig_resp_humxseason, aes(x = x, y = predicted, colour = season)) +
  geom_line()+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1)+
  xlab("Human Activity (seconds/day)")+
  ylab("Predicted Pig Activity (seconds/day)")+
  #scale_color_discrete(name = "", 
  #                    breaks = c(0,3,6,9),
  #                     labels = c("At Campground", "3km", "6km", "9km"))+
  #scale_fill_discrete(name = "",
  #                     breaks = c(0,3,6,9),
  #                     labels = c("At Campground", "3km", "6km", "9km"))+
  theme_classic()+
  facet_wrap(~season)+
  geom_jitter(data = pig_activity, aes(x = humans_per_trapnight, y = seconds_by_trapnight, colour = season))



####### full model (sucks) #######
pig_full_plot  <- ggplot(pig_response_full, aes(x = x, y = predicted, color = group)) +
  geom_line()+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), linetype = 0, alpha = 0.1)+
  #xlab("Human Activity (seconds/day)")+
  xlab("Distance from Jalama Campground (km)")+
  ylab("Predicted Pig Activity (seconds/day)")+
  scale_color_discrete(name = "", 
                      breaks = c(0,20,40),
                       labels = c("none", "20 sec/night", "40 sec/night"))+
  scale_fill_discrete(name = "",
                       breaks = c(0,20,40),
                       labels = c("none", "20 sec/night", "40 sec/night"))+
  theme_classic()+
  geom_jitter(data = pig_activity, aes(x = dist_jalama, y = seconds_by_trapnight, color = sitename))

####### plotting the two seasons separately for scale ####### 

pig_raw_wetonly <- pig_activity %>% 
  filter(season == "wet")
pig_response_wet <- pig_response_full %>% 
  filter(season == "wet") %>%
  rename(humans = group)
pig_full_wet <- ggplot(pig_response_wet, aes(x = x, y = predicted, color = humans)) +
  geom_line()+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = humans), linetype = 0, alpha = 0.1)+
  #xlab("Human Activity (seconds/day)")+
  xlab("Distance from Jalama Campground (km)")+
  ylab("Predicted Pig Activity (seconds/day)")+
  scale_color_discrete(name = "", 
                      breaks = c(0,20,40),
                       labels = c("none", "20 sec/night", "40 sec/night"))+
  scale_fill_discrete(name = "",
                       breaks = c(0,20,40),
                       labels = c("none", "20 sec/night", "40 sec/night"))+
  theme_classic()+
  geom_jitter(data = pig_raw_wetonly, aes(x = dist_jalama, y = seconds_by_trapnight, color = "blue"))
  
  
pig_full_dry <-

#png(here("figures/pig_activity_inxn.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#pig_inxn_plot
#dev.off()

r.squaredGLMM(pig_full)
r.squaredGLMM(pig_1)

```

### deer
```{r deer prediction graph}

############# ~ DISTANCE #############
deer_dist_response <- data.frame(predict_response(deer_dist, terms = c("dist_jalama [0:12]"))) 

deer_dist_plot <- ggplot(deer_dist_response, aes (x, predicted)) +
  geom_line(colour = "#1CBE4F")+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1, fill = "#1CBE4F")+
  xlab("Distance From Jalama Beach (km)")+
  ylab("Predicted Deer Activity (seconds)")+
  theme_classic()+
  geom_jitter(data = deer_activity, aes(x = dist_jalama, y = seconds_by_trapnight, colour = season), shape = 1)+
  scale_color_manual(name = "Season", 
                    values = c("wet" = "dodgerblue1", "dry" = "gold")
                      )

#png(here("figures/deer_activity_dist.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#deer_dist_plot
#dev.off()

############# ~ HUMAN #############
#deer_human_response <- data.frame(predict_response(deer_add, terms = c("human_seconds"))) 

#deer_human_plot <- ggplot(deer_human_response, aes (x, predicted)) +
#  geom_line(colour = "#1CBE4F")+
#  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1, fill = "#1CBE4F")+
#  xlab("Human Activity (seconds)")+
#  ylab("Predicted Deer Activity (seconds)")+
#  theme_classic()
  #geom_jitter(data = deer_activity, aes(x = dist_jalama, y = activity_seconds, colour = property))

#png(here("figures/deer_activity_human.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#deer_human_plot
#dev.off()

r.squaredGLMM(deer_add)
r.squaredGLMM(deer_1)

```


#### Making a combo plot for all species responses
```{r combo species activity}
# using same colors as before:
IDcolors <- alphabet.colors(15)
# coyote is 'forest' #1C8356
# pig is 'green' #16FF32
# deer is 'jade' #1CBE4F

pig_response_dist <- data.frame(predict_response(pig_full, terms = "dist_jalama"))
pig_response_humans <- data.frame(predict_response(pig_full, terms = "human_seconds"))

all_spp_humans <- ggplot()+ # something is funky so i'm having to include 'mapping =' in front of aes() to get it to recognize
  geom_line(coyote_human_response, mapping = aes(log10(x), log10(predicted)), colour = "#1C8356")+
  #geom_ribbon(coyote_human_response, mapping = aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1)+
  
  geom_line(pig_response_humans, mapping = aes(x = log10(x), y = log10(predicted)), colour = "#16FF32")+
  #geom_ribbon(pig_response_humans, mapping = aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1)+
  
  geom_line(deer_human_response, mapping = aes(x = log10(x), y = log10(predicted)), colour = "#1CBE4F")+
  #geom_ribbon(deer_human_response,mapping =  aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1)+
  
  xlab("Distance From Jalama Beach (km)")+
  ylab("Predicted Mammal Activity (seconds)")+
  theme_classic()

# ok this sucks even when log transformed

```
### Finally plotting richness predictions
```{r richenss response}

############# ~ DISTANCExSEASON #############
richness_dist_response <- data.frame(predict_response(richness_full_distxseas, terms = c("dist_jalama", "season"))) %>% 
  rename(season = group)

richness_inxn_plot <- ggplot(richness_dist_response, aes (x, predicted, color = season)) +
  geom_line()+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = season), linetype = 0, alpha = 0.1)+
  geom_point(data = diversity_df, aes(x = dist_jalama, y = richness, colour = season), shape =1)+
  xlab("Distance From Jalama Beach (km)")+
  ylab("Predicted Richness")+
  scale_color_manual(name = "Season", 
                    values = c("wet" = "dodgerblue1", "dry" = "gold")
                      )+
    scale_fill_manual(name = "Season", 
                    values = c("wet" = "dodgerblue1", "dry" = "gold")
                      )+
  ylim(1, 6) +
  theme_classic()

#png(here("figures/richness_distxseas.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#richness_inxn_plot
#dev.off()

############# ~ HUMAN #############
richness_human_response <- data.frame(predict_response(richness_full_distxseas, terms = c("humans_per_trapnight"))) 

richness_human_plot <- ggplot(richness_human_response, aes (x, predicted)) +
  geom_line(colour = "#1CBE4F")+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high), linetype = 0, alpha = 0.1, fill = "#1CBE4F")+
  xlab("Human Activity (seconds/trap-night)")+
  ylab("Predicted Species Richness")+
  theme_classic()+
  geom_jitter(data = diversity_df, aes(x = humans_per_trapnight, y = richness))

#png(here("figures/richness_activity_human.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#richness_human_plot
#dev.off()

r.squaredGLMM(richness_add)
r.squaredGLMM(richness_1)
r.squaredGLMM(richness_gaus)

```



# Leaving the world of Frequentist Stats now...
#### We need to bin the data into 1 day "surveys" so that we can run a multi-species occupancy model using spOccupancy and have multiple samples per month, also need to plug in zeros
```{r make new dataframe with 1 day samples}
# marisa is using a different dataset for the bayesian stuff that I made her - it includes daily human counts (although a lot fewer than our estimates suggest)
## this is the one I made for her
mammals_humans <- read_csv(here("data/classifications_w_humans_metadata_full_9aug2024.csv")) %>% 
  mutate(date = mdy(date)) %>% 
  rename(trapnights = days_per_month)

all_days_sampled # will need this dataframe

all_days_sampled2 <- all_days_sampled %>% 
  mutate(date = date(each_date)) %>% # hopefully this column will let us full_join on date
  mutate(year = year(each_date)) %>% 
  mutate(day = day(each_date)) %>% 
  mutate(number_of_objects = 0) %>% 
  rename(timestamp = each_date) %>% 
  unite("sitemonth_id", c(sitename, month), sep = ".", remove = FALSE) %>% 
  select(property, deployment_id, sitename, month, sitemonth_id, year, day, date, number_of_objects, timestamp)  %>% 
  #mutate(check = gl(7, 1, length = length(site_sample_id))) # gl is apparently a way to generate a sequence that i'll never remember (7 integers, repeat each integer once, until end of df) don't need this anymore

# still need to add in metadata to this so it's filled in correctly during the full bind, including dist, habitats, lat lon, iz type, burst settings, humans, and trapnights
  left_join(., metadata_by_month, by = join_by(property, sitename, month, sitemonth_id)) %>% 
  select(!c(human_seconds, n_human_shots)) # will calculate this from human data in dataframe


mammals_occupancy_full <- mammals_humans %>% 
  full_join(., all_days_sampled2 #, by = join_by(property, deployment_id, sitename, month, sitemonth_id, year, day, date, number_of_objects, timestamp) # so many columns to bind on, but it should automatically select all the similar ones
            ) 

#write_csv(mammals_occupancy, "mammals_occupancy.csv")

mammals_occupancy_byday <- mammals_occupancy_full %>% 
  group_by(property, sitename, dist_jalama, utm_x, utm_y, habitat_adjacent, habitat_secondary, iz_type, burst_settings, # site specific grouping vars
           month, sitemonth_id, trapnights, # month specific grouping vars
           date, # day specific grouping vars
           common_name # need sums by species
           ) %>% 
  summarise(raw_daily_counts = sum(number_of_objects)) %>% 
  
  # now we need to pivot to remove NA rows and insert zeroes
  
  pivot_wider(names_from = common_name, 
              #names_sep = "_",
              names_prefix = "sp_",
              values_from = raw_daily_counts, 
              values_fill = 0) %>% 
  select(!sp_NA) %>% # we no longer need this columns now that all the zeros are filled in, so we should be able to pivot longer now and get a count of spp for every day sampled
  pivot_longer(cols = starts_with("sp_"),
               names_to = "common_name",
               names_prefix = "sp_",
               values_to = "raw_daily_counts") %>% 
  mutate(binary_occupancy = case_when(raw_daily_counts > 1 ~ 1,
                                      raw_daily_counts == 0 ~ 0)) # creates a binary column for occupancy

write_csv(mammals_occupancy_byday, here("data/mammals_daily_occupancy.csv"))

```


# graveyard

## First time we did this, we used the data binned by month. Now, 9/3, we're using data binned by week
So going through and replacing df: site_month_sp with _7day (y from here on out will be )
these were not my favorite results so moving them to graveyard
```{r glmm activity model selection 7 day sample}

# we know we need a mixed model because we need a random nesting effect of property and site x month
# variables of interest are monthly human seconds, season, distance from jalama, species, and interactions between all
# number of trapnights logged as an offset -- not sure why we log this but other papers do it (because there's a log link for the rest of the function)

incomplete <- activity_7day %>% 
  filter(!complete.cases(.)) #make sure every row has all data (no NAs)

# real random effect here, in this nested design, would be SITE as the thing that is repeatedly sampled, but site = distance from jalama and is the variable we are interested in...
# week/survey_id is also nested within site (and repeatedly sampled), so is a random variable, but it's the level of observation atm since we binned (summed) all occurences into weekly sums - i don't believe we need to include the level of sampling as an error term

# many models standardize their explanatory variables... why?
## found the answer here: https://stats.stackexchange.com/questions/35071/what-is-rank-deficiency-and-how-to-deal-with-it/35077#35077
### "The trick is usually to use common units, but on some problems even that is an issue when variables vary by too many orders of magnitude. More important is to scale your numbers to be similar in magnitude."

############### VARS ############### 

# RESPONSE variable is activity_seconds, offset is weekly_trapnights
# SAMPLES are 4 weeks x 9 months x 30 sites per species (although a lot of zeros)
# RANDOM variable is survey_id
# potential EXPLANATORY variables are:
## property: VSFB, JLDP, Jalama ## rank deficient
## month (going to see if this is sig and then replace it with an explanatory variable)
## common_name (species) ## rank deficient
## iz_type ## rank deficient
## dist_jalama
## human_seconds

glmm_null <- glmmTMB(activity_seconds ~ 1, 
                     data = activity_7day, 
                     family = nbinom2(link = "log"), 
                     offset = log10(weekly_trapnights), 
                         na.action = "na.fail")

glmm_null_randoms <- glmmTMB(activity_seconds ~ 1 + (1|sitename), 
                             data = activity_7day, 
                             family = nbinom2(link = "log"), 
                             offset = log10(weekly_trapnights), 
                         na.action = "na.fail")

glmm_full_inx <- glmmTMB(activity_seconds ~ month + human_seconds + dist_jalama +
                         #inxs
                           (month * human_seconds) +
                           (month * dist_jalama) +
                           (dist_jalama * human_seconds) +
                          + (1|sitename), 
                         data = activity_7day, 
                         family = nbinom2(link = "log"), 
                         offset = log10(weekly_trapnights), 
                         na.action = "na.fail") # necessary for dredge to work

glmm_full_add <- 


glmm_1 <- glmmTMB(activity_seconds ~ dist_jalama + human_seconds 
                          + (1|sitename) + (1|property), 
                         data = activity_7day, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_2 <- glmmTMB(activity_seconds ~ dist_jalama
                          + (1|sitename) + (1|property), 
                         data = activity_7day, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_3 <- glmmTMB(activity_seconds ~ human_seconds 
                          + (1|sitename) + (1|property), 
                         data = activity_7day, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail")

glmm_full_off <- glmmTMB(activity_seconds ~ dist_jalama * humans_per_trapnight #* common_name 
                          + (1|sitename) + (1|property), 
                         data = activity_site_month_sp, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         na.action = "na.fail") # necessary for dredge to work

#glmm_5 <- glmmTMB(activity_seconds ~ dist_jalama + human_seconds + protection_rank #* common_name 
#                          + (1|sitename), 
#                         data = activity_7day, 
#                         family = nbinom2(link = "log"), 
#                         offset = log10(trapnights), 
#                         na.action = "na.fail") 

#glmm_full_off <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds * protection_rank #* common_name 
#                          + (1|sitename), 
#                         data = activity_7day, 
#                         family = nbinom2(link = "log"), 
#                         offset = log10(trapnights), 
#                         na.action = "na.fail") # necessary for dredge to work

activity_models <- c(glmm_full_off, glmm_1, glmm_2, glmm_3, glmm_null_randoms, glmm_null_randoms1, glmm_null_randoms2)
model_names <- c('glmm_full_off', 'glmm_1', 'glmm_2', 'glmm_3', 'glmm_null_rndms', 'glmm_null_rndms1','glmm_null_rndms2')
#aictab(cand.set = activity_models)

summary(glmm_full_off)
#offset(activity_7day$days_active)

simulationOutput <- simulateResiduals(fittedModel = glmm_full_off)
plot(simulationOutput)
plotResiduals(simulationOutput)
testZeroInflation(simulationOutput)
#testTemporalAutocorrelation(simulationOutput)

### trying a zero inflated version, apparently this is how you code it in?? source:
# https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html#interpreting-residuals-and-recognizing-misspecification-problems

#glmm_full_ZI <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds * common_name 
#                          + (1|sitename) + (1|property) + offset(log10(trapnights)), 
#                         data = activity_7day, 
#                         family = nbinom2(link = "log"), 
#                         ziformula = ~ dist_jalama + human_seconds + common_name,
#                         na.action = "na.fail")
#summary(glmm_full_ZI)
#res<- simulateResiduals(glmm_full_ZI, plot = T)

plot(allEffects(glmm_full_off))
r.squaredGLMM(glmm_full_off)

```

### Testing out GAMM for pig data
```{r pig gamm}

# code from Zuur video @ end, they didnt actually cover it
# got help here: https://www.youtube.com/watch?v=0zZopLlomsQ
# Section 11: Poisson GAMM----

# alain's code:
#' This should have been the second model, or perhaps even the starting
#' point of the entire analysis.
M5 <- gam(NCalls ~ 1 + SexParent + FoodTreatment + 
                   s(ArrivalTime, bs = "cr") + # s means spline - controls the complexity of spline, bs is spline type, m is penalty for ??
                   s(Nest, bs = "re"), # or this
          select = TRUE, # or this
          method = "REML",
          data = Owls,
          family = "poisson")

summary(M5)
draw(M5, select = 1)

### try with our pig data
pig_gam1 <- gam(activity_seconds ~ humans_per_trapnight * season + # inxs, how will this handle??
                  s(dist_jalama, bs = ),
                data = pig_activity, 
                method = "REML", 
                family = "poisson")


#' Now the whole game starts over again. 
#'  -Execute a NB GAMM.
#'  -Execute a ZIP GAMM.
#'  -Execute a ZINB GAMM.

#' In our course:  
#'    Zero-inflated GLM, GAM, GLMM and GAMM for the analysis of 
#'    spatial and spatial-temporal correlated data using R-INLA
#' we apply a zero-inflated Poisson GAM with spatial correlation in 
#' these data. The reason for that is as follows.

#' We will extract the estimated random effects for the nests, obtained by
#' the GAMM.



#' We first specify a MyData data frame containing the Site identities
MyData <- data.frame(Nest = levels(Owls$Nest))

#' Next we set all other covariates to something (what is irrelevant).
MyData$ArrivalTime   <- 0
MyData$SexParent     <- factor("Female", levels = c("Female", "Male"))
MyData$FoodTreatment <- factor("Deprived", levels = c("Deprived", "Satiated"))


#' This is what we just created:
head(MyData, 50)


#' We then use the predict function with type = "terms"
P5 <- predict(M5, 
              MyData, 
              type = "terms", 
              se.fit = TRUE)
head(P5$fit)

#' If you were to add up all the values on a specific row, then
#' We extract the estimated random effects and their standard errors:
MyData$ai    <- P5[["fit"]][ , "s(Nest)"]
MyData$ai.se <- P5[["se.fit"]][ , "s(Nest)"]

#' Here is our final product. The ai column contains the estimated
#' random effects. They are the same as those in coef(M2). There is no
#' benefit in extracting them in this way.
head(MyData)

#' We put the random effects in a dotchart.
MyData$RowID <- 1:nrow(MyData)

p <- ggplot(data = MyData, 
            aes(x = ai, 
                y = RowID)) 
p <- p + geom_point(size = 1, col = grey(0.5)) 
p <- p + xlab("Estimated random effect")
p <- p + ylab("Nest identity")                       
p <- p + theme(text = element_text(size = 15)) 
p <- p + geom_vline(xintercept = 0, lty = 2, col = "red")
p

#' And add errorbars.
p2 <- p + geom_errorbarh(data = MyData,
                         aes(y = RowID, 
                             xmax = ai + 1.96 * ai.se, 
                             xmin = ai - 1.96 * ai.se),
                         col = "blue",
                         height = 0.05,
                         alpha = 0.2)
p2
#' We can't see the spatial information in this graph.
#' To do this, we first calculate average latitude and longitude 
#' values for each site.
MyData$Latitude  <- tapply(Owls$Latitude, FUN = mean, INDEX = Owls$Nest)
MyData$Longitude <- tapply(Owls$Longitude, FUN = mean, INDEX = Owls$Nest)

#' We will plot longitude versus latitude, and superimpose the 
#' random effects on this graph. We will use different colours
#' for positive and negative random effects, and a large dot
#' refers to a large (in absolute sense) random effect. There should
#' be no spatial clustering of random effects.
MyData$Size <- abs(MyData$ai)
MyData$Sign <- ifelse(MyData$ai >=0, "Positive", "Negative")

p <- ggplot(data = MyData, 
            aes(x = Longitude, 
                y = Latitude,
                size = Size,
                col = Sign)) 
p <- p + geom_point() 
p <- p + xlab("Longitude")
p <- p + ylab("Latitude")     
p <- p + coord_fixed(ratio = 1) 
p <- p + theme(text = element_text(size = 15)) 
p
#' Bugger...do we have a spatial pattern in these random effects?


```


### Old model plots
```{r}
#all_activity_ZI <-
#  data.frame(predict_response(glmm_full_distxhum_zi, terms = c("dist_jalama [0:12]", "total_human_sec [0, 100, 200]"))) # not the best model anymore but keeping it because good code

#activity_plot_ZI <- 
#  ggplot(all_activity_ZI, aes (x, predicted, colour = group)) +
#  geom_line()+
#  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), linetype = 0, alpha = 0.1)+
#  xlab("Distance From Jalama Beach (km)")+
#  ylab("Predicted Mammal Activity (seconds/day)")+
#  scale_fill_discrete(name = "", 
#                      breaks = c(0,100,200),
#                       labels = c("No Human Activity", "Moderate Human Activity (100 seconds/day)", "High Human Activity (200 seconds/day)"))+
#  scale_color_discrete(name = "",
#                       breaks = c(0,100,200),
#                       labels = c("No Human Activity", "Moderate Human Activity (100 seconds/day)", "High Human #Activity (200 seconds/day)"))+
  #theme(legend.title = element_blank())+
#  theme_classic()+
#  geom_jitter(data = activity_site_month_sp, aes(x = dist_jalama, y = seconds_by_trapnight, colour = property), shape = 1)+ # seconds per trapnight
#  scale_color_discrete(name = "",
                       #breaks = c(,3,6,9),
                       #labels = c("At Jalama Beach Campground", "3 Kilometers Away", "6 Kilometers Away", "9 Kilometers Away")
#                       )


######## with distance from jalama on the x and human activity as separate curves ######## 

all_activity_response1 <- #plot( # plot function is helpful but i want ot ggplot
  data.frame(predict_response(glmm_full_distxhum, terms = c("dist_jalama", "total_human_sec [0, 100, 200]"))) # old syntax was ggpredict, now it's predict_response, brackets indicate desired values of factor
             #condition = c(trapnights = mean(activity_site_month_sp$trapnights)) # apparently this is how we deal with offset, according to [https://stackoverflow.com/questions/75998389/how-to-get-ggeffects-to-use-the-actual-offset-values-rather-than-the-average] ## might not be necessary, see below
             

# from github fix log for ggeffects:
#When the model formula contains an offset-term, and the offset term is fixed at a specific value, the response variable is now automatically transformed back to the original scale, and the offset-term is added to the predicted values. A warning is printed when model contains transformed offset-terms that are not fixed, e.g. via the condition argument.
# SO PRETTY SURE the predicted value is value/mean offset = activity seconds per day

activity_plot <- ggplot(all_activity_response, aes (x, predicted, colour = group)) +
  geom_line()+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), linetype = 0, alpha = 0.1)+
  xlab("Distance From Jalama Beach (km)")+
  ylab("Predicted Mammal Activity (seconds)")+
  scale_fill_discrete(name = "", 
                      breaks = c(0,100,200),
                       labels = c("No Human Activity", "Moderate Human Activity (100 seconds)", "High Human Activity (200 seconds)"))+
  scale_color_discrete(name = "",
                       breaks = c(0,100,200),
                       labels = c("No Human Activity", "Moderate Human Activity (100 seconds)", "High Human Activity (200 seconds)"))+
  #theme(legend.title = element_blank())+
  theme_classic()+
  geom_jitter(data = activity_site_month_sp, aes(x = dist_jalama, y = seconds_by_trapnight, colour = property), shape = 1)

#png(here("figures/total_activity_model.png"), width = 10, height = 8, units = "in", pointsize = 12, res = 1000)
#activity_plot
#dev.off()

######## with human activity on the x and distance as separate curves ######## 

all_activity_response2 <- #plot( # plot function is helpful but i want ot ggplot
  data.frame(predict_response(glmm_full_distxhum, terms = c("total_human_sec [0:2000]", "dist_jalama [0, 3, 6, 9]"))) # brackets indicate desired values of factor


activity_plot2 <- ggplot(all_activity_response2, aes (x, predicted, colour = group)) +
  geom_line()+
  geom_ribbon(aes(x, ymin = conf.low, ymax = conf.high, fill = group), linetype = 0, alpha = 0.1)+
  xlab("Human Activity (seconds)")+
  ylab("Predicted Mammal Activity (seconds)")+
  scale_fill_discrete(name = "", 
                      breaks = c(0,3,6,9),
                       labels = c("At Jalama Beach Campground", "3 Kilometers Away", "6 Kilometers Away", "9 Kilometers Away"))+
  scale_color_discrete(name = "",
                       breaks = c(0,3,6,9),
                       labels = c("At Jalama Beach Campground", "3 Kilometers Away", "6 Kilometers Away", "9 Kilometers Away"))+
  #theme(legend.title = element_blank())+
  theme_classic()+
  geom_jitter(data = activity_site_month_sp, aes(x = total_human_sec, y = seconds_by_trapnight, colour = property))
```


### Visualize Data ~ Point Conception
```{r plot diffs above and below PC, eval=FALSE, include=FALSE}

pc_violin <- ggplot(activity_site_month_sp, aes(x = point_conception, y = activity_seconds)) +
  geom_violin()

pc_boxplot <- ggplot(activity_site_month_sp, aes(x = point_conception, y = activity_seconds)) +
  geom_boxplot()

```
#### Run a T-Test
```{r point conception t test, eval=FALSE, include=FALSE}

activity_site_month_ttest <- activity_site_month_sp %>% 
  filter(!point_conception == "north of Jalama")

t.test(activity_seconds ~ point_conception, data = activity_site_month_ttest)

```
#### Full community t-test was non-sig so investigating species by species//some groups
```{r pc hypothesis with spp and groups, eval=FALSE, include=FALSE}

######### make table of animal activity ( = incidences adjusted by burst number and corrected for by trap nights) by site and month ###########

activity_site_month_sp <- mammals_clean %>% # first append monthly trapnights to full dataset
  group_by(sitename, month, dist_jalama, days_active, burst_settings, utm_x, utm_y, common_name) %>% 
  summarize(total_captures = n(), # count number of rows (observations) in each category
            ) %>% # had a hard time with the other columns so I'm tossing them out into a mutate:
  mutate(activity_seconds = ceiling(total_captures/burst_settings)) %>%  # adjusts # of captures by the number of images taken in a burst (8 or 10)), rounds them UP to integers for glms
  mutate(seconds_by_trapnight = activity_seconds/as.numeric(days_active)) %>% 
  # we're interested in whether sites are above or below point conception (south of jalama) # because people can't really walk past it from jalama, so we will mutate in a column
  mutate(point_conception = case_when(utm_y < -120.452725 & utm_y > -120.506037 ~ "above", #lon of PC and south of north jalama cam
                                      utm_y > -120.452725 ~ "below",
                                      utm_y < -120.506037 ~ "north of Jalama")) # lon of north jalama cam
  
  
######################################### visualize ###############################################

pc_violin_byspp <- ggplot(activity_site_month_sp, aes(x = point_conception, y = activity_seconds)) +
  geom_violin()+
  facet_wrap(~common_name)

pc_boxplot_byspp <- ggplot(activity_site_month_sp, aes(x = point_conception, y = activity_seconds)) +
  geom_boxplot()+
  facet_wrap(~common_name)

########################################### coyotes only #############################################
coyote_activity <- activity_site_month_sp %>% 
  filter(common_name == "coyote") #%>% 
  filter(!point_conception == "north of Jalama")

t.test(activity_seconds ~ point_conception, data = coyote_activity)


```


#### original species models below:
```{r coyote glmm}

# already made a yote df, need to update it tho
coyote_activity <- activity_site_month_sp %>% 
  filter(common_name == "coyote")

# how does the data look
ggplot(coyote_activity, aes(x = activity_seconds)) +
  geom_histogram()

# poisson diagnostics NOT good, def a neg binom situation
#coyote_full <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds #+ (1|sitename) 
#                     + (1|property), # swapping from glmmTMB to glmer to see if it'll let us make an AIC table
#                         data = coyote_activity, 
#                         family = poisson(link = "log"), 
#                         offset = log10(trapnights), 
#                         #na.action = "na.fail",# necessary for dredge to work
#                       na.action = "na.exclude"
#                       ) 

coyote_full <- glmmTMB(activity_seconds ~ dist_jalama * total_human_sec * season + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 


coyote_null_rndms <- glmmTMB(activity_seconds ~ 1 + (1|sitename) + (1|property), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_null_rndms1 <- glmmTMB(activity_seconds ~ 1 + (1|sitename), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_null_rndms2 <- glmmTMB(activity_seconds ~ 1 + (1|property), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_null <- glmmTMB(activity_seconds ~ 1, 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_add <- glmmTMB(activity_seconds ~ dist_jalama + human_seconds + (1|sitename) + (1|property), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                       na.action = "na.exclude"
                       )


coyote_1 <- glmmTMB(activity_seconds ~ dist_jalama + (1|sitename) + (1|property), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

coyote_2 <- glmmTMB(activity_seconds ~ human_seconds + (1|sitename) + (1|property), 
                         data = coyote_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 


# test assumptions on full model
simulationOutput_yote <- simulateResiduals(fittedModel = coyote_full)
plot(simulationOutput_yote)
plotResiduals(simulationOutput_yote)
testZeroInflation(simulationOutput_yote)

#coyote_models <- c(coyote_full, coyote_1, coyote_2, coyote_null_rndms, coyote_null_rndms1, coyote_null_rndms2)
#coyote_names <- c('coyote_full', 'coyote_1', 'coyote_2', 'coyote_null_rndms', 'coyote_null_rndms1','coyote_null_rndms2')
#summary(coyote_models)
#aictab(cand.set = coyote_models)
#AIC(coyote_models)

plot(allEffects(coyote_add))
r.squaredGLMM(coyote_add)
```
### now pigs
```{r pig glmm}

# already made a yote df, need to update it tho
pig_activity <- activity_site_month_sp %>% 
  filter(common_name == "feral pig")

# how does the data look
ggplot(pig_activity, aes(x = activity_seconds)) +
  geom_histogram()

pig_full <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds + (1|sitename) + (1|property), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 


pig_null_rndms <- glmmTMB(activity_seconds ~ 1 + (1|sitename) + (1|property), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_null_rndms1 <- glmmTMB(activity_seconds ~ 1 + (1|sitename), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_null_rndms2 <- glmmTMB(activity_seconds ~ 1 + (1|property), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_null <- glmmTMB(activity_seconds ~ 1, 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_add <- glmmTMB(activity_seconds ~ dist_jalama + human_seconds + (1|sitename) + (1|property), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_1 <- glmmTMB(activity_seconds ~ dist_jalama + (1|sitename) + (1|property), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

pig_2 <- glmmTMB(activity_seconds ~ human_seconds + (1|sitename) + (1|property), 
                         data = pig_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

summary(pig_full)

# test assumptions on full model
simulationOutput_pig <- simulateResiduals(fittedModel = pig_full)
plot(simulationOutput_pig)
plotResiduals(simulationOutput_pig)
testZeroInflation(simulationOutput_pig)

plot(allEffects(pig_full))
r.squaredGLMM(pig_full)

#pig_models <- c(pig_full, pig_null, pig_1, pig_2, pig_null_rndms, pig_null_rndms1, pig_null_rndms2)
#pig_names <- c('pig_full', 'pig_null', 'pig_1', 'pig_2', 'pig_null_rndms', 'pig_null_rndms1','pig_null_rndms2')

```
### finally for deer
```{r deer models}

# already made a yote df, need to update it tho
deer_activity <- activity_site_month_sp %>% 
  filter(common_name == "mule deer")

# how does the data look
ggplot(deer_activity, aes(x = activity_seconds)) +
  geom_histogram()

deer_full <- glmmTMB(activity_seconds ~ dist_jalama * human_seconds + (1|sitename) + (1|property), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 


deer_null_rndms <- glmmTMB(activity_seconds ~ 1 + (1|sitename) + (1|property), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_null_rndms1 <- glmmTMB(activity_seconds ~ 1 + (1|sitename), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_null_rndms2 <- glmmTMB(activity_seconds ~ 1 + (1|property), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_null <- glmmTMB(activity_seconds ~ 1, 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_add <- glmmTMB(activity_seconds ~ dist_jalama + human_seconds + (1|sitename) + (1|property), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_1 <- glmmTMB(activity_seconds ~ dist_jalama + (1|sitename) + (1|property), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

deer_2 <- glmmTMB(activity_seconds ~ human_seconds + (1|sitename) + (1|property), 
                         data = deer_activity, 
                         family = nbinom2(link = "log"), 
                         offset = log10(trapnights), 
                         #na.action = "na.fail",# necessary for dredge to work
                       na.action = "na.exclude"
                       ) 

summary(deer_1)
summary(deer_full)

plot(allEffects(deer_add))
r.squaredGLMM(deer_add)

```

## We're going to change gears and use some ready-made packages to understand activity levels

### Specifically the 'activity' package
```{r playing with activity}
require(activity)

### following this guide: https://bookdown.org/c_w_beirne/wildCo-Data-Analysis/activity.html


# adding solar time to the dataset to account for sunrise and sunset (Vazquez, Carmen, et al. “Comparing diel activity patterns of wildlife across latitudes and seasons: Time transformations using day length.” Methods in Ecology and Evolution 10.12 (2019): 2057-2066.)
#mammals_predictors1 <- mammals_clean %>% 
#  unite("timestamp", c(date, time))

#solart <- solartime(ymd_hms(mammals_predictors1$timestamp, tz="UTC"),
#                           mammals_predictors1$utm_x, 
#                           mammals_predictors1$utm_y,
#                           tz = 7, # an offset in numeric hours to UTC
#                           format = "%Y-%m-%d %H:%M:%S")

#mammals_predictors <- mammals_predictors1 %>% 
#  mutate(solar = solart$solar) %>% 
#  mutate(clock = solart$clock)

#plot(mammals_predictors$solar, mammals_predictors$clock)

# Fit an activity model
#m1 <- fitact(mammals_predictors$solar[mammals_predictors$common_name=="coyote"], sample="model", reps=100, show = TRUE) # reps are number of bootstraps, we're sampling the data instead of the model, show = TRUE shows a progress bar while bootstrapping
#plot(m1)

```


## learning to rarefy wiht iNEXT and other richness cacl attempts
```{r practicing rarefy, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

# we need to adjust for effort by rarefying, specaccum rarefies by site
# Use specaccum for species accumulation curves where sites are sampled instead of individuals.
## Methods "random" and "collector" can take weights (w) that give the sampling effort for each site. The weights w do not influence the order the sites are accumulated, but only the value of the sampling effort so that not all sites are equal. 
## For weighted "random" method the effort refers to the average effort per site, or sum of weights per number of sites. With weighted method = "random", the averaged species richness is found from linear interpolation of single random permutations. Therefore at least the first value (and often several first) have NA richness, because these values cannot be interpolated in all cases but should be extrapolated. The plot function defaults to display the results as scaled to sites, but this can be changed selecting xvar = "effort" (weighted methods)

# *method =  collector: “adds sites in the order they happen to be in the data”
#specaccum(richness_matrix, method = "collector", w = trapnights) # w weights each sitemonth to standardize for effort

#rarefication_df <- rarefy(richness_matrix, 2) %>%  # rarefy to smallest size sample, e.g. least trapnights which is 1 but 1 fucks with the equation
#  as_tibble(rownames = "sitemonth_id") #%>% 
  #mutate(richness_per_day = value/2) # right now it's rarefied to a sample size of two days...

#rarecurve(richness_matrix) # just pass the raw dataframe, can include step = [however many observations]


# rarefy function rarefies individual rows of your data: it takes a subsample of your occurrences ("individuals") within each row. If all these sampled individuals have value 1, you will have a subsample of ones, and the sum of ones is the sample size: that was what you got. There is no meaningful way of rarefying a vector of ones: you need count data with some counts > 1.
# You were perhaps looking for accumulation of genes in your whole data set when subsampling rows of the matrix. This is done in vegan function specaccum (argument method = "exact") which has its own plot etc methods.

# attempting to rarefy using iNEXT because we need sample-based (not indiv-based) rarefaction
library(iNEXT)

###### iNEXT requires a list of dataframes/matrices that's the opposite shape of ours: "samples" (day) as columns and species names as rownames #####
### to run "incidence" and "incidence frequency" curves - important for sampling-based rarefication - we need a list of presence/absence matrices for each assemblage ###
### to run incidence freq, we further need to calculate row.sums for each matrix ###
# I need columsn for each site, a row of how many days that site was sampled, and the cells have teh FREQUENCY that animals were seen, e.g. how many days of the days sampled that species was seen

source(here("DataWrangling/iNEXT_matrix_generation"))


# make sure it works for iNext:
DataInfo(inext_sitemonth_incraw, datatype = "incidence_raw")

#diversity_estimates <- iNEXT(inext_sitemonth_incraw, datatype = "incidence_raw") # this gives us a list of dataframes, 1) with data about the inext object 2) and 3) information about how rarefaction was done?? and 4) diversity estimates + CI

# now we can graph with a built in grapher:

ggiNEXT(diversity_estimates, type = 1) # type 1 is a sample-size based RE curve, plots curves with CI as a function of sample size (individuals for some reason is the only option if you have raw incidence data)

###### need to now extract the column of richness estimates from the correct dataframe within the iNEXT object ######

# deprecated code because I wrote this into a source code file:
incid_freq_sitemonth1 <- activity_by_day_matrix %>% 
  rename(plot.no = trapnights) %>% # testing out renaming this so maybe it works??
    mutate(across(14:28, ~ case_when( . == 0 ~ 0,
                                 . > 0 ~ 1))) %>%
  # need to sum "incidences" (seen or not) over all days per month per site
  group_by(sitemonth_id, plot.no) %>% 
  summarise(across(starts_with("sp"), ~ sum(.x))) %>% # this syntax sums all columsn that start with sp by site x month
  ungroup() %>% 
  t(.) %>% # transposes, e.g. flips the axis. nice thing is it takes trapnights with it!
  row_to_names(row_number = 1)
  # for some reason transpose turned a lot of values into characters
  #apply(., 2, as.numeric) # don't do this, it deletes row names, only soluation below:

incid_freq_sitemonth <- apply(incid_freq_sitemonth1, 2, as.numeric)
rownames(incid_freq_sitemonth) <- rownames(incid_freq_sitemonth1)

# attempting to rarefy using iNEXT because we need sample-based (not indiv-based) rarefaction
library(iNEXT)

hp.abund <- read.delim ('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/data/hp.abund.txt')
hp.incid <- read.delim ('https://raw.githubusercontent.com/zdealveindy/anadat-r/master/data/hp.incid.txt')

# need to make the daily activity matrix into the right shape:
abund <- activity_by_day_matrix %>% 
  select(!c(property, sitename, month, date, dist_jalama, habitat_adjacent, habitat_secondary, iz_type, total_human_sec, humans_per_trapnight, trapnights, burst_settings)) # need only sitemonth and counts for all spp i think

incid <- abund %>% 
  mutate(across(2:16, ~ case_when(. > 0 ~ 1,
                                 . == 0 ~ 0)))

incid <- as.data.frame(incid)

DataInfo(incid, datatype = 'incidence_raw')



### no longer need the following code because i made my freq matrix a different way! below is to make a single raw matrix
###### creating dataframe for site:month samples ######
# pull out trapnights
#trapnights <- activity_wide %>% 
#  select(sitemonth_id, trapnights) %>% 
#  #pull(trapnights) %>% 
#  pivot_wider(names_from = sitemonth_id, 
#              values_from = trapnights) %>% 
#  mutate(common_name = "trapnights") %>% 
#  relocate(common_name) # mutating and relocating this column in there to trick r into binding on a cell that says "trapnights" into the right place, default is to relocate to col 1

# create dataframe
#inext_incid <- activity_site_month_sp %>% 
#  # only want species columns, trapnights (# samples needs to be first row), and group (sitemonth)
#  select(sitemonth_id, common_name, activity_seconds) %>% 
#  # need incidence, not abundance
#  mutate(incidence = case_when(activity_seconds == 0 ~ 0,
#                               activity_seconds > 0 ~ 1)) %>% 
#  select(-activity_seconds) %>% 
#  # now pivot so colnames are sitemonths
#  pivot_wider(#id_cols = c(sitemonth_id, trapnights),
#              names_from = sitemonth_id, 
#              values_from = incidence, 
#              values_fill = 0) %>% 
#  # now add on trapnights row
#  full_join(., trapnights) %>% 
#  clean_names() %>% 
#  # get row to top
#  mutate(common_name = fct_relevel(inext_incid$common_name, "trapnights")) %>%  #Any levels not mentioned will be left in their existing order, by default after the explicitly mentioned levels.
#  arrange(common_name) 

#inext_incid_matrix <- as.data.frame(inext_incid) %>% 
#  column_to_rownames(var = "common_name")

############### BELOW is for visualizing ############### 


###### creating dataframe for site samples ######
activity_site_sp <- activity_site_month_sp %>% 
  group_by(sitename, common_name) %>% 
  summarize(activity_seconds = sum(activity_seconds), 
            trapnights = sum(trapnights))

trapnights_m <- activity_site_sp %>% 
  distinct(sitename, trapnights) %>% 
  pivot_wider(names_from = sitename, 
              values_from = trapnights) %>% 
  mutate(common_name = "trapnights") %>% 
  relocate(common_name)

inext_incid_site <- activity_site_sp %>% 
  # only want species columns, trapnights (# samples needs to be first row), and group (sitemonth)
  select(sitename, common_name, activity_seconds) %>% 
  # need incidence, not abundance
  mutate(incidence = case_when(activity_seconds == 0 ~ 0,
                               activity_seconds > 0 ~ 1)) %>% 
  select(-activity_seconds) %>% 
  # now pivot so colnames are sitemonths
  pivot_wider(
              names_from = sitename, 
              values_from = incidence, 
              values_fill = 0) %>% 
  # now add on trapnights row
  full_join(., trapnights_m) %>% 
  clean_names() %>% 
  # get row to top
  mutate(common_name = fct_relevel(common_name, "trapnights")) %>%  #Any levels not mentioned will be left in their existing order, by default after the explicitly mentioned levels.
  arrange(common_name) 

inext_incid_matrix_site <- as.data.frame(inext_incid_site) %>% 
  column_to_rownames(var = "common_name")

# make sure it works for iNext:
DataInfo(inext_incid_matrix_site, datatype = "incidence_raw")

diversity_estimates_site <- iNEXT(inext_incid_matrix_site)

ggiNEXT(diversity_estimates_site, type = 1)

###### creating dataframe for month samples ######
activity_month_sp <- activity_site_month_sp %>% 
  group_by(month, common_name) %>% 
  summarize(activity_seconds = sum(activity_seconds), 
            trapnights = sum(trapnights))

trapnights_m <- activity_month_sp %>% 
  distinct(month, trapnights) %>% 
  pivot_wider(names_from = month, 
              values_from = trapnights) %>% 
  mutate(common_name = "trapnights") %>% 
  relocate(common_name)

inext_incid_month <- activity_month_sp %>% 
  # only want species columns, trapnights (# samples needs to be first row), and group (sitemonth)
  select(month, common_name, activity_seconds) %>% 
  # need incidence, not abundance
  mutate(incidence = case_when(activity_seconds == 0 ~ 0,
                               activity_seconds > 0 ~ 1)) %>% 
  select(-activity_seconds) %>% 
  # now pivot so colnames are sitemonths
  pivot_wider(
              names_from = month, 
              values_from = incidence, 
              values_fill = 0) %>% 
  # now add on trapnights row
  full_join(., trapnights_m) %>% 
  clean_names() %>% 
  # get row to top
  mutate(common_name = fct_relevel(common_name, "trapnights")) %>%  #Any levels not mentioned will be left in their existing order, by default after the explicitly mentioned levels.
  arrange(common_name) 

inext_incid_matrix_month <- as.data.frame(inext_incid_month) %>% 
  column_to_rownames(var = "common_name")

# make sure it works for iNext:
DataInfo(inext_incid_matrix_month, datatype = "incidence_raw")

diversity_estimates_month <- iNEXT(inext_incid_matrix_month)

ggiNEXT(diversity_estimates_month, type = 2)


```





